category,title,abstract
cs.AI,Dynamic Backtracking,"  Because of their occasional need to return to shallow points in a search
tree, existing backtracking methods can sometimes erase meaningful progress
toward solving a search problem. In this paper, we present a method by which
backtrack points can be moved deeper in the search space, thereby avoiding this
difficulty. The technique developed is a variant of dependency-directed
backtracking that uses only polynomial space while still providing useful
control information and retaining the completeness guarantees provided by
earlier approaches.
"
cs.AI,"A Market-Oriented Programming Environment and its Application to
  Distributed Multicommodity Flow Problems","  Market price systems constitute a well-understood class of mechanisms that
under certain conditions provide effective decentralization of decision making
with minimal communication overhead. In a market-oriented programming approach
to distributed problem solving, we derive the activities and resource
allocations for a set of computational agents by computing the competitive
equilibrium of an artificial economy. WALRAS provides basic constructs for
defining computational market structures, and protocols for deriving their
corresponding price equilibria. In a particular realization of this approach
for a form of multicommodity flow problem, we see that careful construction of
the decision process according to economic principles can lead to efficient
distributed resource allocation, and that the behavior of the system can be
meaningfully analyzed in economic terms.
"
cs.AI,An Empirical Analysis of Search in GSAT,"  We describe an extensive study of search in GSAT, an approximation procedure
for propositional satisfiability. GSAT performs greedy hill-climbing on the
number of satisfied clauses in a truth assignment. Our experiments provide a
more complete picture of GSAT's search than previous accounts. We describe in
detail the two phases of search: rapid hill-climbing followed by a long plateau
search. We demonstrate that when applied to randomly generated 3SAT problems,
there is a very simple scaling with problem size for both the mean number of
satisfied clauses and the mean branching rate. Our results allow us to make
detailed numerical conjectures about the length of the hill-climbing phase, the
average gradient of this phase, and to conjecture that both the average score
and average branching rate decay exponentially during plateau search. We end by
showing how these results can be used to direct future theoretical analysis.
This work provides a case study of how computer experiments can be used to
improve understanding of the theoretical properties of algorithms.
"
cs.AI,The Difficulties of Learning Logic Programs with Cut,"  As real logic programmers normally use cut (!), an effective learning
procedure for logic programs should be able to deal with it. Because the cut
predicate has only a procedural meaning, clauses containing cut cannot be
learned using an extensional evaluation method, as is done in most learning
systems. On the other hand, searching a space of possible programs (instead of
a space of independent clauses) is unfeasible. An alternative solution is to
generate first a candidate base program which covers the positive examples, and
then make it consistent by inserting cut where appropriate. The problem of
learning programs with cut has not been investigated before and this seems to
be a natural and reasonable approach. We generalize this scheme and investigate
the difficulties that arise. Some of the major shortcomings are actually
caused, in general, by the need for intensional evaluation. As a conclusion,
the analysis of this paper suggests, on precise and technical grounds, that
learning cut is difficult, and current induction techniques should probably be
restricted to purely declarative logic languages.
"
cs.AI,Software Agents: Completing Patterns and Constructing User Interfaces,"  To support the goal of allowing users to record and retrieve information,
this paper describes an interactive note-taking system for pen-based computers
with two distinctive features. First, it actively predicts what the user is
going to write. Second, it automatically constructs a custom, button-box user
interface on request. The system is an example of a learning-apprentice
software- agent. A machine learning component characterizes the syntax and
semantics of the user's information. A performance system uses this learned
information to generate completion strings and construct a user interface.
Description of Online Appendix: People like to record information. Doing this
on paper is initially efficient, but lacks flexibility. Recording information
on a computer is less efficient but more powerful. In our new note taking
softwre, the user records information directly on a computer. Behind the
interface, an agent acts for the user. To help, it provides defaults and
constructs a custom user interface. The demonstration is a QuickTime movie of
the note taking agent in action. The file is a binhexed self-extracting
archive. Macintosh utilities for binhex are available from
mac.archive.umich.edu. QuickTime is available from ftp.apple.com in the
dts/mac/sys.soft/quicktime.
"
cs.AI,Decidable Reasoning in Terminological Knowledge Representation Systems,"  Terminological knowledge representation systems (TKRSs) are tools for
designing and using knowledge bases that make use of terminological languages
(or concept languages). We analyze from a theoretical point of view a TKRS
whose capabilities go beyond the ones of presently available TKRSs. The new
features studied, often required in practical applications, can be summarized
in three main points. First, we consider a highly expressive terminological
language, called ALCNR, including general complements of concepts, number
restrictions and role conjunction. Second, we allow to express inclusion
statements between general concepts, and terminological cycles as a particular
case. Third, we prove the decidability of a number of desirable TKRS-deduction
services (like satisfiability, subsumption and instance checking) through a
sound, complete and terminating calculus for reasoning in ALCNR-knowledge
bases. Our calculus extends the general technique of constraint systems. As a
byproduct of the proof, we get also the result that inclusion statements in
ALCNR can be simulated by terminological cycles, if descriptive semantics is
adopted.
"
cs.AI,Teleo-Reactive Programs for Agent Control,"  A formalism is presented for computing and organizing actions for autonomous
agents in dynamic environments. We introduce the notion of teleo-reactive (T-R)
programs whose execution entails the construction of circuitry for the
continuous computation of the parameters and conditions on which agent action
is based. In addition to continuous feedback, T-R programs support parameter
binding and recursion. A primary difference between T-R programs and many other
circuit-based systems is that the circuitry of T-R programs is more compact; it
is constructed at run time and thus does not have to anticipate all the
contingencies that might arise over all possible runs. In addition, T-R
programs are intuitive and easy to write and are written in a form that is
compatible with automatic planning and learning methods. We briefly describe
some experimental applications of T-R programs in the control of simulated and
actual mobile robots.
"
cs.AI,"Learning the Past Tense of English Verbs: The Symbolic Pattern
  Associator vs. Connectionist Models","  Learning the past tense of English verbs - a seemingly minor aspect of
language acquisition - has generated heated debates since 1986, and has become
a landmark task for testing the adequacy of cognitive modeling. Several
artificial neural networks (ANNs) have been implemented, and a challenge for
better symbolic models has been posed. In this paper, we present a
general-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree
learning algorithm ID3. We conduct extensive head-to-head comparisons on the
generalization ability between ANN models and the SPA under different
representations. We conclude that the SPA generalizes the past tense of unseen
verbs better than ANN models by a wide margin, and we offer insights as to why
this should be the case. We also discuss a new default strategy for
decision-tree learning algorithms.
"
cs.AI,"Substructure Discovery Using Minimum Description Length and Background
  Knowledge","  The ability to identify interesting and repetitive substructures is an
essential component to discovering knowledge in structural data. We describe a
new version of our SUBDUE substructure discovery system based on the minimum
description length principle. The SUBDUE system discovers substructures that
compress the original data and represent structural concepts in the data. By
replacing previously-discovered substructures in the data, multiple passes of
SUBDUE produce a hierarchical description of the structural regularities in the
data. SUBDUE uses a computationally-bounded inexact graph match that identifies
similar, but not identical, instances of a substructure and finds an
approximate measure of closeness of two substructures when under computational
constraints. In addition to the minimum description length principle, other
background knowledge can be used by SUBDUE to guide the search towards more
appropriate substructures. Experiments in a variety of domains demonstrate
SUBDUE's ability to find substructures capable of compressing the original data
and to discover structural concepts important to the domain. Description of
Online Appendix: This is a compressed tar file containing the SUBDUE discovery
system, written in C. The program accepts as input databases represented in
graph form, and will output discovered substructures with their corresponding
value.
"
cs.AI,Bias-Driven Revision of Logical Domain Theories,"  The theory revision problem is the problem of how best to go about revising a
deficient domain theory using information contained in examples that expose
inaccuracies. In this paper we present our approach to the theory revision
problem for propositional domain theories. The approach described here, called
PTR, uses probabilities associated with domain theory elements to numerically
track the ``flow'' of proof through the theory. This allows us to measure the
precise role of a clause or literal in allowing or preventing a (desired or
undesired) derivation for a given example. This information is used to
efficiently locate and repair flawed elements of the theory. PTR is proved to
converge to a theory which correctly classifies all examples, and shown
experimentally to be fast and accurate even for deep theories.
"
cs.AI,"Exploring the Decision Forest: An Empirical Investigation of Occam's
  Razor in Decision Tree Induction","  We report on a series of experiments in which all decision trees consistent
with the training data are constructed. These experiments were run to gain an
understanding of the properties of the set of consistent decision trees and the
factors that affect the accuracy of individual trees. In particular, we
investigated the relationship between the size of a decision tree consistent
with some training data and the accuracy of the tree on test data. The
experiments were performed on a massively parallel Maspar computer. The results
of the experiments on several artificial and two real world problems indicate
that, for many of the problems investigated, smaller consistent decision trees
are on average less accurate than the average accuracy of slightly larger
trees.
"
cs.AI,"A Semantics and Complete Algorithm for Subsumption in the CLASSIC
  Description Logic","  This paper analyzes the correctness of the subsumption algorithm used in
CLASSIC, a description logic-based knowledge representation system that is
being used in practical applications. In order to deal efficiently with
individuals in CLASSIC descriptions, the developers have had to use an
algorithm that is incomplete with respect to the standard, model-theoretic
semantics for description logics. We provide a variant semantics for
descriptions with respect to which the current implementation is complete, and
which can be independently motivated. The soundness and completeness of the
polynomial-time subsumption algorithm is established using description graphs,
which are an abstracted version of the implementation structures used in
CLASSIC, and are of independent interest.
"
cs.AI,Applying GSAT to Non-Clausal Formulas,"  In this paper we describe how to modify GSAT so that it can be applied to
non-clausal formulas. The idea is to use a particular ``score'' function which
gives the number of clauses of the CNF conversion of a formula which are false
under a given truth assignment. Its value is computed in linear time, without
constructing the CNF conversion itself. The proposed methodology applies to
most of the variants of GSAT proposed so far.
"
cs.AI,Random Worlds and Maximum Entropy,"  Given a knowledge base KB containing first-order and statistical facts, we
consider a principled method, called the random-worlds method, for computing a
degree of belief that some formula Phi holds given KB. If we are reasoning
about a world or system consisting of N individuals, then we can consider all
possible worlds, or first-order models, with domain {1,...,N} that satisfy KB,
and compute the fraction of them in which Phi is true. We define the degree of
belief to be the asymptotic value of this fraction as N grows large. We show
that when the vocabulary underlying Phi and KB uses constants and unary
predicates only, we can naturally associate an entropy with each world. As N
grows larger, there are many more worlds with higher entropy. Therefore, we can
use a maximum-entropy computation to compute the degree of belief. This result
is in a similar spirit to previous work in physics and artificial intelligence,
but is far more general. Of equal interest to the result itself are the
limitations on its scope. Most importantly, the restriction to unary predicates
seems necessary. Although the random-worlds method makes sense in general, the
connection to maximum entropy seems to disappear in the non-unary case. These
observations suggest unexpected limitations to the applicability of
maximum-entropy methods.
"
cs.AI,"Pattern Matching and Discourse Processing in Information Extraction from
  Japanese Text","  Information extraction is the task of automatically picking up information of
interest from an unconstrained text. Information of interest is usually
extracted in two steps. First, sentence level processing locates relevant
pieces of information scattered throughout the text; second, discourse
processing merges coreferential information to generate the output. In the
first step, pieces of information are locally identified without recognizing
any relationships among them. A key word search or simple pattern search can
achieve this purpose. The second step requires deeper knowledge in order to
understand relationships among separately identified pieces of information.
Previous information extraction systems focused on the first step, partly
because they were not required to link up each piece of information with other
pieces. To link the extracted pieces of information and map them onto a
structured output format, complex discourse processing is essential. This paper
reports on a Japanese information extraction system that merges information
using a pattern matcher and discourse processor. Evaluation results show a high
level of system performance which approaches human performance.
"
cs.AI,A System for Induction of Oblique Decision Trees,"  This article describes a new system for induction of oblique decision trees.
This system, OC1, combines deterministic hill-climbing with two forms of
randomization to find a good oblique split (in the form of a hyperplane) at
each node of a decision tree. Oblique decision tree methods are tuned
especially for domains in which the attributes are numeric, although they can
be adapted to symbolic or mixed symbolic/numeric attributes. We present
extensive empirical studies, using both real and artificial data, that analyze
OC1's ability to construct oblique trees that are smaller and more accurate
than their axis-parallel counterparts. We also examine the benefits of
randomization for the construction of oblique decision trees.
"
cs.AI,On Planning while Learning,"  This paper introduces a framework for Planning while Learning where an agent
is given a goal to achieve in an environment whose behavior is only partially
known to the agent. We discuss the tractability of various plan-design
processes. We show that for a large natural class of Planning while Learning
systems, a plan can be presented and verified in a reasonable time. However,
coming up algorithmically with a plan, even for simple classes of systems is
apparently intractable. We emphasize the role of off-line plan-design
processes, and show that, in most natural cases, the verification (projection)
part can be carried out in an efficient algorithmic manner.
"
cs.AI,Wrap-Up: a Trainable Discourse Module for Information Extraction,"  The vast amounts of on-line text now available have led to renewed interest
in information extraction (IE) systems that analyze unrestricted text,
producing a structured representation of selected information from the text.
This paper presents a novel approach that uses machine learning to acquire
knowledge for some of the higher level IE processing. Wrap-Up is a trainable IE
discourse component that makes intersentential inferences and identifies
logical relations among information extracted from the text. Previous
corpus-based approaches were limited to lower level processing such as
part-of-speech tagging, lexical disambiguation, and dictionary construction.
Wrap-Up is fully trainable, and not only automatically decides what classifiers
are needed, but even derives the feature set for each classifier automatically.
Performance equals that of a partially trainable discourse module requiring
manual customization for each domain.
"
cs.AI,Operations for Learning with Graphical Models,"  This paper is a multidisciplinary review of empirical, statistical learning
from a graphical model perspective. Well-known examples of graphical models
include Bayesian networks, directed graphs representing a Markov chain, and
undirected networks representing a Markov field. These graphical models are
extended to model data analysis and empirical learning using the notation of
plates. Graphical operations for simplifying and manipulating a problem are
provided including decomposition, differentiation, and the manipulation of
probability models from the exponential family. Two standard algorithm schemas
for learning are reviewed in a graphical framework: Gibbs sampling and the
expectation maximization algorithm. Using these operations and schemas, some
popular algorithms can be synthesized from their graphical specification. This
includes versions of linear regression, techniques for feed-forward networks,
and learning Gaussian and discrete Bayesian networks from data. The paper
concludes by sketching some implications for data analysis and summarizing how
some popular algorithms fall within the framework presented. The main original
contributions here are the decomposition techniques and the demonstration that
graphical models provide a framework for understanding and developing complex
learning algorithms.
"
cs.AI,Total-Order and Partial-Order Planning: A Comparative Analysis,"  For many years, the intuitions underlying partial-order planning were largely
taken for granted. Only in the past few years has there been renewed interest
in the fundamental principles underlying this paradigm. In this paper, we
present a rigorous comparative analysis of partial-order and total-order
planning by focusing on two specific planners that can be directly compared. We
show that there are some subtle assumptions that underly the wide-spread
intuitions regarding the supposed efficiency of partial-order planning. For
instance, the superiority of partial-order planning can depend critically upon
the search strategy and the structure of the search space. Understanding the
underlying assumptions is crucial for constructing efficient planners.
"
cs.AI,Solving Multiclass Learning Problems via Error-Correcting Output Codes,"  Multiclass learning problems involve finding a definition for an unknown
function f(x) whose range is a discrete set containing k &gt 2 values (i.e., k
``classes''). The definition is acquired by studying collections of training
examples of the form [x_i, f (x_i)]. Existing approaches to multiclass learning
problems include direct application of multiclass algorithms such as the
decision-tree algorithms C4.5 and CART, application of binary concept learning
algorithms to learn individual binary functions for each of the k classes, and
application of binary concept learning algorithms with distributed output
representations. This paper compares these three approaches to a new technique
in which error-correcting codes are employed as a distributed output
representation. We show that these output representations improve the
generalization performance of both C4.5 and backpropagation on a wide range of
multiclass learning tasks. We also demonstrate that this approach is robust
with respect to changes in the size of the training sample, the assignment of
distributed representations to particular classes, and the application of
overfitting avoidance techniques such as decision-tree pruning. Finally, we
show that---like the other methods---the error-correcting code technique can
provide reliable class probability estimates. Taken together, these results
demonstrate that error-correcting output codes provide a general-purpose method
for improving the performance of inductive learning programs on multiclass
problems.
"
cs.AI,A Domain-Independent Algorithm for Plan Adaptation,"  The paradigms of transformational planning, case-based planning, and plan
debugging all involve a process known as plan adaptation - modifying or
repairing an old plan so it solves a new problem. In this paper we provide a
domain-independent algorithm for plan adaptation, demonstrate that it is sound,
complete, and systematic, and compare it to other adaptation algorithms in the
literature. Our approach is based on a view of planning as searching a graph of
partial plans. Generative planning starts at the graph's root and moves from
node to node using plan-refinement operators. In planning by adaptation, a
library plan - an arbitrary node in the plan graph - is the starting point for
the search, and the plan-adaptation algorithm can apply both the same
refinement operators available to a generative planner and can also retract
constraints and steps from the plan. Our algorithm's completeness ensures that
the adaptation algorithm will eventually search the entire graph and its
systematicity ensures that it will do so without redundantly searching any
parts of the graph.
"
cs.AI,"Truncating Temporal Differences: On the Efficient Implementation of
  TD(lambda) for Reinforcement Learning","  Temporal difference (TD) methods constitute a class of methods for learning
predictions in multi-step prediction problems, parameterized by a recency
factor lambda. Currently the most important application of these methods is to
temporal credit assignment in reinforcement learning. Well known reinforcement
learning algorithms, such as AHC or Q-learning, may be viewed as instances of
TD learning. This paper examines the issues of the efficient and general
implementation of TD(lambda) for arbitrary lambda, for use with reinforcement
learning algorithms optimizing the discounted sum of rewards. The traditional
approach, based on eligibility traces, is argued to suffer from both
inefficiency and lack of generality. The TTD (Truncated Temporal Differences)
procedure is proposed as an alternative, that indeed only approximates
TD(lambda), but requires very little computation per action and can be used
with arbitrary function representation methods. The idea from which it is
derived is fairly simple and not new, but probably unexplored so far.
Encouraging experimental results are presented, suggesting that using lambda
&gt 0 with the TTD procedure allows one to obtain a significant learning
speedup at essentially the same cost as usual TD(0) learning.
"
cs.AI,"Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic
  Decision Tree Induction Algorithm","  This paper introduces ICET, a new algorithm for cost-sensitive
classification. ICET uses a genetic algorithm to evolve a population of biases
for a decision tree induction algorithm. The fitness function of the genetic
algorithm is the average cost of classification when using the decision tree,
including both the costs of tests (features, measurements) and the costs of
classification errors. ICET is compared here with three other algorithms for
cost-sensitive classification - EG2, CS-ID3, and IDX - and also with C4.5,
which classifies without regard to cost. The five algorithms are evaluated
empirically on five real-world medical datasets. Three sets of experiments are
performed. The first set examines the baseline performance of the five
algorithms on the five datasets and establishes that ICET performs
significantly better than its competitors. The second set tests the robustness
of ICET under a variety of conditions and shows that ICET maintains its
advantage. The third set looks at ICET's search in bias space and discovers a
way to improve the search.
"
cs.AI,"Rerepresenting and Restructuring Domain Theories: A Constructive
  Induction Approach","  Theory revision integrates inductive learning and background knowledge by
combining training examples with a coarse domain theory to produce a more
accurate theory. There are two challenges that theory revision and other
theory-guided systems face. First, a representation language appropriate for
the initial theory may be inappropriate for an improved theory. While the
original representation may concisely express the initial theory, a more
accurate theory forced to use that same representation may be bulky,
cumbersome, and difficult to reach. Second, a theory structure suitable for a
coarse domain theory may be insufficient for a fine-tuned theory. Systems that
produce only small, local changes to a theory have limited value for
accomplishing complex structural alterations that may be required.
Consequently, advanced theory-guided learning systems require flexible
representation and flexible structure. An analysis of various theory revision
systems and theory-guided learning systems reveals specific strengths and
weaknesses in terms of these two desired properties. Designed to capture the
underlying qualities of each system, a new system uses theory-guided
constructive induction. Experiments in three domains show improvement over
previous theory-guided systems. This leads to a study of the behavior,
limitations, and potential of theory-guided constructive induction.
"
cs.AI,Using Pivot Consistency to Decompose and Solve Functional CSPs,"  Many studies have been carried out in order to increase the search efficiency
of constraint satisfaction problems; among them, some make use of structural
properties of the constraint network; others take into account semantic
properties of the constraints, generally assuming that all the constraints
possess the given property. In this paper, we propose a new decomposition
method benefiting from both semantic properties of functional constraints (not
bijective constraints) and structural properties of the network; furthermore,
not all the constraints need to be functional. We show that under some
conditions, the existence of solutions can be guaranteed. We first characterize
a particular subset of the variables, which we name a root set. We then
introduce pivot consistency, a new local consistency which is a weak form of
path consistency and can be achieved in O(n^2d^2) complexity (instead of
O(n^3d^3) for path consistency), and we present associated properties; in
particular, we show that any consistent instantiation of the root set can be
linearly extended to a solution, which leads to the presentation of the
aforementioned new method for solving by decomposing functional CSPs.
"
cs.AI,Adaptive Load Balancing: A Study in Multi-Agent Learning,"  We study the process of multi-agent reinforcement learning in the context of
load balancing in a distributed system, without use of either central
coordination or explicit communication. We first define a precise framework in
which to study adaptive load balancing, important features of which are its
stochastic nature and the purely local information available to individual
agents. Given this framework, we show illuminating results on the interplay
between basic adaptive behavior parameters and their effect on system
efficiency. We then investigate the properties of adaptive load balancing in
heterogeneous populations, and address the issue of exploration vs.
exploitation in that context. Finally, we show that naive use of communication
may not improve, and might even harm system efficiency.
"
cs.AI,Provably Bounded-Optimal Agents,"  Since its inception, artificial intelligence has relied upon a theoretical
foundation centered around perfect rationality as the desired property of
intelligent systems. We argue, as others have done, that this foundation is
inadequate because it imposes fundamentally unsatisfiable requirements. As a
result, there has arisen a wide gap between theory and practice in AI,
hindering progress in the field. We propose instead a property called bounded
optimality. Roughly speaking, an agent is bounded-optimal if its program is a
solution to the constrained optimization problem presented by its architecture
and the task environment. We show how to construct agents with this property
for a simple class of machine architectures in a broad class of real-time
environments. We illustrate these results using a simple model of an automated
mail sorting facility. We also define a weaker property, asymptotic bounded
optimality (ABO), that generalizes the notion of optimality in classical
complexity theory. We then construct universal ABO programs, i.e., programs
that are ABO no matter what real-time constraints are applied. Universal ABO
programs can be used as building blocks for more complex systems. We conclude
with a discussion of the prospects for bounded optimality as a theoretical
basis for AI, and relate it to similar trends in philosophy, economics, and
game theory.
"
cs.AI,Pac-Learning Recursive Logic Programs: Efficient Algorithms,"  We present algorithms that learn certain classes of function-free recursive
logic programs in polynomial time from equivalence queries. In particular, we
show that a single k-ary recursive constant-depth determinate clause is
learnable. Two-clause programs consisting of one learnable recursive clause and
one constant-depth determinate non-recursive clause are also learnable, if an
additional ``basecase'' oracle is assumed. These results immediately imply the
pac-learnability of these classes. Although these classes of learnable
recursive programs are very constrained, it is shown in a companion paper that
they are maximally general, in that generalizing either class in any natural
way leads to a computationally difficult learning problem. Thus, taken together
with its companion paper, this paper establishes a boundary of efficient
learnability for recursive logic programs.
"
cs.AI,Pac-learning Recursive Logic Programs: Negative Results,"  In a companion paper it was shown that the class of constant-depth
determinate k-ary recursive clauses is efficiently learnable. In this paper we
present negative results showing that any natural generalization of this class
is hard to learn in Valiant's model of pac-learnability. In particular, we show
that the following program classes are cryptographically hard to learn:
programs with an unbounded number of constant-depth linear recursive clauses;
programs with one constant-depth determinate clause containing an unbounded
number of recursive calls; and programs with one linear recursive clause of
constant locality. These results immediately imply the non-learnability of any
more general class of programs. We also show that learning a constant-depth
determinate program with either two linear recursive clauses or one linear
recursive clause and one non-recursive clause is as hard as learning boolean
DNF. Together with positive results from the companion paper, these negative
results establish a boundary of efficient learnability for recursive
function-free clauses.
"
cs.AI,FLECS: Planning with a Flexible Commitment Strategy,"  There has been evidence that least-commitment planners can efficiently handle
planning problems that involve difficult goal interactions. This evidence has
led to the common belief that delayed-commitment is the ""best"" possible
planning strategy. However, we recently found evidence that eager-commitment
planners can handle a variety of planning problems more efficiently, in
particular those with difficult operator choices. Resigned to the futility of
trying to find a universally successful planning strategy, we devised a planner
that can be used to study which domains and problems are best for which
planning strategies. In this article we introduce this new planning algorithm,
FLECS, which uses a FLExible Commitment Strategy with respect to plan-step
orderings. It is able to use any strategy from delayed-commitment to
eager-commitment. The combination of delayed and eager operator-ordering
commitments allows FLECS to take advantage of the benefits of explicitly using
a simulated execution state and reasoning about planning constraints. FLECS can
vary its commitment strategy across different problems and domains, and also
during the course of a single planning problem. FLECS represents a novel
contribution to planning in that it explicitly provides the choice of which
commitment strategy to use while planning. FLECS provides a framework to
investigate the mapping from planning domains and problems to efficient
planning strategies.
"
cs.AI,"Induction of First-Order Decision Lists: Results on Learning the Past
  Tense of English Verbs","  This paper presents a method for inducing logic programs from examples that
learns a new class of concepts called first-order decision lists, defined as
ordered lists of clauses each ending in a cut. The method, called FOIDL, is
based on FOIL (Quinlan, 1990) but employs intensional background knowledge and
avoids the need for explicit negative examples. It is particularly useful for
problems that involve rules with specific exceptions, such as learning the
past-tense of English verbs, a task widely studied in the context of the
symbolic/connectionist debate. FOIDL is able to learn concise, accurate
programs for this problem from significantly fewer examples than previous
methods (both connectionist and symbolic).
"
cs.AI,"Building and Refining Abstract Planning Cases by Change of
  Representation Language","  ion is one of the most promising approaches to improve the performance of
problem solvers. In several domains abstraction by dropping sentences of a
domain description -- as used in most hierarchical planners -- has proven
useful. In this paper we present examples which illustrate significant
drawbacks of abstraction by dropping sentences. To overcome these drawbacks, we
propose a more general view of abstraction involving the change of
representation language. We have developed a new abstraction methodology and a
related sound and complete learning algorithm that allows the complete change
of representation language of planning cases from concrete to abstract.
However, to achieve a powerful change of the representation language, the
abstract language itself as well as rules which describe admissible ways of
abstracting states must be provided in the domain model. This new abstraction
approach is the core of Paris (Plan Abstraction and Refinement in an Integrated
System), a system in which abstract planning cases are automatically learned
from given concrete cases. An empirical study in the domain of process planning
in mechanical engineering shows significant advantages of the proposed
reasoning from abstract cases over classical hierarchical planning.
"
cs.AI,Using Qualitative Hypotheses to Identify Inaccurate Data,"  Identifying inaccurate data has long been regarded as a significant and
difficult problem in AI. In this paper, we present a new method for identifying
inaccurate data on the basis of qualitative correlations among related data.
First, we introduce the definitions of related data and qualitative
correlations among related data. Then we put forward a new concept called
support coefficient function (SCF). SCF can be used to extract, represent, and
calculate qualitative correlations among related data within a dataset. We
propose an approach to determining dynamic shift intervals of inaccurate data,
and an approach to calculating possibility of identifying inaccurate data,
respectively. Both of the approaches are based on SCF. Finally we present an
algorithm for identifying inaccurate data by using qualitative correlations
among related data as confirmatory or disconfirmatory evidence. We have
developed a practical system for interpreting infrared spectra by applying the
method, and have fully tested the system against several hundred real spectra.
The experimental results show that the method is significantly better than the
conventional methods used in many similar systems.
"
cs.AI,An Integrated Framework for Learning and Reasoning,"  Learning and reasoning are both aspects of what is considered to be
intelligence. Their studies within AI have been separated historically,
learning being the topic of machine learning and neural networks, and reasoning
falling under classical (or symbolic) AI. However, learning and reasoning are
in many ways interdependent. This paper discusses the nature of some of these
interdependencies and proposes a general framework called FLARE, that combines
inductive learning using prior knowledge together with reasoning in a
propositional setting. Several examples that test the framework are presented,
including classical induction, many important reasoning protocols and two
simple expert systems.
"
cs.AI,Diffusion of Context and Credit Information in Markovian Models,"  This paper studies the problem of ergodicity of transition probability
matrices in Markovian models, such as hidden Markov models (HMMs), and how it
makes very difficult the task of learning to represent long-term context for
sequential data. This phenomenon hurts the forward propagation of long-term
context information, as well as learning a hidden state representation to
represent long-term context, which depends on propagating credit information
backwards in time. Using results from Markov chain theory, we show that this
problem of diffusion of context and credit is reduced when the transition
probabilities approach 0 or 1, i.e., the transition probability matrices are
sparse and the model essentially deterministic. The results found in this paper
apply to learning approaches based on continuous optimization, such as gradient
descent and the Baum-Welch algorithm.
"
cs.AI,Improving Connectionist Energy Minimization,"  Symmetric networks designed for energy minimization such as Boltzman machines
and Hopfield nets are frequently investigated for use in optimization,
constraint satisfaction and approximation of NP-hard problems. Nevertheless,
finding a global solution (i.e., a global minimum for the energy function) is
not guaranteed and even a local solution may take an exponential number of
steps. We propose an improvement to the standard local activation function used
for such networks. The improved algorithm guarantees that a global minimum is
found in linear time for tree-like subnetworks. The algorithm, called activate,
is uniform and does not assume that the network is tree-like. It can identify
tree-like subnetworks even in cyclic topologies (arbitrary networks) and avoid
local minima along these trees. For acyclic networks, the algorithm is
guaranteed to converge to a global minimum from any initial state of the system
(self-stabilization) and remains correct under various types of schedulers. On
the negative side, we show that in the presence of cycles, no uniform algorithm
exists that guarantees optimality even under a sequential asynchronous
scheduler. An asynchronous scheduler can activate only one unit at a time while
a synchronous scheduler can activate any number of units in a single time step.
In addition, no uniform algorithm exists to optimize even acyclic networks when
the scheduler is synchronous. Finally, we show how the algorithm can be
improved using the cycle-cutset scheme. The general algorithm, called
activate-with-cutset, improves over activate and has some performance
guarantees that are related to the size of the network's cycle-cutset.
"
cs.AI,"Learning Membership Functions in a Function-Based Object Recognition
  System","  Functionality-based recognition systems recognize objects at the category
level by reasoning about how well the objects support the expected function.
Such systems naturally associate a ``measure of goodness'' or ``membership
value'' with a recognized object. This measure of goodness is the result of
combining individual measures, or membership values, from potentially many
primitive evaluations of different properties of the object's shape. A
membership function is used to compute the membership value when evaluating a
primitive of a particular physical property of an object. In previous versions
of a recognition system known as Gruff, the membership function for each of the
primitive evaluations was hand-crafted by the system designer. In this paper,
we provide a learning component for the Gruff system, called Omlet, that
automatically learns membership functions given a set of example objects
labeled with their desired category measure. The learning algorithm is
generally applicable to any problem in which low-level membership values are
combined through an and-or tree structure to give a final overall membership
value.
"
cs.AI,Flexibly Instructable Agents,"  This paper presents an approach to learning from situated, interactive
tutorial instruction within an ongoing agent. Tutorial instruction is a
flexible (and thus powerful) paradigm for teaching tasks because it allows an
instructor to communicate whatever types of knowledge an agent might need in
whatever situations might arise. To support this flexibility, however, the
agent must be able to learn multiple kinds of knowledge from a broad range of
instructional interactions. Our approach, called situated explanation, achieves
such learning through a combination of analytic and inductive techniques. It
combines a form of explanation-based learning that is situated for each
instruction with a full suite of contextually guided responses to incomplete
explanations. The approach is implemented in an agent called Instructo-Soar
that learns hierarchies of new tasks and other domain knowledge from
interactive natural language instructions. Instructo-Soar meets three key
requirements of flexible instructability that distinguish it from previous
systems: (1) it can take known or unknown commands at any instruction point;
(2) it can handle instructions that apply to either its current situation or to
a hypothetical situation specified in language (as in, for instance,
conditional instructions); and (3) it can learn, from instructions, each class
of knowledge it uses to perform tasks.
"
cs.AI,OPUS: An Efficient Admissible Algorithm for Unordered Search,"  OPUS is a branch and bound search algorithm that enables efficient admissible
search through spaces for which the order of search operator application is not
significant. The algorithm's search efficiency is demonstrated with respect to
very large machine learning search spaces. The use of admissible search is of
potential value to the machine learning community as it means that the exact
learning biases to be employed for complex learning tasks can be precisely
specified and manipulated. OPUS also has potential for application in other
areas of artificial intelligence, notably, truth maintenance.
"
cs.AI,"Vision-Based Road Detection in Automotive Systems: A Real-Time
  Expectation-Driven Approach","  The main aim of this work is the development of a vision-based road detection
system fast enough to cope with the difficult real-time constraints imposed by
moving vehicle applications. The hardware platform, a special-purpose massively
parallel system, has been chosen to minimize system production and operational
costs. This paper presents a novel approach to expectation-driven low-level
image segmentation, which can be mapped naturally onto mesh-connected massively
parallel SIMD architectures capable of handling hierarchical data structures.
The input image is assumed to contain a distorted version of a given template;
a multiresolution stretching process is used to reshape the original template
in accordance with the acquired image content, minimizing a potential function.
The distorted template is the process output.
"
cs.AI,Generalization of Clauses under Implication,"  In the area of inductive learning, generalization is a main operation, and
the usual definition of induction is based on logical implication. Recently
there has been a rising interest in clausal representation of knowledge in
machine learning. Almost all inductive learning systems that perform
generalization of clauses use the relation theta-subsumption instead of
implication. The main reason is that there is a well-known and simple technique
to compute least general generalizations under theta-subsumption, but not under
implication. However generalization under theta-subsumption is inappropriate
for learning recursive clauses, which is a crucial problem since recursion is
the basic program structure of logic programs. We note that implication between
clauses is undecidable, and we therefore introduce a stronger form of
implication, called T-implication, which is decidable between clauses. We show
that for every finite set of clauses there exists a least general
generalization under T-implication. We describe a technique to reduce
generalizations under implication of a clause to generalizations under
theta-subsumption of what we call an expansion of the original clause. Moreover
we show that for every non-tautological clause there exists a T-complete
expansion, which means that every generalization under T-implication of the
clause is reduced to a generalization under theta-subsumption of the expansion.
"
cs.AI,Decision-Theoretic Foundations for Causal Reasoning,"  We present a definition of cause and effect in terms of decision-theoretic
primitives and thereby provide a principled foundation for causal reasoning.
Our definition departs from the traditional view of causation in that causal
assertions may vary with the set of decisions available. We argue that this
approach provides added clarity to the notion of cause. Also in this paper, we
examine the encoding of causal relationships in directed acyclic graphs. We
describe a special class of influence diagrams, those in canonical form, and
show its relationship to Pearl's representation of cause and effect. Finally,
we show how canonical form facilitates counterfactual reasoning.
"
cs.AI,Translating between Horn Representations and their Characteristic Models,"  Characteristic models are an alternative, model based, representation for
Horn expressions. It has been shown that these two representations are
incomparable and each has its advantages over the other. It is therefore
natural to ask what is the cost of translating, back and forth, between these
representations. Interestingly, the same translation questions arise in
database theory, where it has applications to the design of relational
databases. This paper studies the computational complexity of these problems.
Our main result is that the two translation problems are equivalent under
polynomial reductions, and that they are equivalent to the corresponding
decision problem. Namely, translating is equivalent to deciding whether a given
set of models is the set of characteristic models for a given Horn expression.
We also relate these problems to the hypergraph transversal problem, a well
known problem which is related to other applications in AI and for which no
polynomial time algorithm is known. It is shown that in general our translation
problems are at least as hard as the hypergraph transversal problem, and in a
special case they are equivalent to it.
"
cs.AI,Statistical Feature Combination for the Evaluation of Game Positions,"  This article describes an application of three well-known statistical methods
in the field of game-tree search: using a large number of classified Othello
positions, feature weights for evaluation functions with a
game-phase-independent meaning are estimated by means of logistic regression,
Fisher's linear discriminant, and the quadratic discriminant function for
normally distributed features. Thereafter, the playing strengths are compared
by means of tournaments between the resulting versions of a world-class Othello
program. In this application, logistic regression - which is used here for the
first time in the context of game playing - leads to better results than the
other approaches.
"
cs.AI,Rule-based Machine Learning Methods for Functional Prediction,"  We describe a machine learning method for predicting the value of a
real-valued function, given the values of multiple input variables. The method
induces solutions from samples in the form of ordered disjunctive normal form
(DNF) decision rules. A central objective of the method and representation is
the induction of compact, easily interpretable solutions. This rule-based
decision model can be extended to search efficiently for similar cases prior to
approximating function values. Experimental results on real-world data
demonstrate that the new techniques are competitive with existing machine
learning and statistical methods and can sometimes yield superior regression
performance.
"
cs.AI,"The Design and Experimental Analysis of Algorithms for Temporal
  Reasoning","  Many applications -- from planning and scheduling to problems in molecular
biology -- rely heavily on a temporal reasoning component. In this paper, we
discuss the design and empirical analysis of algorithms for a temporal
reasoning system based on Allen's influential interval-based framework for
representing temporal information. At the core of the system are algorithms for
determining whether the temporal information is consistent, and, if so, finding
one or more scenarios that are consistent with the temporal information. Two
important algorithms for these tasks are a path consistency algorithm and a
backtracking algorithm. For the path consistency algorithm, we develop
techniques that can result in up to a ten-fold speedup over an already highly
optimized implementation. For the backtracking algorithm, we develop variable
and value ordering heuristics that are shown empirically to dramatically
improve the performance of the algorithm. As well, we show that a previously
suggested reformulation of the backtracking search problem can reduce the time
and space requirements of the backtracking search. Taken together, the
techniques we develop allow a temporal reasoning component to solve problems
that are of practical size.
"
cs.AI,"Well-Founded Semantics for Extended Logic Programs with Dynamic
  Preferences","  The paper describes an extension of well-founded semantics for logic programs
with two types of negation. In this extension information about preferences
between rules can be expressed in the logical language and derived dynamically.
This is achieved by using a reserved predicate symbol and a naming technique.
Conflicts among rules are resolved whenever possible on the basis of derived
preference information. The well-founded conclusions of prioritized logic
programs can be computed in polynomial time. A legal reasoning example
illustrates the usefulness of the approach.
"
cs.AI,Logarithmic-Time Updates and Queries in Probabilistic Networks,"  Traditional databases commonly support efficient query and update procedures
that operate in time which is sublinear in the size of the database. Our goal
in this paper is to take a first step toward dynamic reasoning in probabilistic
databases with comparable efficiency. We propose a dynamic data structure that
supports efficient algorithms for updating and querying singly connected
Bayesian networks. In the conventional algorithm, new evidence is absorbed in
O(1) time and queries are processed in time O(N), where N is the size of the
network. We propose an algorithm which, after a preprocessing phase, allows us
to answer queries in time O(log N) at the expense of O(log N) time per evidence
absorption. The usefulness of sub-linear processing time manifests itself in
applications requiring (near) real-time response over large probabilistic
databases. We briefly discuss a potential application of dynamic probabilistic
reasoning in computational biology.
"
cs.AI,Quantum Computing and Phase Transitions in Combinatorial Search,"  We introduce an algorithm for combinatorial search on quantum computers that
is capable of significantly concentrating amplitude into solutions for some NP
search problems, on average. This is done by exploiting the same aspects of
problem structure as used by classical backtrack methods to avoid unproductive
search choices. This quantum algorithm is much more likely to find solutions
than the simple direct use of quantum parallelism. Furthermore, empirical
evaluation on small problems shows this quantum algorithm displays the same
phase transition behavior, and at the same location, as seen in many previously
studied classical search methods. Specifically, difficult problem instances are
concentrated near the abrupt change from underconstrained to overconstrained
problems.
"
cs.AI,Mean Field Theory for Sigmoid Belief Networks,"  We develop a mean field theory for sigmoid belief networks based on ideas
from statistical mechanics. Our mean field theory provides a tractable
approximation to the true probability distribution in these networks; it also
yields a lower bound on the likelihood of evidence. We demonstrate the utility
of this framework on a benchmark problem in statistical pattern
recognition---the classification of handwritten digits.
"
cs.AI,Improved Use of Continuous Attributes in C4.5,"  A reported weakness of C4.5 in domains with continuous attributes is
addressed by modifying the formation and evaluation of tests on continuous
attributes. An MDL-inspired penalty is applied to such tests, eliminating some
of them from consideration and altering the relative desirability of all tests.
Empirical trials show that the modifications lead to smaller decision trees
with higher predictive accuracies. Results also confirm that a new version of
C4.5 incorporating these changes is superior to recent approaches that use
global discretization and that construct small trees with multi-interval
splits.
"
cs.AI,Active Learning with Statistical Models,"  For many types of machine learning algorithms, one can compute the
statistically `optimal' way to select training data. In this paper, we review
how optimal data selection techniques have been used with feedforward neural
networks. We then show how the same principles may be used to select data for
two alternative, statistically-based learning architectures: mixtures of
Gaussians and locally weighted regression. While the techniques for neural
networks are computationally expensive and approximate, the techniques for
mixtures of Gaussians and locally weighted regression are both efficient and
accurate. Empirically, we observe that the optimality criterion sharply
decreases the number of training examples the learner needs in order to achieve
good performance.
"
cs.AI,A Divergence Critic for Inductive Proof,"  Inductive theorem provers often diverge. This paper describes a simple
critic, a computer program which monitors the construction of inductive proofs
attempting to identify diverging proof attempts. Divergence is recognized by
means of a ``difference matching'' procedure. The critic then proposes lemmas
and generalizations which ``ripple'' these differences away so that the proof
can go through without divergence. The critic enables the theorem prover Spike
to prove many theorems completely automatically from the definitions alone.
"
cs.AI,Practical Methods for Proving Termination of General Logic Programs,"  Termination of logic programs with negated body atoms (here called general
logic programs) is an important topic. One reason is that many computational
mechanisms used to process negated atoms, like Clark's negation as failure and
Chan's constructive negation, are based on termination conditions. This paper
introduces a methodology for proving termination of general logic programs
w.r.t. the Prolog selection rule. The idea is to distinguish parts of the
program depending on whether or not their termination depends on the selection
rule. To this end, the notions of low-, weakly up-, and up-acceptable program
are introduced. We use these notions to develop a methodology for proving
termination of general logic programs, and show how interesting problems in
non-monotonic reasoning can be formalized and implemented by means of
terminating general logic programs.
"
cs.AI,Iterative Optimization and Simplification of Hierarchical Clusterings,"  Clustering is often used for discovering structure in data. Clustering
systems differ in the objective function used to evaluate clustering quality
and the control strategy used to search the space of clusterings. Ideally, the
search strategy should consistently construct clusterings of high quality, but
be computationally inexpensive as well. In general, we cannot have it both
ways, but we can partition the search so that a system inexpensively constructs
a `tentative' clustering for initial examination, followed by iterative
optimization, which continues to search in background for improved clusterings.
Given this motivation, we evaluate an inexpensive strategy for creating initial
clusterings, coupled with several control strategies for iterative
optimization, each of which repeatedly modifies an initial clustering in search
of a better one. One of these methods appears novel as an iterative
optimization strategy in clustering contexts. Once a clustering has been
constructed it is judged by analysts -- often according to task-specific
criteria. Several authors have abstracted these criteria and posited a generic
performance task akin to pattern completion, where the error rate over
completed patterns is used to `externally' judge clustering utility. Given this
performance task, we adapt resampling-based pruning strategies used by
supervised learning systems to the task of simplifying hierarchical
clusterings, thus promising to ease post-clustering analysis. Finally, we
propose a number of objective functions, based on attribute-selection measures
for decision-tree induction, that might perform well on the error rate and
simplicity dimensions.
"
cs.AI,Further Experimental Evidence against the Utility of Occam's Razor,"  This paper presents new experimental evidence against the utility of Occam's
razor. A~systematic procedure is presented for post-processing decision trees
produced by C4.5. This procedure was derived by rejecting Occam's razor and
instead attending to the assumption that similar objects are likely to belong
to the same class. It increases a decision tree's complexity without altering
the performance of that tree on the training data from which it is inferred.
The resulting more complex decision trees are demonstrated to have, on average,
for a variety of common learning tasks, higher predictive accuracy than the
less complex original decision trees. This result raises considerable doubt
about the utility of Occam's razor as it is commonly applied in modern machine
learning.
"
cs.AI,Least Generalizations and Greatest Specializations of Sets of Clauses,"  The main operations in Inductive Logic Programming (ILP) are generalization
and specialization, which only make sense in a generality order. In ILP, the
three most important generality orders are subsumption, implication and
implication relative to background knowledge. The two languages used most often
are languages of clauses and languages of only Horn clauses. This gives a total
of six different ordered languages. In this paper, we give a systematic
treatment of the existence or non-existence of least generalizations and
greatest specializations of finite sets of clauses in each of these six ordered
sets. We survey results already obtained by others and also contribute some
answers of our own. Our main new results are, firstly, the existence of a
computable least generalization under implication of every finite set of
clauses containing at least one non-tautologous function-free clause (among
other, not necessarily function-free clauses). Secondly, we show that such a
least generalization need not exist under relative implication, not even if
both the set that is to be generalized and the background knowledge are
function-free. Thirdly, we give a complete discussion of existence and
non-existence of greatest specializations in each of the six ordered languages.
"
cs.AI,Reinforcement Learning: A Survey,"  This paper surveys the field of reinforcement learning from a
computer-science perspective. It is written to be accessible to researchers
familiar with machine learning. Both the historical basis of the field and a
broad selection of current work are summarized. Reinforcement learning is the
problem faced by an agent that learns behavior through trial-and-error
interactions with a dynamic environment. The work described here has a
resemblance to work in psychology, but differs considerably in the details and
in the use of the word ``reinforcement.'' The paper discusses central issues of
reinforcement learning, including trading off exploration and exploitation,
establishing the foundations of the field via Markov decision theory, learning
from delayed reinforcement, constructing empirical models to accelerate
learning, making use of generalization and hierarchy, and coping with hidden
state. It concludes with a survey of some implemented systems and an assessment
of the practical utility of current methods for reinforcement learning.
"
cs.AI,"Adaptive Problem-solving for Large-scale Scheduling Problems: A Case
  Study","  Although most scheduling problems are NP-hard, domain specific techniques
perform well in practice but are quite expensive to construct. In adaptive
problem-solving solving, domain specific knowledge is acquired automatically
for a general problem solver with a flexible control architecture. In this
approach, a learning system explores a space of possible heuristic methods for
one well-suited to the eccentricities of the given domain and problem
distribution. In this article, we discuss an application of the approach to
scheduling satellite communications. Using problem distributions based on
actual mission requirements, our approach identifies strategies that not only
decrease the amount of CPU time required to produce schedules, but also
increase the percentage of problems that are solvable within computational
resource limitations.
"
cs.AI,A Formal Framework for Speedup Learning from Problems and Solutions,"  Speedup learning seeks to improve the computational efficiency of problem
solving with experience. In this paper, we develop a formal framework for
learning efficient problem solving from random problems and their solutions. We
apply this framework to two different representations of learned knowledge,
namely control rules and macro-operators, and prove theorems that identify
sufficient conditions for learning in each representation. Our proofs are
constructive in that they are accompanied with learning algorithms. Our
framework captures both empirical and explanation-based speedup learning in a
unified fashion. We illustrate our framework with implementations in two
domains: symbolic integration and Eight Puzzle. This work integrates many
strands of experimental and theoretical work in machine learning, including
empirical learning of control rules, macro-operator learning, Explanation-Based
Learning (EBL), and Probably Approximately Correct (PAC) Learning.
"
cs.AI,2Planning for Contingencies: A Decision-based Approach,"  A fundamental assumption made by classical AI planners is that there is no
uncertainty in the world: the planner has full knowledge of the conditions
under which the plan will be executed and the outcome of every action is fully
predictable. These planners cannot therefore construct contingency plans, i.e.,
plans in which different actions are performed in different circumstances. In
this paper we discuss some issues that arise in the representation and
construction of contingency plans and describe Cassandra, a partial-order
contingency planner. Cassandra uses explicit decision-steps that enable the
agent executing the plan to decide which plan branch to follow. The
decision-steps in a plan result in subgoals to acquire knowledge, which are
planned for in the same way as any other subgoals. Cassandra thus distinguishes
the process of gathering information from the process of making decisions. The
explicit representation of decisions in Cassandra allows a coherent approach to
the problems of contingent planning, and provides a solid base for extensions
such as the use of different decision-making procedures.
"
cs.AI,A Principled Approach Towards Symbolic Geometric Constraint Satisfaction,"  An important problem in geometric reasoning is to find the configuration of a
collection of geometric bodies so as to satisfy a set of given constraints.
Recently, it has been suggested that this problem can be solved efficiently by
symbolically reasoning about geometry. This approach, called degrees of freedom
analysis, employs a set of specialized routines called plan fragments that
specify how to change the configuration of a set of bodies to satisfy a new
constraint while preserving existing constraints. A potential drawback, which
limits the scalability of this approach, is concerned with the difficulty of
writing plan fragments. In this paper we address this limitation by showing how
these plan fragments can be automatically synthesized using first principles
about geometric bodies, actions, and topology.
"
cs.AI,On Partially Controlled Multi-Agent Systems,"  Motivated by the control theoretic distinction between controllable and
uncontrollable events, we distinguish between two types of agents within a
multi-agent system: controllable agents, which are directly controlled by the
system's designer, and uncontrollable agents, which are not under the
designer's direct control. We refer to such systems as partially controlled
multi-agent systems, and we investigate how one might influence the behavior of
the uncontrolled agents through appropriate design of the controlled agents. In
particular, we wish to understand which problems are naturally described in
these terms, what methods can be applied to influence the uncontrollable
agents, the effectiveness of such methods, and whether similar methods work
across different domains. Using a game-theoretic framework, this paper studies
the design of partially controlled multi-agent systems in two contexts: in one
context, the uncontrollable agents are expected utility maximizers, while in
the other they are reinforcement learners. We suggest different techniques for
controlling agents' behavior in each domain, assess their success, and examine
their relationship.
"
cs.AI,Spatial Aggregation: Theory and Applications,"  Visual thinking plays an important role in scientific reasoning. Based on the
research in automating diverse reasoning tasks about dynamical systems,
nonlinear controllers, kinematic mechanisms, and fluid motion, we have
identified a style of visual thinking, imagistic reasoning. Imagistic reasoning
organizes computations around image-like, analogue representations so that
perceptual and symbolic operations can be brought to bear to infer structure
and behavior. Programs incorporating imagistic reasoning have been shown to
perform at an expert level in domains that defy current analytic or numerical
methods. We have developed a computational paradigm, spatial aggregation, to
unify the description of a class of imagistic problem solvers. A program
written in this paradigm has the following properties. It takes a continuous
field and optional objective functions as input, and produces high-level
descriptions of structure, behavior, or control actions. It computes a
multi-layer of intermediate representations, called spatial aggregates, by
forming equivalence classes and adjacency relations. It employs a small set of
generic operators such as aggregation, classification, and localization to
perform bidirectional mapping between the information-rich field and
successively more abstract spatial aggregates. It uses a data structure, the
neighborhood graph, as a common interface to modularize computations. To
illustrate our theory, we describe the computational structure of three
implemented problem solvers -- KAM, MAPS, and HIPAIR --- in terms of the
spatial aggregation generic operators by mixing and matching a library of
commonly used routines.
"
cs.AI,A Hierarchy of Tractable Subsets for Computing Stable Models,"  Finding the stable models of a knowledge base is a significant computational
problem in artificial intelligence. This task is at the computational heart of
truth maintenance systems, autoepistemic logic, and default logic.
Unfortunately, it is NP-hard. In this paper we present a hierarchy of classes
of knowledge bases, Omega_1,Omega_2,..., with the following properties: first,
Omega_1 is the class of all stratified knowledge bases; second, if a knowledge
base Pi is in Omega_k, then Pi has at most k stable models, and all of them may
be found in time O(lnk), where l is the length of the knowledge base and n the
number of atoms in Pi; third, for an arbitrary knowledge base Pi, we can find
the minimum k such that Pi belongs to Omega_k in time polynomial in the size of
Pi; and, last, where K is the class of all knowledge bases, it is the case that
union{i=1 to infty} Omega_i = K, that is, every knowledge base belongs to some
class in the hierarchy.
"
cs.AI,"Accelerating Partial-Order Planners: Some Techniques for Effective
  Search Control and Pruning","  We propose some domain-independent techniques for bringing well-founded
partial-order planners closer to practicality. The first two techniques are
aimed at improving search control while keeping overhead costs low. One is
based on a simple adjustment to the default A* heuristic used by UCPOP to
select plans for refinement. The other is based on preferring ``zero
commitment'' (forced) plan refinements whenever possible, and using LIFO
prioritization otherwise. A more radical technique is the use of operator
parameter domains to prune search. These domains are initially computed from
the definitions of the operators and the initial and goal conditions, using a
polynomial-time algorithm that propagates sets of constants through the
operator graph, starting in the initial conditions. During planning, parameter
domains can be used to prune nonviable operator instances and to remove
spurious clobbering threats. In experiments based on modifications of UCPOP,
our improved plan and goal selection strategies gave speedups by factors
ranging from 5 to more than 1000 for a variety of problems that are nontrivial
for the unmodified version. Crucially, the hardest problems gave the greatest
improvements. The pruning technique based on parameter domains often gave
speedups by an order of magnitude or more for difficult problems, both with the
default UCPOP search strategy and with our improved strategy. The Lisp code for
our techniques and for the test problems is provided in on-line appendices.
"
cs.AI,Cue Phrase Classification Using Machine Learning,"  Cue phrases may be used in a discourse sense to explicitly signal discourse
structure, but also in a sentential sense to convey semantic rather than
structural information. Correctly classifying cue phrases as discourse or
sentential is critical in natural language processing systems that exploit
discourse structure, e.g., for performing tasks such as anaphora resolution and
plan recognition. This paper explores the use of machine learning for
classifying cue phrases as discourse or sentential. Two machine learning
programs (Cgrendel and C4.5) are used to induce classification models from sets
of pre-classified cue phrases and their features in text and speech. Machine
learning is shown to be an effective technique for not only automating the
generation of classification models, but also for improving upon previous
results. When compared to manually derived classification models already in the
literature, the learned models often perform with higher accuracy and contain
new linguistic insights into the data. In addition, the ability to
automatically construct classification models makes it easier to comparatively
analyze the utility of alternative feature representations of the data.
Finally, the ease of retraining makes the learning approach more scalable and
flexible than manual methods.
"
cs.AI,Mechanisms for Automated Negotiation in State Oriented Domains,"  This paper lays part of the groundwork for a domain theory of negotiation,
that is, a way of classifying interactions so that it is clear, given a domain,
which negotiation mechanisms and strategies are appropriate. We define State
Oriented Domains, a general category of interaction. Necessary and sufficient
conditions for cooperation are outlined. We use the notion of worth in an
altered definition of utility, thus enabling agreements in a wider class of
joint-goal reachable situations. An approach is offered for conflict
resolution, and it is shown that even in a conflict situation, partial
cooperative steps can be taken by interacting agents (that is, agents in
fundamental conflict might still agree to cooperate up to a certain point). A
Unified Negotiation Protocol (UNP) is developed that can be used in all types
of encounters. It is shown that in certain borderline cooperative situations, a
partial cooperative agreement (i.e., one that does not achieve all agents'
goals) might be preferred by all agents, even though there exists a rational
agreement that would achieve all their goals. Finally, we analyze cases where
agents have incomplete information on the goals and worth of other agents.
First we consider the case where agents' goals are private information, and we
analyze what goal declaration strategies the agents might adopt to increase
their utility. Then, we consider the situation where the agents' goals (and
therefore stand-alone costs) are common knowledge, but the worth they attach to
their goals is private information. We introduce two mechanisms, one 'strict',
the other 'tolerant', and analyze their affects on the stability and efficiency
of negotiation outcomes.
"
cs.AI,Learning First-Order Definitions of Functions,"  First-order learning involves finding a clause-form definition of a relation
from examples of the relation and relevant background information. In this
paper, a particular first-order learning system is modified to customize it for
finding definitions of functional relations. This restriction leads to faster
learning times and, in some cases, to definitions that have higher predictive
accuracy. Other first-order learning systems might benefit from similar
specialization.
"
cs.AI,MUSE CSP: An Extension to the Constraint Satisfaction Problem,"  This paper describes an extension to the constraint satisfaction problem
(CSP) called MUSE CSP (MUltiply SEgmented Constraint Satisfaction Problem).
This extension is especially useful for those problems which segment into
multiple sets of partially shared variables. Such problems arise naturally in
signal processing applications including computer vision, speech processing,
and handwriting recognition. For these applications, it is often difficult to
segment the data in only one way given the low-level information utilized by
the segmentation algorithms. MUSE CSP can be used to compactly represent
several similar instances of the constraint satisfaction problem. If multiple
instances of a CSP have some common variables which have the same domains and
constraints, then they can be combined into a single instance of a MUSE CSP,
reducing the work required to apply the constraints. We introduce the concepts
of MUSE node consistency, MUSE arc consistency, and MUSE path consistency. We
then demonstrate how MUSE CSP can be used to compactly represent lexically
ambiguous sentences and the multiple sentence hypotheses that are often
generated by speech recognition algorithms so that grammar constraints can be
used to provide parses for all syntactically correct sentences. Algorithms for
MUSE arc and path consistency are provided. Finally, we discuss how to create a
MUSE CSP from a set of CSPs which are labeled to indicate when the same
variable is shared by more than a single CSP.
"
cs.AI,Exploiting Causal Independence in Bayesian Network Inference,"  A new method is proposed for exploiting causal independencies in exact
Bayesian network inference. A Bayesian network can be viewed as representing a
factorization of a joint probability into the multiplication of a set of
conditional probabilities. We present a notion of causal independence that
enables one to further factorize the conditional probabilities into a
combination of even smaller factors and consequently obtain a finer-grain
factorization of the joint probability. The new formulation of causal
independence lets us specify the conditional probability of a variable given
its parents in terms of an associative and commutative operator, such as
``or'', ``sum'' or ``max'', on the contribution of each parent. We start with a
simple algorithm VE for Bayesian network inference that, given evidence and a
query variable, uses the factorization to find the posterior distribution of
the query. We show how this algorithm can be extended to exploit causal
independence. Empirical studies, based on the CPCS networks for medical
diagnosis, show that this method is more efficient than previous methods and
allows for inference in larger networks than previous algorithms.
"
cs.AI,"Quantitative Results Comparing Three Intelligent Interfaces for
  Information Capture: A Case Study Adding Name Information into an Electronic
  Personal Organizer","  Efficiently entering information into a computer is key to enjoying the
benefits of computing. This paper describes three intelligent user interfaces:
handwriting recognition, adaptive menus, and predictive fillin. In the context
of adding a personUs name and address to an electronic organizer, tests show
handwriting recognition is slower than typing on an on-screen, soft keyboard,
while adaptive menus and predictive fillin can be twice as fast. This paper
also presents strategies for applying these three interfaces to other
information collection domains.
"
cs.AI,Characterizations of Decomposable Dependency Models,"  Decomposable dependency models possess a number of interesting and useful
properties. This paper presents new characterizations of decomposable models in
terms of independence relationships, which are obtained by adding a single
axiom to the well-known set characterizing dependency models that are
isomorphic to undirected graphs. We also briefly discuss a potential
application of our results to the problem of learning graphical models from
data.
"
cs.AI,Improved Heterogeneous Distance Functions,"  Instance-based learning techniques typically handle continuous and linear
input values well, but often do not handle nominal input attributes
appropriately. The Value Difference Metric (VDM) was designed to find
reasonable distance values between nominal attribute values, but it largely
ignores continuous attributes, requiring discretization to map continuous
values into nominal values. This paper proposes three new heterogeneous
distance functions, called the Heterogeneous Value Difference Metric (HVDM),
the Interpolated Value Difference Metric (IVDM), and the Windowed Value
Difference Metric (WVDM). These new distance functions are designed to handle
applications with nominal attributes, continuous attributes, or both. In
experiments on 48 applications the new distance metrics achieve higher
classification accuracy on average than three previous distance functions on
those datasets that have both nominal and continuous attributes.
"
cs.AI,"SCREEN: Learning a Flat Syntactic and Semantic Spoken Language Analysis
  Using Artificial Neural Networks","  Previous approaches of analyzing spontaneously spoken language often have
been based on encoding syntactic and semantic knowledge manually and
symbolically. While there has been some progress using statistical or
connectionist language models, many current spoken- language systems still use
a relatively brittle, hand-coded symbolic grammar or symbolic semantic
component. In contrast, we describe a so-called screening approach for learning
robust processing of spontaneously spoken language. A screening approach is a
flat analysis which uses shallow sequences of category representations for
analyzing an utterance at various syntactic, semantic and dialog levels. Rather
than using a deeply structured symbolic analysis, we use a flat connectionist
analysis. This screening approach aims at supporting speech and language
processing by using (1) data-driven learning and (2) robustness of
connectionist networks. In order to test this approach, we have developed the
SCREEN system which is based on this new robust, learned and flat analysis. In
this paper, we focus on a detailed description of SCREEN's architecture, the
flat syntactic and semantic analysis, the interaction with a speech recognizer,
and a detailed evaluation analysis of the robustness under the influence of
noisy or incomplete input. The main result of this paper is that flat
representations allow more robust processing of spontaneous spoken language
than deeply structured representations. In particular, we show how the
fault-tolerance and learning capability of connectionist networks can support a
flat analysis for providing more robust spoken-language processing within an
overall hybrid symbolic/connectionist framework.
"
cs.AI,A Uniform Framework for Concept Definitions in Description Logics,"  Most modern formalisms used in Databases and Artificial Intelligence for
describing an application domain are based on the notions of class (or concept)
and relationship among classes. One interesting feature of such formalisms is
the possibility of defining a class, i.e., providing a set of properties that
precisely characterize the instances of the class. Many recent articles point
out that there are several ways of assigning a meaning to a class definition
containing some sort of recursion. In this paper, we argue that, instead of
choosing a single style of semantics, we achieve better results by adopting a
formalism that allows for different semantics to coexist. We demonstrate the
feasibility of our argument, by presenting a knowledge representation
formalism, the description logic muALCQ, with the above characteristics. In
addition to the constructs for conjunction, disjunction, negation, quantifiers,
and qualified number restrictions, muALCQ includes special fixpoint constructs
to express (suitably interpreted) recursive definitions. These constructs
enable the usual frame-based descriptions to be combined with definitions of
recursive data structures such as directed acyclic graphs, lists, streams, etc.
We establish several properties of muALCQ, including the decidability and the
computational complexity of reasoning, by formulating a correspondence with a
particular modal logic of programs called the modal mu-calculus.
"
cs.AI,Lifeworld Analysis,"  We argue that the analysis of agent/environment interactions should be
extended to include the conventions and invariants maintained by agents
throughout their activity. We refer to this thicker notion of environment as a
lifeworld and present a partial set of formal tools for describing structures
of lifeworlds and the ways in which they computationally simplify activity. As
one specific example, we apply the tools to the analysis of the Toast system
and show how versions of the system with very different control structures in
fact implement a common control structure together with different conventions
for encoding task state in the positions or states of objects in the
environment.
"
cs.AI,"Query DAGs: A Practical Paradigm for Implementing Belief-Network
  Inference","  We describe a new paradigm for implementing inference in belief networks,
which consists of two steps: (1) compiling a belief network into an arithmetic
expression called a Query DAG (Q-DAG); and (2) answering queries using a simple
evaluation algorithm. Each node of a Q-DAG represents a numeric operation, a
number, or a symbol for evidence. Each leaf node of a Q-DAG represents the
answer to a network query, that is, the probability of some event of interest.
It appears that Q-DAGs can be generated using any of the standard algorithms
for exact inference in belief networks (we show how they can be generated using
clustering and conditioning algorithms). The time and space complexity of a
Q-DAG generation algorithm is no worse than the time complexity of the
inference algorithm on which it is based. The complexity of a Q-DAG evaluation
algorithm is linear in the size of the Q-DAG, and such inference amounts to a
standard evaluation of the arithmetic expression it represents. The intended
value of Q-DAGs is in reducing the software and hardware resources required to
utilize belief networks in on-line, real-world applications. The proposed
framework also facilitates the development of on-line inference on different
software and hardware platforms due to the simplicity of the Q-DAG evaluation
algorithm. Interestingly enough, Q-DAGs were found to serve other purposes:
simple techniques for reducing Q-DAGs tend to subsume relatively complex
optimization techniques for belief-network inference, such as network-pruning
and computation-caching.
"
cs.AI,"Connectionist Theory Refinement: Genetically Searching the Space of
  Network Topologies","  An algorithm that learns from a set of examples should ideally be able to
exploit the available resources of (a) abundant computing power and (b)
domain-specific knowledge to improve its ability to generalize. Connectionist
theory-refinement systems, which use background knowledge to select a neural
network's topology and initial weights, have proven to be effective at
exploiting domain-specific knowledge; however, most do not exploit available
computing power. This weakness occurs because they lack the ability to refine
the topology of the neural networks they produce, thereby limiting
generalization, especially when given impoverished domain theories. We present
the REGENT algorithm which uses (a) domain-specific knowledge to help create an
initial population of knowledge-based neural networks and (b) genetic operators
of crossover and mutation (specifically designed for knowledge-based networks)
to continually search for better network topologies. Experiments on three
real-world domains indicate that our new algorithm is able to significantly
increase generalization compared to a standard connectionist theory-refinement
system, as well as our previous algorithm for growing knowledge-based networks.
"
cs.AI,Flaw Selection Strategies for Partial-Order Planning,"  Several recent studies have compared the relative efficiency of alternative
flaw selection strategies for partial-order causal link (POCL) planning. We
review this literature, and present new experimental results that generalize
the earlier work and explain some of the discrepancies in it. In particular, we
describe the Least-Cost Flaw Repair (LCFR) strategy developed and analyzed by
Joslin and Pollack (1994), and compare it with other strategies, including
Gerevini and Schubert's (1996) ZLIFO strategy. LCFR and ZLIFO make very
different, and apparently conflicting claims about the most effective way to
reduce search-space size in POCL planning. We resolve this conflict, arguing
that much of the benefit that Gerevini and Schubert ascribe to the LIFO
component of their ZLIFO strategy is better attributed to other causes. We show
that for many problems, a strategy that combines least-cost flaw selection with
the delay of separable threats will be effective in reducing search-space size,
and will do so without excessive computational overhead. Although such a
strategy thus provides a good default, we also show that certain domain
characteristics may reduce its effectiveness.
"
cs.AI,A Complete Classification of Tractability in RCC-5,"  We investigate the computational properties of the spatial algebra RCC-5
which is a restricted version of the RCC framework for spatial reasoning. The
satisfiability problem for RCC-5 is known to be NP-complete but not much is
known about its approximately four billion subclasses. We provide a complete
classification of satisfiability for all these subclasses into polynomial and
NP-complete respectively. In the process, we identify all maximal tractable
subalgebras which are four in total.
"
cs.AI,"A New Look at the Easy-Hard-Easy Pattern of Combinatorial Search
  Difficulty","  The easy-hard-easy pattern in the difficulty of combinatorial search problems
as constraints are added has been explained as due to a competition between the
decrease in number of solutions and increased pruning. We test the generality
of this explanation by examining one of its predictions: if the number of
solutions is held fixed by the choice of problems, then increased pruning
should lead to a monotonic decrease in search cost. Instead, we find the
easy-hard-easy pattern in median search cost even when the number of solutions
is held constant, for some search methods. This generalizes previous
observations of this pattern and shows that the existing theory does not
explain the full range of the peak in search cost. In these cases the pattern
appears to be due to changes in the size of the minimal unsolvable subproblems,
rather than changing numbers of solutions.
"
cs.AI,Eight Maximal Tractable Subclasses of Allen's Algebra with Metric Time,"  This paper combines two important directions of research in temporal
resoning: that of finding maximal tractable subclasses of Allen's interval
algebra, and that of reasoning with metric temporal information. Eight new
maximal tractable subclasses of Allen's interval algebra are presented, some of
them subsuming previously reported tractable algebras. The algebras allow for
metric temporal constraints on interval starting or ending points, using the
recent framework of Horn DLRs. Two of the algebras can express the notion of
sequentiality between intervals, being the first such algebras admitting both
qualitative and metric time.
"
cs.AI,"Defining Relative Likelihood in Partially-Ordered Preferential
  Structures","  Starting with a likelihood or preference order on worlds, we extend it to a
likelihood ordering on sets of worlds in a natural way, and examine the
resulting logic. Lewis earlier considered such a notion of relative likelihood
in the context of studying counterfactuals, but he assumed a total preference
order on worlds. Complications arise when examining partial orders that are not
present for total orders. There are subtleties involving the exact approach to
lifting the order on worlds to an order on sets of worlds. In addition, the
axiomatization of the logic of relative likelihood in the case of partial
orders gives insight into the connection between relative likelihood and
default reasoning.
"
cs.AI,Towards Flexible Teamwork,"  Many AI researchers are today striving to build agent teams for complex,
dynamic multi-agent domains, with intended applications in arenas such as
education, training, entertainment, information integration, and collective
robotics. Unfortunately, uncertainties in these complex, dynamic domains
obstruct coherent teamwork. In particular, team members often encounter
differing, incomplete, and possibly inconsistent views of their environment.
Furthermore, team members can unexpectedly fail in fulfilling responsibilities
or discover unexpected opportunities. Highly flexible coordination and
communication is key in addressing such uncertainties. Simply fitting
individual agents with precomputed coordination plans will not do, for their
inflexibility can cause severe failures in teamwork, and their
domain-specificity hinders reusability. Our central hypothesis is that the key
to such flexibility and reusability is providing agents with general models of
teamwork. Agents exploit such models to autonomously reason about coordination
and communication, providing requisite flexibility. Furthermore, the models
enable reuse across domains, both saving implementation effort and enforcing
consistency. This article presents one general, implemented model of teamwork,
called STEAM. The basic building block of teamwork in STEAM is joint intentions
(Cohen & Levesque, 1991b); teamwork in STEAM is based on agents' building up a
(partial) hierarchy of joint intentions (this hierarchy is seen to parallel
Grosz & Kraus's partial SharedPlans, 1996). Furthermore, in STEAM, team members
monitor the team's and individual members' performance, reorganizing the team
as necessary. Finally, decision-theoretic communication selectivity in STEAM
ensures reduction in communication overheads of teamwork, with appropriate
sensitivity to the environmental conditions. This article describes STEAM's
application in three different complex domains, and presents detailed empirical
results.
"
cs.AI,Identifying Hierarchical Structure in Sequences: A linear-time algorithm,"  SEQUITUR is an algorithm that infers a hierarchical structure from a sequence
of discrete symbols by replacing repeated phrases with a grammatical rule that
generates the phrase, and continuing this process recursively. The result is a
hierarchical representation of the original sequence, which offers insights
into its lexical structure. The algorithm is driven by two constraints that
reduce the size of the grammar, and produce structure as a by-product. SEQUITUR
breaks new ground by operating incrementally. Moreover, the method's simple
structure permits a proof that it operates in space and time that is linear in
the size of the input. Our implementation can process 50,000 symbols per second
and has been applied to an extensive range of real world sequences.
"
cs.AI,"Storing and Indexing Plan Derivations through Explanation-based Analysis
  of Retrieval Failures","  Case-Based Planning (CBP) provides a way of scaling up domain-independent
planning to solve large problems in complex domains. It replaces the detailed
and lengthy search for a solution with the retrieval and adaptation of previous
planning experiences. In general, CBP has been demonstrated to improve
performance over generative (from-scratch) planning. However, the performance
improvements it provides are dependent on adequate judgements as to problem
similarity. In particular, although CBP may substantially reduce planning
effort overall, it is subject to a mis-retrieval problem. The success of CBP
depends on these retrieval errors being relatively rare. This paper describes
the design and implementation of a replay framework for the case-based planner
DERSNLP+EBL. DERSNLP+EBL extends current CBP methodology by incorporating
explanation-based learning techniques that allow it to explain and learn from
the retrieval failures it encounters. These techniques are used to refine
judgements about case similarity in response to feedback when a wrong decision
has been made. The same failure analysis is used in building the case library,
through the addition of repairing cases. Large problems are split and stored as
single goal subproblems. Multi-goal problems are stored only when these smaller
cases fail to be merged into a full solution. An empirical evaluation of this
approach demonstrates the advantage of learning from experienced retrieval
failure.
"
cs.AI,"A Model Approximation Scheme for Planning in Partially Observable
  Stochastic Domains","  Partially observable Markov decision processes (POMDPs) are a natural model
for planning problems where effects of actions are nondeterministic and the
state of the world is not completely observable. It is difficult to solve
POMDPs exactly. This paper proposes a new approximation scheme. The basic idea
is to transform a POMDP into another one where additional information is
provided by an oracle. The oracle informs the planning agent that the current
state of the world is in a certain region. The transformed POMDP is
consequently said to be region observable. It is easier to solve than the
original POMDP. We propose to solve the transformed POMDP and use its optimal
policy to construct an approximate policy for the original POMDP. By
controlling the amount of additional information that the oracle provides, it
is possible to find a proper tradeoff between computational time and
approximation quality. In terms of algorithmic contributions, we study in
details how to exploit region observability in solving the transformed POMDP.
To facilitate the study, we also propose a new exact algorithm for general
POMDPs. The algorithm is conceptually simple and yet is significantly more
efficient than all previous exact algorithms.
"
cs.AI,Dynamic Non-Bayesian Decision Making,"  The model of a non-Bayesian agent who faces a repeated game with incomplete
information against Nature is an appropriate tool for modeling general
agent-environment interactions. In such a model the environment state
(controlled by Nature) may change arbitrarily, and the feedback/reward function
is initially unknown. The agent is not Bayesian, that is he does not form a
prior probability neither on the state selection strategy of Nature, nor on his
reward function. A policy for the agent is a function which assigns an action
to every history of observations and actions. Two basic feedback structures are
considered. In one of them -- the perfect monitoring case -- the agent is able
to observe the previous environment state as part of his feedback, while in the
other -- the imperfect monitoring case -- all that is available to the agent is
the reward obtained. Both of these settings refer to partially observable
processes, where the current environment state is unknown. Our main result
refers to the competitive ratio criterion in the perfect monitoring case. We
prove the existence of an efficient stochastic policy that ensures that the
competitive ratio is obtained at almost all stages with an arbitrarily high
probability, where efficiency is measured in terms of rate of convergence. It
is further shown that such an optimal policy does not exist in the imperfect
monitoring case. Moreover, it is proved that in the perfect monitoring case
there does not exist a deterministic policy that satisfies our long run
optimality criterion. In addition, we discuss the maxmin criterion and prove
that a deterministic efficient optimal strategy does exist in the imperfect
monitoring case under this criterion. Finally we show that our approach to
long-run optimality can be viewed as qualitative, which distinguishes it from
previous work in this area.
"
cs.AI,When Gravity Fails: Local Search Topology,"  Local search algorithms for combinatorial search problems frequently
encounter a sequence of states in which it is impossible to improve the value
of the objective function; moves through these regions, called plateau moves,
dominate the time spent in local search. We analyze and characterize plateaus
for three different classes of randomly generated Boolean Satisfiability
problems. We identify several interesting features of plateaus that impact the
performance of local search algorithms. We show that local minima tend to be
small but occasionally may be very large. We also show that local minima can be
escaped without unsatisfying a large number of clauses, but that systematically
searching for an escape route may be computationally expensive if the local
minimum is large. We show that plateaus with exits, called benches, tend to be
much larger than minima, and that some benches have very few exit states which
local search can use to escape. We show that the solutions (i.e., global
minima) of randomly generated problem instances form clusters, which behave
similarly to local minima. We revisit several enhancements of local search
algorithms and explain their performance in light of our results. Finally we
discuss strategies for creating the next generation of local search algorithms.
"
cs.AI,Bidirectional Heuristic Search Reconsidered,"  The assessment of bidirectional heuristic search has been incorrect since it
was first published more than a quarter of a century ago. For quite a long
time, this search strategy did not achieve the expected results, and there was
a major misunderstanding about the reasons behind it. Although there is still
wide-spread belief that bidirectional heuristic search is afflicted by the
problem of search frontiers passing each other, we demonstrate that this
conjecture is wrong. Based on this finding, we present both a new generic
approach to bidirectional heuristic search and a new approach to dynamically
improving heuristic values that is feasible in bidirectional search only. These
approaches are put into perspective with both the traditional and more recently
proposed approaches in order to facilitate a better overall understanding.
Empirical results of experiments with our new approaches show that
bidirectional heuristic search can be performed very efficiently and also with
limited memory. These results suggest that bidirectional heuristic search
appears to be better for solving certain difficult problems than corresponding
unidirectional search. This provides some evidence for the usefulness of a
search strategy that was long neglected. In summary, we show that bidirectional
heuristic search is viable and consequently propose that it be reconsidered.
"
cs.AI,Incremental Recompilation of Knowledge,"  Approximating a general formula from above and below by Horn formulas (its
Horn envelope and Horn core, respectively) was proposed by Selman and Kautz
(1991, 1996) as a form of ``knowledge compilation,'' supporting rapid
approximate reasoning; on the negative side, this scheme is static in that it
supports no updates, and has certain complexity drawbacks pointed out by
Kavvadias, Papadimitriou and Sideri (1993). On the other hand, the many
frameworks and schemes proposed in the literature for theory update and
revision are plagued by serious complexity-theoretic impediments, even in the
Horn case, as was pointed out by Eiter and Gottlob (1992), and is further
demonstrated in the present paper. More fundamentally, these schemes are not
inductive, in that they may lose in a single update any positive properties of
the represented sets of formulas (small size, Horn structure, etc.). In this
paper we propose a new scheme, incremental recompilation, which combines Horn
approximation and model-based updates; this scheme is inductive and very
efficient, free of the problems facing its constituents. A set of formulas is
represented by an upper and lower Horn approximation. To update, we replace the
upper Horn formula by the Horn envelope of its minimum-change update, and
similarly the lower one by the Horn core of its update; the key fact which
enables this scheme is that Horn envelopes and cores are easy to compute when
the underlying formula is the result of a minimum-change update of a Horn
formula by a clause. We conjecture that efficient algorithms are possible for
more complex updates.
"
cs.AI,Monotonicity and Persistence in Preferential Logics,"  An important characteristic of many logics for Artificial Intelligence is
their nonmonotonicity. This means that adding a formula to the premises can
invalidate some of the consequences. There may, however, exist formulae that
can always be safely added to the premises without destroying any of the
consequences: we say they respect monotonicity. Also, there may be formulae
that, when they are a consequence, can not be invalidated when adding any
formula to the premises: we call them conservative. We study these two classes
of formulae for preferential logics, and show that they are closely linked to
the formulae whose truth-value is preserved along the (preferential) ordering.
We will consider some preferential logics for illustration, and prove syntactic
characterization results for them. The results in this paper may improve the
efficiency of theorem provers for preferential logics.
"
cs.AI,Synthesizing Customized Planners from Specifications,"  Existing plan synthesis approaches in artificial intelligence fall into two
categories -- domain independent and domain dependent. The domain independent
approaches are applicable across a variety of domains, but may not be very
efficient in any one given domain. The domain dependent approaches need to be
(re)designed for each domain separately, but can be very efficient in the
domain for which they are designed. One enticing alternative to these
approaches is to automatically synthesize domain independent planners given the
knowledge about the domain and the theory of planning. In this paper, we
investigate the feasibility of using existing automated software synthesis
tools to support such synthesis. Specifically, we describe an architecture
called CLAY in which the Kestrel Interactive Development System (KIDS) is used
to derive a domain-customized planner through a semi-automatic combination of a
declarative theory of planning, and the declarative control knowledge specific
to a given domain, to semi-automatically combine them to derive
domain-customized planners. We discuss what it means to write a declarative
theory of planning and control knowledge for KIDS, and illustrate our approach
by generating a class of domain-specific planners using state space
refinements. Our experiments show that the synthesized planners can outperform
classical refinement planners (implemented as instantiations of UCP,
Kambhampati & Srivastava, 1995), using the same control knowledge. We will
contrast the costs and benefits of the synthesis approach with conventional
methods for customizing domain independent planners.
"
cs.AI,"Cached Sufficient Statistics for Efficient Machine Learning with Large
  Datasets","  This paper introduces new algorithms and data structures for quick counting
for machine learning datasets. We focus on the counting task of constructing
contingency tables, but our approach is also applicable to counting the number
of records in a dataset that match conjunctive queries. Subject to certain
assumptions, the costs of these operations can be shown to be independent of
the number of records in the dataset and loglinear in the number of non-zero
entries in the contingency table. We provide a very sparse data structure, the
ADtree, to minimize memory use. We provide analytical worst-case bounds for
this structure for several models of data distribution. We empirically
demonstrate that tractably-sized data structures can be produced for large
real-world datasets by (a) using a sparse tree structure that never allocates
memory for counts of zero, (b) never allocating memory for counts that can be
deduced from other counts, and (c) not bothering to expand the tree fully near
its leaves. We show how the ADtree can be used to accelerate Bayes net
structure finding algorithms, rule learning algorithms, and feature selection
algorithms, and we provide a number of empirical results comparing ADtree
methods against traditional direct counting approaches. We also discuss the
possible uses of ADtrees in other machine learning methods, and discuss the
merits of ADtrees in comparison with alternative representations such as
kd-trees, R-trees and Frequent Sets.
"
cs.AI,Tractability of Theory Patching,"  In this paper we consider the problem of `theory patching', in which we are
given a domain theory, some of whose components are indicated to be possibly
flawed, and a set of labeled training examples for the domain concept. The
theory patching problem is to revise only the indicated components of the
theory, such that the resulting theory correctly classifies all the training
examples. Theory patching is thus a type of theory revision in which revisions
are made to individual components of the theory. Our concern in this paper is
to determine for which classes of logical domain theories the theory patching
problem is tractable. We consider both propositional and first-order domain
theories, and show that the theory patching problem is equivalent to that of
determining what information contained in a theory is `stable' regardless of
what revisions might be performed to the theory. We show that determining
stability is tractable if the input theory satisfies two conditions: that
revisions to each theory component have monotonic effects on the classification
of examples, and that theory components act independently in the classification
of examples in the theory. We also show how the concepts introduced can be used
to determine the soundness and completeness of particular theory patching
algorithms.
"
cs.AI,Integrative Windowing,"  In this paper we re-investigate windowing for rule learning algorithms. We
show that, contrary to previous results for decision tree learning, windowing
can in fact achieve significant run-time gains in noise-free domains and
explain the different behavior of rule learning algorithms by the fact that
they learn each rule independently. The main contribution of this paper is
integrative windowing, a new type of algorithm that further exploits this
property by integrating good rules into the final theory right after they have
been discovered. Thus it avoids re-learning these rules in subsequent
iterations of the windowing process. Experimental evidence in a variety of
noise-free domains shows that integrative windowing can in fact achieve
substantial run-time gains. Furthermore, we discuss the problem of noise in
windowing and present an algorithm that is able to achieve run-time gains in a
set of experiments in a simple domain with artificial noise.
"
cs.AI,Model-Based Diagnosis using Structured System Descriptions,"  This paper presents a comprehensive approach for model-based diagnosis which
includes proposals for characterizing and computing preferred diagnoses,
assuming that the system description is augmented with a system structure (a
directed graph explicating the interconnections between system components).
Specifically, we first introduce the notion of a consequence, which is a
syntactically unconstrained propositional sentence that characterizes all
consistency-based diagnoses and show that standard characterizations of
diagnoses, such as minimal conflicts, correspond to syntactic variations on a
consequence. Second, we propose a new syntactic variation on the consequence
known as negation normal form (NNF) and discuss its merits compared to standard
variations. Third, we introduce a basic algorithm for computing consequences in
NNF given a structured system description. We show that if the system structure
does not contain cycles, then there is always a linear-size consequence in NNF
which can be computed in linear time. For arbitrary system structures, we show
a precise connection between the complexity of computing consequences and the
topology of the underlying system structure. Finally, we present an algorithm
that enumerates the preferred diagnoses characterized by a consequence. The
algorithm is shown to take linear time in the size of the consequence if the
preference criterion satisfies some general conditions.
"
cs.AI,"A Selective Macro-learning Algorithm and its Application to the NxN
  Sliding-Tile Puzzle","  One of the most common mechanisms used for speeding up problem solvers is
macro-learning. Macros are sequences of basic operators acquired during problem
solving. Macros are used by the problem solver as if they were basic operators.
The major problem that macro-learning presents is the vast number of macros
that are available for acquisition. Macros increase the branching factor of the
search space and can severely degrade problem-solving efficiency. To make macro
learning useful, a program must be selective in acquiring and utilizing macros.
This paper describes a general method for selective acquisition of macros.
Solvable training problems are generated in increasing order of difficulty. The
only macros acquired are those that take the problem solver out of a local
minimum to a better state. The utility of the method is demonstrated in several
domains, including the domain of NxN sliding-tile puzzles. After learning on
small puzzles, the system is able to efficiently solve puzzles of any size.
"
cs.LG,"Hierarchical Reinforcement Learning with the MAXQ Value Function
  Decomposition","  This paper presents the MAXQ approach to hierarchical reinforcement learning
based on decomposing the target Markov decision process (MDP) into a hierarchy
of smaller MDPs and decomposing the value function of the target MDP into an
additive combination of the value functions of the smaller MDPs. The paper
defines the MAXQ hierarchy, proves formal results on its representational
power, and establishes five conditions for the safe use of state abstractions.
The paper presents an online model-free learning algorithm, MAXQ-Q, and proves
that it converges wih probability 1 to a kind of locally-optimal policy known
as a recursively optimal policy, even in the presence of the five kinds of
state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q
through a series of experiments in three domains and shows experimentally that
MAXQ-Q (with state abstractions) converges to a recursively optimal policy much
faster than flat Q learning. The fact that MAXQ learns a representation of the
value function has an important benefit: it makes it possible to compute and
execute an improved, non-hierarchical policy via a procedure similar to the
policy improvement step of policy iteration. The paper demonstrates the
effectiveness of this non-hierarchical execution experimentally. Finally, the
paper concludes with a comparison to related work and a discussion of the
design tradeoffs in hierarchical reinforcement learning.
"
cs.LG,State Abstraction in MAXQ Hierarchical Reinforcement Learning,"  Many researchers have explored methods for hierarchical reinforcement
learning (RL) with temporal abstractions, in which abstract actions are defined
that can perform many primitive actions before terminating. However, little is
known about learning with state abstractions, in which aspects of the state
space are ignored. In previous work, we developed the MAXQ method for
hierarchical RL. In this paper, we define five conditions under which state
abstraction can be combined with the MAXQ value function decomposition. We
prove that the MAXQ-Q learning algorithm converges under these conditions and
show experimentally that state abstraction is important for the successful
application of MAXQ-Q learning.
"
cs.LG,"Multiplicative Algorithm for Orthgonal Groups and Independent Component
  Analysis","  The multiplicative Newton-like method developed by the author et al. is
extended to the situation where the dynamics is restricted to the orthogonal
group. A general framework is constructed without specifying the cost function.
Though the restriction to the orthogonal groups makes the problem somewhat
complicated, an explicit expression for the amount of individual jumps is
obtained. This algorithm is exactly second-order-convergent. The global
instability inherent in the Newton method is remedied by a
Levenberg-Marquardt-type variation. The method thus constructed can readily be
applied to the independent component analysis. Its remarkable performance is
illustrated by a numerical simulation.
"
cs.LG,Multiplicative Nonholonomic/Newton -like Algorithm,"  We construct new algorithms from scratch, which use the fourth order cumulant
of stochastic variables for the cost function. The multiplicative updating rule
here constructed is natural from the homogeneous nature of the Lie group and
has numerous merits for the rigorous treatment of the dynamics. As one
consequence, the second order convergence is shown. For the cost function,
functions invariant under the componentwise scaling are choosen. By identifying
points which can be transformed to each other by the scaling, we assume that
the dynamics is in a coset space. In our method, a point can move toward any
direction in this coset. Thus, no prewhitening is required.
"
cs.LG,Complexity analysis for algorithmically simple strings,"  Given a reference computer, Kolmogorov complexity is a well defined function
on all binary strings. In the standard approach, however, only the asymptotic
properties of such functions are considered because they do not depend on the
reference computer. We argue that this approach can be more useful if it is
refined to include an important practical case of simple binary strings.
Kolmogorov complexity calculus may be developed for this case if we restrict
the class of available reference computers. The interesting problem is to
define a class of computers which is restricted in a {\it natural} way modeling
the real-life situation where only a limited class of computers is physically
available to us. We give an example of what such a natural restriction might
look like mathematically, and show that under such restrictions some error
terms, even logarithmic in complexity, can disappear from the standard
complexity calculus.
  Keywords: Kolmogorov complexity; Algorithmic information theory.
"
cs.LG,Robust Classification for Imprecise Environments,"  In real-world environments it usually is difficult to specify target
operating conditions precisely, for example, target misclassification costs.
This uncertainty makes building robust classification systems problematic. We
show that it is possible to build a hybrid classifier that will perform at
least as well as the best available classifier for any target conditions. In
some cases, the performance of the hybrid actually can surpass that of the best
known classifier. This robust performance extends across a wide variety of
comparison frameworks, including the optimization of metrics such as accuracy,
expected cost, lift, precision, recall, and workforce utilization. The hybrid
also is efficient to build, to store, and to update. The hybrid is based on a
method for the comparison of classifier performance that is robust to imprecise
class distributions and misclassification costs. The ROC convex hull (ROCCH)
method combines techniques from ROC analysis, decision analysis and
computational geometry, and adapts them to the particulars of analyzing learned
classifiers. The method is efficient and incremental, minimizes the management
of classifier performance data, and allows for clear visual comparisons and
sensitivity analyses. Finally, we point to empirical evidence that a robust
hybrid classifier indeed is needed for many real-world problems.
"
cs.LG,Top-down induction of clustering trees,"  An approach to clustering is presented that adapts the basic top-down
induction of decision trees method towards clustering. To this aim, it employs
the principles of instance based learning. The resulting methodology is
implemented in the TIC (Top down Induction of Clustering trees) system for
first order clustering. The TIC system employs the first order logical decision
tree representation of the inductive logic programming system Tilde. Various
experiments with TIC are presented, in both propositional and relational
domains.
"
cs.LG,Scaling Up Inductive Logic Programming by Learning from Interpretations,"  When comparing inductive logic programming (ILP) and attribute-value learning
techniques, there is a trade-off between expressive power and efficiency.
Inductive logic programming techniques are typically more expressive but also
less efficient. Therefore, the data sets handled by current inductive logic
programming systems are small according to general standards within the data
mining community. The main source of inefficiency lies in the assumption that
several examples may be related to each other, so they cannot be handled
independently.
  Within the learning from interpretations framework for inductive logic
programming this assumption is unnecessary, which allows to scale up existing
ILP algorithms. In this paper we explain this learning setting in the context
of relational databases. We relate the setting to propositional data mining and
to the classical ILP setting, and show that learning from interpretations
corresponds to learning from multiple relations and thus extends the
expressiveness of propositional learning, while maintaining its efficiency to a
large extent (which is not the case in the classical ILP setting).
  As a case study, we present two alternative implementations of the ILP system
Tilde (Top-down Induction of Logical DEcision trees): Tilde-classic, which
loads all data in main memory, and Tilde-LDS, which loads the examples one by
one. We experimentally compare the implementations, showing Tilde-LDS can
handle large data sets (in the order of 100,000 examples or 100 MB) and indeed
scales up linearly in the number of examples.
"
cs.LG,Learning Policies with External Memory,"  In order for an agent to perform well in partially observable domains, it is
usually necessary for actions to depend on the history of observations. In this
paper, we explore a {\it stigmergic} approach, in which the agent's actions
include the ability to set and clear bits in an external memory, and the
external memory is included as part of the input to the agent. In this case, we
need to learn a reactive policy in a highly non-Markovian domain. We explore
two algorithms: SARSA(\lambda), which has had empirical success in partially
observable domains, and VAPS, a new algorithm due to Baird and Moore, with
convergence guarantees in partially observable domains. We compare the
performance of these two algorithms on benchmark problems.
"
cs.LG,Efficient algorithms for decision tree cross-validation,"  Cross-validation is a useful and generally applicable technique often
employed in machine learning, including decision tree induction. An important
disadvantage of straightforward implementation of the technique is its
computational overhead. In this paper we show that, for decision trees, the
computational overhead of cross-validation can be reduced significantly by
integrating the cross-validation with the normal decision tree induction
process. We discuss how existing decision tree algorithms can be adapted to
this aim, and provide an analysis of the speedups these adaptations may yield.
The analysis is supported by experimental results.
"
cs.LG,"Evaluation of the Performance of the Markov Blanket Bayesian Classifier
  Algorithm","  The Markov Blanket Bayesian Classifier is a recently-proposed algorithm for
construction of probabilistic classifiers. This paper presents an empirical
comparison of the MBBC algorithm with three other Bayesian classifiers: Naive
Bayes, Tree-Augmented Naive Bayes and a general Bayesian network. All of these
are implemented using the K2 framework of Cooper and Herskovits. The
classifiers are compared in terms of their performance (using simple accuracy
measures and ROC curves) and speed, on a range of standard benchmark data sets.
It is concluded that MBBC is competitive in terms of speed and accuracy with
the other algorithms considered.
"
cs.LG,Approximating Incomplete Kernel Matrices by the em Algorithm,"  In biological data, it is often the case that observed data are available
only for a subset of samples. When a kernel matrix is derived from such data,
we have to leave the entries for unavailable samples as missing. In this paper,
we make use of a parametric model of kernel matrices, and estimate missing
entries by fitting the model to existing entries. The parametric model is
created as a set of spectral variants of a complete kernel matrix derived from
another information source. For model fitting, we adopt the em algorithm based
on the information geometry of positive definite matrices. We will report
promising results on bacteria clustering experiments using two marker
sequences: 16S and gyrB.
"
cs.LG,"Reliable and Efficient Inference of Bayesian Networks from Sparse Data
  by Statistical Learning Theory","  To learn (statistical) dependencies among random variables requires
exponentially large sample size in the number of observed random variables if
any arbitrary joint probability distribution can occur.
  We consider the case that sparse data strongly suggest that the probabilities
can be described by a simple Bayesian network, i.e., by a graph with small
in-degree \Delta. Then this simple law will also explain further data with high
confidence. This is shown by calculating bounds on the VC dimension of the set
of those probability measures that correspond to simple graphs. This allows to
select networks by structural risk minimization and gives reliability bounds on
the error of the estimated joint measure without (in contrast to a previous
paper) any prior assumptions on the set of possible joint measures.
  The complexity for searching the optimal Bayesian networks of in-degree
\Delta increases only polynomially in the number of random varibales for
constant \Delta and the optimal joint measure associated with a given graph can
be found by convex optimization.
"
cs.LG,Toward Attribute Efficient Learning Algorithms,"  We make progress on two important problems regarding attribute efficient
learnability.
  First, we give an algorithm for learning decision lists of length $k$ over
$n$ variables using $2^{\tilde{O}(k^{1/3})} \log n$ examples and time
$n^{\tilde{O}(k^{1/3})}$. This is the first algorithm for learning decision
lists that has both subexponential sample complexity and subexponential running
time in the relevant parameters. Our approach establishes a relationship
between attribute efficient learning and polynomial threshold functions and is
based on a new construction of low degree, low weight polynomial threshold
functions for decision lists. For a wide range of parameters our construction
matches a 1994 lower bound due to Beigel for the ODDMAXBIT predicate and gives
an essentially optimal tradeoff between polynomial threshold function degree
and weight.
  Second, we give an algorithm for learning an unknown parity function on $k$
out of $n$ variables using $O(n^{1-1/k})$ examples in time polynomial in $n$.
For $k=o(\log n)$ this yields a polynomial time algorithm with sample
complexity $o(n)$. This is the first polynomial time algorithm for learning
parity on a superconstant number of variables with sublinear sample complexity.
"
cs.LG,"Improving spam filtering by combining Naive Bayes with simple k-nearest
  neighbor searches","  Using naive Bayes for email classification has become very popular within the
last few months. They are quite easy to implement and very efficient. In this
paper we want to present empirical results of email classification using a
combination of naive Bayes and k-nearest neighbor searches. Using this
technique we show that the accuracy of a Bayes filter can be improved slightly
for a high number of features and significantly for a small number of features.
"
cs.LG,About Unitary Rating Score Constructing,"  It is offered to pool test points of different subjects and different aspects
of the same subject together in order to get the unitary rating score, by the
way of nonlinear transformation of indicator points in accordance with Zipf's
distribution. It is proposed to use the well-studied distribution of
Intellectuality Quotient IQ as the reference distribution for latent variable
""progress in studies"".
"
cs.LG,"Mining Heterogeneous Multivariate Time-Series for Learning Meaningful
  Patterns: Application to Home Health Telecare","  For the last years, time-series mining has become a challenging issue for
researchers. An important application lies in most monitoring purposes, which
require analyzing large sets of time-series for learning usual patterns. Any
deviation from this learned profile is then considered as an unexpected
situation. Moreover, complex applications may involve the temporal study of
several heterogeneous parameters. In that paper, we propose a method for mining
heterogeneous multivariate time-series for learning meaningful patterns. The
proposed approach allows for mixed time-series -- containing both pattern and
non-pattern data -- such as for imprecise matches, outliers, stretching and
global translating of patterns instances in time. We present the early results
of our approach in the context of monitoring the health status of a person at
home. The purpose is to build a behavioral profile of a person by analyzing the
time variations of several quantitative or qualitative parameters recorded
through a provision of sensors installed in the home.
"
cs.LG,Stability Analysis for Regularized Least Squares Regression,"  We discuss stability for a class of learning algorithms with respect to noisy
labels. The algorithms we consider are for regression, and they involve the
minimization of regularized risk functionals, such as L(f) := 1/N sum_i
(f(x_i)-y_i)^2+ lambda ||f||_H^2. We shall call the algorithm `stable' if, when
y_i is a noisy version of f*(x_i) for some function f* in H, the output of the
algorithm converges to f* as the regularization term and noise simultaneously
vanish. We consider two flavors of this problem, one where a data set of N
points remains fixed, and the other where N -> infinity. For the case where N
-> infinity, we give conditions for convergence to f_E (the function which is
the expectation of y(x) for each x), as lambda -> 0. For the fixed N case, we
describe the limiting 'non-noisy', 'non-regularized' function f*, and give
conditions for convergence. In the process, we develop a set of tools for
dealing with functionals such as L(f), which are applicable to many other
problems in learning theory.
"
cs.LG,Probabilistic and Team PFIN-type Learning: General Properties,"  We consider the probability hierarchy for Popperian FINite learning and study
the general properties of this hierarchy. We prove that the probability
hierarchy is decidable, i.e. there exists an algorithm that receives p_1 and
p_2 and answers whether PFIN-type learning with the probability of success p_1
is equivalent to PFIN-type learning with the probability of success p_2.
  To prove our result, we analyze the topological structure of the probability
hierarchy. We prove that it is well-ordered in descending ordering and
order-equivalent to ordinal epsilon_0. This shows that the structure of the
hierarchy is very complicated.
  Using similar methods, we also prove that, for PFIN-type learning, team
learning and probabilistic learning are of the same power.
"
cs.LG,Non-asymptotic calibration and resolution,"  We analyze a new algorithm for probability forecasting of binary observations
on the basis of the available data, without making any assumptions about the
way the observations are generated. The algorithm is shown to be well
calibrated and to have good resolution for long enough sequences of
observations and for a suitable choice of its parameter, a kernel on the
Cartesian product of the forecast space $[0,1]$ and the data space. Our main
results are non-asymptotic: we establish explicit inequalities, shown to be
tight, for the performance of the algorithm.
"
cs.LG,Defensive forecasting for linear protocols,"  We consider a general class of forecasting protocols, called ""linear
protocols"", and discuss several important special cases, including multi-class
forecasting. Forecasting is formalized as a game between three players:
Reality, whose role is to generate observations; Forecaster, whose goal is to
predict the observations; and Skeptic, who tries to make money on any lack of
agreement between Forecaster's predictions and the actual observations. Our
main mathematical result is that for any continuous strategy for Skeptic in a
linear protocol there exists a strategy for Forecaster that does not allow
Skeptic's capital to grow. This result is a meta-theorem that allows one to
transform any continuous law of probability in a linear protocol into a
forecasting strategy whose predictions are guaranteed to satisfy this law. We
apply this meta-theorem to a weak law of large numbers in Hilbert spaces to
obtain a version of the K29 prediction algorithm for linear protocols and show
that this version also satisfies the attractive properties of proper
calibration and resolution under a suitable choice of its kernel parameter,
with no assumptions about the way the data is generated.
"
cs.LG,About one 3-parameter Model of Testing,"  This article offers a 3-parameter model of testing, with 1) the difference
between the ability level of the examinee and item difficulty; 2) the examinee
discrimination and 3) the item discrimination as model parameters.
"
cs.LG,On the Job Training,"  We propose a new framework for building and evaluating machine learning
algorithms. We argue that many real-world problems require an agent which must
quickly learn to respond to demands, yet can continue to perform and respond to
new training throughout its useful life. We give a framework for how such
agents can be built, describe several metrics for evaluating them, and show
that subtle changes in system construction can significantly affect agent
performance.
"
cs.LG,Multiresolution Kernels,"  We present in this work a new methodology to design kernels on data which is
structured with smaller components, such as text, images or sequences. This
methodology is a template procedure which can be applied on most kernels on
measures and takes advantage of a more detailed ""bag of components""
representation of the objects. To obtain such a detailed description, we
consider possible decompositions of the original bag into a collection of
nested bags, following a prior knowledge on the objects' structure. We then
consider these smaller bags to compare two objects both in a detailed
perspective, stressing local matches between the smaller bags, and in a global
or coarse perspective, by considering the entire bag. This multiresolution
approach is likely to be best suited for tasks where the coarse approach is not
precise enough, and where a more subtle mixture of both local and global
similarities is necessary to compare objects. The approach presented here would
not be computationally tractable without a factorization trick that we
introduce before presenting promising results on an image retrieval task.
"
cs.LG,Defensive Universal Learning with Experts,"  This paper shows how universal learning can be achieved with expert advice.
To this aim, we specify an experts algorithm with the following
characteristics: (a) it uses only feedback from the actions actually chosen
(bandit setup), (b) it can be applied with countably infinite expert classes,
and (c) it copes with losses that may grow in time appropriately slowly. We
prove loss bounds against an adaptive adversary. From this, we obtain a master
algorithm for ""reactive"" experts problems, which means that the master's
actions may influence the behavior of the adversary. Our algorithm can
significantly outperform standard experts algorithms on such problems. Finally,
we combine it with a universal expert class. The resulting universal learner
performs -- in a certain sense -- almost as well as any computable strategy,
for any online decision problem. We also specify the (worst-case) convergence
speed, which is very slow.
"
cs.LG,FPL Analysis for Adaptive Bandits,"  A main problem of ""Follow the Perturbed Leader"" strategies for online
decision problems is that regret bounds are typically proven against oblivious
adversary. In partial observation cases, it was not clear how to obtain
performance guarantees against adaptive adversary, without worsening the
bounds. We propose a conceptually simple argument to resolve this problem.
Using this, a regret bound of O(t^(2/3)) for FPL in the adversarial multi-armed
bandit problem is shown. This bound holds for the common FPL variant using only
the observations from designated exploration rounds. Using all observations
allows for the stronger bound of O(t^(1/2)), matching the best bound known so
far (and essentially the known lower bound) for adversarial bandits.
Surprisingly, this variant does not even need explicit exploration, it is
self-stabilizing. However the sampling probabilities have to be either
externally provided or approximated to sufficient accuracy, using O(t^2 log t)
samples in each step.
"
cs.LG,Learning Optimal Augmented Bayes Networks,"  Naive Bayes is a simple Bayesian classifier with strong independence
assumptions among the attributes. This classifier, desipte its strong
independence assumptions, often performs well in practice. It is believed that
relaxing the independence assumptions of a naive Bayes classifier may improve
the classification accuracy of the resulting structure. While finding an
optimal unconstrained Bayesian Network (for most any reasonable scoring
measure) is an NP-hard problem, it is possible to learn in polynomial time
optimal networks obeying various structural restrictions. Several authors have
examined the possibilities of adding augmenting arcs between attributes of a
Naive Bayes classifier. Friedman, Geiger and Goldszmidt define the TAN
structure in which the augmenting arcs form a tree on the attributes, and
present a polynomial time algorithm that learns an optimal TAN with respect to
MDL score. Keogh and Pazzani define Augmented Bayes Networks in which the
augmenting arcs form a forest on the attributes (a collection of trees, hence a
relaxation of the stuctural restriction of TAN), and present heuristic search
methods for learning good, though not optimal, augmenting arc sets. The
authors, however, evaluate the learned structure only in terms of observed
misclassification error and not against a scoring metric, such as MDL. In this
paper, we present a simple, polynomial time greedy algorithm for learning an
optimal Augmented Bayes Network with respect to MDL score.
"
cs.LG,Learning Unions of $(1)$-Dimensional Rectangles,"  We consider the problem of learning unions of rectangles over the domain
$[b]^n$, in the uniform distribution membership query learning setting, where
both b and n are ""large"". We obtain poly$(n, \log b)$-time algorithms for the
following classes:
  - poly$(n \log b)$-way Majority of $O(\frac{\log(n \log b)} {\log \log(n \log
b)})$-dimensional rectangles.
  - Union of poly$(\log(n \log b))$ many $O(\frac{\log^2 (n \log b)} {(\log
\log(n \log b) \log \log \log (n \log b))^2})$-dimensional rectangles.
  - poly$(n \log b)$-way Majority of poly$(n \log b)$-Or of disjoint
$O(\frac{\log(n \log b)} {\log \log(n \log b)})$-dimensional rectangles.
  Our main algorithmic tool is an extension of Jackson's boosting- and
Fourier-based Harmonic Sieve algorithm [Jackson 1997] to the domain $[b]^n$,
building on work of [Akavia, Goldwasser, Safra 2003]. Other ingredients used to
obtain the results stated above are techniques from exact learning [Beimel,
Kushilevitz 1998] and ideas from recent work on learning augmented $AC^{0}$
circuits [Jackson, Klivans, Servedio 2002] and on representing Boolean
functions as thresholds of parities [Klivans, Servedio 2001].
"
cs.LG,On-line regression competitive with reproducing kernel Hilbert spaces,"  We consider the problem of on-line prediction of real-valued labels, assumed
bounded in absolute value by a known constant, of new objects from known
labeled objects. The prediction algorithm's performance is measured by the
squared deviation of the predictions from the actual labels. No stochastic
assumptions are made about the way the labels and objects are generated.
Instead, we are given a benchmark class of prediction rules some of which are
hoped to produce good predictions. We show that for a wide range of
infinite-dimensional benchmark classes one can construct a prediction algorithm
whose cumulative loss over the first N examples does not exceed the cumulative
loss of any prediction rule in the class plus O(sqrt(N)); the main differences
from the known results are that we do not impose any upper bound on the norm of
the considered prediction rules and that we achieve an optimal leading term in
the excess loss of our algorithm. If the benchmark class is ""universal"" (dense
in the class of continuous functions on each compact set), this provides an
on-line non-stochastic analogue of universally consistent prediction in
non-parametric statistics. We use two proof techniques: one is based on the
Aggregating Algorithm and the other on the recently developed method of
defensive forecasting.
"
cs.LG,Bounds on Query Convergence,"  The problem of finding an optimum using noisy evaluations of a smooth cost
function arises in many contexts, including economics, business, medicine,
experiment design, and foraging theory. We derive an asymptotic bound E[ (x_t -
x*)^2 ] >= O(1/sqrt(t)) on the rate of convergence of a sequence (x_0, x_1,
>...) generated by an unbiased feedback process observing noisy evaluations of
an unknown quadratic function maximised at x*. The bound is tight, as the proof
leads to a simple algorithm which meets it. We further establish a bound on the
total regret, E[ sum_{i=1..t} (x_i - x*)^2 ] >= O(sqrt(t)) These bounds may
impose practical limitations on an agent's performance, as O(eps^-4) queries
are made before the queries converge to x* with eps accuracy.
"
cs.LG,Preference Learning in Terminology Extraction: A ROC-based approach,"  A key data preparation step in Text Mining, Term Extraction selects the
terms, or collocation of words, attached to specific concepts. In this paper,
the task of extracting relevant collocations is achieved through a supervised
learning algorithm, exploiting a few collocations manually labelled as
relevant/irrelevant. The candidate terms are described along 13 standard
statistical criteria measures. From these examples, an evolutionary learning
algorithm termed Roger, based on the optimization of the Area under the ROC
curve criterion, extracts an order on the candidate terms. The robustness of
the approach is demonstrated on two real-world domain applications, considering
different domains (biology and human resources) and different languages
(English and French).
"
cs.LG,Competing with wild prediction rules,"  We consider the problem of on-line prediction competitive with a benchmark
class of continuous but highly irregular prediction rules. It is known that if
the benchmark class is a reproducing kernel Hilbert space, there exists a
prediction algorithm whose average loss over the first N examples does not
exceed the average loss of any prediction rule in the class plus a ""regret
term"" of O(N^(-1/2)). The elements of some natural benchmark classes, however,
are so irregular that these classes are not Hilbert spaces. In this paper we
develop Banach-space methods to construct a prediction algorithm with a regret
term of O(N^(-1/p)), where p is in [2,infty) and p-2 reflects the degree to
which the benchmark class fails to be a Hilbert space.
"
cs.LG,"Genetic Programming, Validation Sets, and Parsimony Pressure","  Fitness functions based on test cases are very common in Genetic Programming
(GP). This process can be assimilated to a learning task, with the inference of
models from a limited number of samples. This paper is an investigation on two
methods to improve generalization in GP-based learning: 1) the selection of the
best-of-run individuals using a three data sets methodology, and 2) the
application of parsimony pressure in order to reduce the complexity of the
solutions. Results using GP in a binary classification setup show that while
the accuracy on the test sets is preserved, with less variances compared to
baseline results, the mean tree size obtained with the tested methods is
significantly reduced.
"
cs.LG,Processing of Test Matrices with Guessing Correction,"  It is suggested to insert into test matrix 1s for correct responses, 0s for
response refusals, and negative corrective elements for incorrect responses.
With the classical test theory approach test scores of examinees and items are
calculated traditionally as sums of matrix elements, organized in rows and
columns. Correlation coefficients are estimated using correction coefficients.
In item response theory approach examinee and item logits are estimated using
maximum likelihood method and probabilities of all matrix elements.
"
cs.LG,Learning rational stochastic languages,"  Given a finite set of words w1,...,wn independently drawn according to a
fixed unknown distribution law P called a stochastic language, an usual goal in
Grammatical Inference is to infer an estimate of P in some class of
probabilistic models, such as Probabilistic Automata (PA). Here, we study the
class of rational stochastic languages, which consists in stochastic languages
that can be generated by Multiplicity Automata (MA) and which strictly includes
the class of stochastic languages generated by PA. Rational stochastic
languages have minimal normal representation which may be very concise, and
whose parameters can be efficiently estimated from stochastic samples. We
design an efficient inference algorithm DEES which aims at building a minimal
normal representation of the target. Despite the fact that no recursively
enumerable class of MA computes exactly the set of rational stochastic
languages over Q, we show that DEES strongly identifies tis set in the limit.
We study the intermediary MA output by DEES and show that they compute rational
series which converge absolutely to one and which can be used to provide
stochastic languages which closely estimate the target.
"
cs.LG,General Discounting versus Average Reward,"  Consider an agent interacting with an environment in cycles. In every
interaction cycle the agent is rewarded for its performance. We compare the
average reward U from cycle 1 to m (average value) with the future discounted
reward V from cycle k to infinity (discounted value). We consider essentially
arbitrary (non-geometric) discount sequences and arbitrary reward sequences
(non-MDP environments). We show that asymptotically U for m->infinity and V for
k->infinity are equal, provided both limits exist. Further, if the effective
horizon grows linearly with k or faster, then existence of the limit of U
implies that the limit of V exists. Conversely, if the effective horizon grows
linearly with k or slower, then existence of the limit of V implies that the
limit of U exists.
"
cs.LG,On Sequence Prediction for Arbitrary Measures,"  Suppose we are given two probability measures on the set of one-way infinite
finite-alphabet sequences and consider the question when one of the measures
predicts the other, that is, when conditional probabilities converge (in a
certain sense) when one of the measures is chosen to generate the sequence.
This question may be considered a refinement of the problem of sequence
prediction in its most general formulation: for a given class of probability
measures, does there exist a measure which predicts all of the measures in the
class? To address this problem, we find some conditions on local absolute
continuity which are sufficient for prediction and which generalize several
different notions which are known to be sufficient for prediction. We also
formulate some open questions to outline a direction for finding the conditions
on classes of measures for which prediction is possible.
"
cs.LG,Predictions as statements and decisions,"  Prediction is a complex notion, and different predictors (such as people,
computer programs, and probabilistic theories) can pursue very different goals.
In this paper I will review some popular kinds of prediction and argue that the
theory of competitive on-line learning can benefit from the kinds of prediction
that are now foreign to it.
"
cs.LG,PAC Classification based on PAC Estimates of Label Class Distributions,"  A standard approach in pattern classification is to estimate the
distributions of the label classes, and then to apply the Bayes classifier to
the estimates of the distributions in order to classify unlabeled examples. As
one might expect, the better our estimates of the label class distributions,
the better the resulting classifier will be. In this paper we make this
observation precise by identifying risk bounds of a classifier in terms of the
quality of the estimates of the label class distributions. We show how PAC
learnability relates to estimates of the distributions that have a PAC
guarantee on their $L_1$ distance from the true distribution, and we bound the
increase in negative log likelihood risk in terms of PAC bounds on the
KL-divergence. We give an inefficient but general-purpose smoothing method for
converting an estimated distribution that is good under the $L_1$ metric into a
distribution that is good under the KL-divergence.
"
cs.LG,Competing with stationary prediction strategies,"  In this paper we introduce the class of stationary prediction strategies and
construct a prediction algorithm that asymptotically performs as well as the
best continuous stationary strategy. We make mild compactness assumptions but
no stochastic assumptions about the environment. In particular, no assumption
of stationarity is made about the environment, and the stationarity of the
considered strategies only means that they do not depend explicitly on time; we
argue that it is natural to consider only stationary strategies even for highly
non-stationary environments.
"
cs.LG,"Using Pseudo-Stochastic Rational Languages in Probabilistic Grammatical
  Inference","  In probabilistic grammatical inference, a usual goal is to infer a good
approximation of an unknown distribution P called a stochastic language. The
estimate of P stands in some class of probabilistic models such as
probabilistic automata (PA). In this paper, we focus on probabilistic models
based on multiplicity automata (MA). The stochastic languages generated by MA
are called rational stochastic languages; they strictly include stochastic
languages generated by PA; they also admit a very concise canonical
representation. Despite the fact that this class is not recursively enumerable,
it is efficiently identifiable in the limit by using the algorithm DEES,
introduced by the authors in a previous paper. However, the identification is
not proper and before the convergence of the algorithm, DEES can produce MA
that do not define stochastic languages. Nevertheless, it is possible to use
these MA to define stochastic languages. We show that they belong to a broader
class of rational series, that we call pseudo-stochastic rational languages.
The aim of this paper is twofold. First we provide a theoretical study of
pseudo-stochastic rational languages, the languages output by DEES, showing for
example that this class is decidable within polynomial time. Second, we have
carried out a lot of experiments in order to compare DEES to classical
inference algorithms such as ALERGIA and MDI. They show that DEES outperforms
them in most cases.
"
cs.LG,"Logical settings for concept learning from incomplete examples in First
  Order Logic","  We investigate here concept learning from incomplete examples. Our first
purpose is to discuss to what extent logical learning settings have to be
modified in order to cope with data incompleteness. More precisely we are
interested in extending the learning from interpretations setting introduced by
L. De Raedt that extends to relational representations the classical
propositional (or attribute-value) concept learning from examples framework. We
are inspired here by ideas presented by H. Hirsh in a work extending the
Version space inductive paradigm to incomplete data. H. Hirsh proposes to
slightly modify the notion of solution when dealing with incomplete examples: a
solution has to be a hypothesis compatible with all pieces of information
concerning the examples. We identify two main classes of incompleteness. First,
uncertainty deals with our state of knowledge concerning an example. Second,
generalization (or abstraction) deals with what part of the description of the
example is sufficient for the learning purpose. These two main sources of
incompleteness can be mixed up when only part of the useful information is
known. We discuss a general learning setting, referred to as ""learning from
possibilities"" that formalizes these ideas, then we present a more specific
learning setting, referred to as ""assumption-based learning"" that cope with
examples which uncertainty can be reduced when considering contextual
information outside of the proper description of the examples. Assumption-based
learning is illustrated on a recent work concerning the prediction of a
consensus secondary structure common to a set of RNA sequences.
"
cs.LG,"A Theory of Probabilistic Boosting, Decision Trees and Matryoshki","  We present a theory of boosting probabilistic classifiers. We place ourselves
in the situation of a user who only provides a stopping parameter and a
probabilistic weak learner/classifier and compare three types of boosting
algorithms: probabilistic Adaboost, decision tree, and tree of trees of ... of
trees, which we call matryoshka. ""Nested tree,"" ""embedded tree"" and ""recursive
tree"" are also appropriate names for this algorithm, which is one of our
contributions. Our other contribution is the theoretical analysis of the
algorithms, in which we give training error bounds. This analysis suggests that
the matryoshka leverages probabilistic weak classifiers more efficiently than
simple decision trees.
"
cs.LG,Leading strategies in competitive on-line prediction,"  We start from a simple asymptotic result for the problem of on-line
regression with the quadratic loss function: the class of continuous
limited-memory prediction strategies admits a ""leading prediction strategy"",
which not only asymptotically performs at least as well as any continuous
limited-memory strategy but also satisfies the property that the excess loss of
any continuous limited-memory strategy is determined by how closely it imitates
the leading strategy. More specifically, for any class of prediction strategies
constituting a reproducing kernel Hilbert space we construct a leading
strategy, in the sense that the loss of any prediction strategy whose norm is
not too large is determined by how closely it imitates the leading strategy.
This result is extended to the loss functions given by Bregman divergences and
by strictly proper scoring rules.
"
cs.LG,Competing with Markov prediction strategies,"  Assuming that the loss function is convex in the prediction, we construct a
prediction strategy universal for the class of Markov prediction strategies,
not necessarily continuous. Allowing randomization, we remove the requirement
of convexity.
"
cs.LG,A Study on Learnability for Rigid Lambek Grammars,"  We present basic notions of Gold's ""learnability in the limit"" paradigm,
first presented in 1967, a formalization of the cognitive process by which a
native speaker gets to grasp the underlying grammar of his/her own native
language by being exposed to well formed sentences generated by that grammar.
Then we present Lambek grammars, a formalism issued from categorial grammars
which, although not as expressive as needed for a full formalization of natural
languages, is particularly suited to easily implement a natural interface
between syntax and semantics. In the last part of this work, we present a
learnability result for Rigid Lambek grammars from structured examples.
"
cs.LG,A Massive Local Rules Search Approach to the Classification Problem,"  An approach to the classification problem of machine learning, based on
building local classification rules, is developed. The local rules are
considered as projections of the global classification rules to the event we
want to classify. A massive global optimization algorithm is used for
optimization of quality criterion. The algorithm, which has polynomial
complexity in typical case, is used to find all high--quality local rules. The
other distinctive feature of the algorithm is the integration of attributes
levels selection (for ordered attributes) with rules searching and original
conflicting rules resolution strategy. The algorithm is practical; it was
tested on a number of data sets from UCI repository, and a comparison with the
other predicting techniques is presented.
"
cs.LG,Metric entropy in competitive on-line prediction,"  Competitive on-line prediction (also known as universal prediction of
individual sequences) is a strand of learning theory avoiding making any
stochastic assumptions about the way the observations are generated. The
predictor's goal is to compete with a benchmark class of prediction rules,
which is often a proper Banach function space. Metric entropy provides a
unifying framework for competitive on-line prediction: the numerous known upper
bounds on the metric entropy of various compact sets in function spaces readily
imply bounds on the performance of on-line prediction strategies. This paper
discusses strengths and limitations of the direct approach to competitive
on-line prediction via metric entropy, including comparisons to other
approaches.
"
cs.LG,"PAC Learning Mixtures of Axis-Aligned Gaussians with No Separation
  Assumption","  We propose and analyze a new vantage point for the learning of mixtures of
Gaussians: namely, the PAC-style model of learning probability distributions
introduced by Kearns et al. Here the task is to construct a hypothesis mixture
of Gaussians that is statistically indistinguishable from the actual mixture
generating the data; specifically, the KL-divergence should be at most epsilon.
  In this scenario, we give a poly(n/epsilon)-time algorithm that learns the
class of mixtures of any constant number of axis-aligned Gaussians in
n-dimensional Euclidean space. Our algorithm makes no assumptions about the
separation between the means of the Gaussians, nor does it have any dependence
on the minimum mixing weight. This is in contrast to learning results known in
the ``clustering'' model, where such assumptions are unavoidable.
  Our algorithm relies on the method of moments, and a subalgorithm developed
in previous work by the authors (FOCS 2005) for a discrete mixture-learning
problem.
"
cs.LG,Hedging predictions in machine learning,"  Recent advances in machine learning make it possible to design efficient
prediction algorithms for data sets with huge numbers of parameters. This paper
describes a new technique for ""hedging"" the predictions output by many such
algorithms, including support vector machines, kernel ridge regression, kernel
nearest neighbours, and by many other state-of-the-art methods. The hedged
predictions for the labels of new objects include quantitative measures of
their own accuracy and reliability. These measures are provably valid under the
assumption of randomness, traditional in machine learning: the objects and
their labels are assumed to be generated independently from the same
probability distribution. In particular, it becomes possible to control (up to
statistical fluctuations) the number of erroneous predictions by selecting a
suitable confidence level. Validity being achieved automatically, the remaining
goal of hedged prediction is efficiency: taking full account of the new
objects' features and other available information to produce as accurate
predictions as possible. This can be done successfully using the powerful
machinery of modern machine learning.
"
cs.LG,"A Unified View of TD Algorithms; Introducing Full-Gradient TD and
  Equi-Gradient Descent TD","  This paper addresses the issue of policy evaluation in Markov Decision
Processes, using linear function approximation. It provides a unified view of
algorithms such as TD(lambda), LSTD(lambda), iLSTD, residual-gradient TD. It is
asserted that they all consist in minimizing a gradient function and differ by
the form of this function and their means of minimizing it. Two new schemes are
introduced in that framework: Full-gradient TD which uses a generalization of
the principle introduced in iLSTD, and EGD TD, which reduces the gradient by
successive equi-gradient descents. These three algorithms form a new
intermediate family with the interesting property of making much better use of
the samples than TD while keeping a gradient descent scheme, which is useful
for complexity issues and optimistic policy iteration.
"
cs.LG,Bandit Algorithms for Tree Search,"  Bandit based methods for tree search have recently gained popularity when
applied to huge trees, e.g. in the game of go (Gelly et al., 2006). The UCT
algorithm (Kocsis and Szepesvari, 2006), a tree search method based on Upper
Confidence Bounds (UCB) (Auer et al., 2002), is believed to adapt locally to
the effective smoothness of the tree. However, we show that UCT is too
``optimistic'' in some cases, leading to a regret O(exp(exp(D))) where D is the
depth of the tree. We propose alternative bandit algorithms for tree search.
First, a modification of UCT using a confidence sequence that scales
exponentially with the horizon depth is proven to have a regret O(2^D
\sqrt{n}), but does not adapt to possible smoothness in the tree. We then
analyze Flat-UCB performed on the leaves and provide a finite regret bound with
high probability. Then, we introduce a UCB-based Bandit Algorithm for Smooth
Trees which takes into account actual smoothness of the rewards for performing
efficient ``cuts'' of sub-optimal branches with high confidence. Finally, we
present an incremental tree search version which applies when the full tree is
too big (possibly infinite) to be entirely represented and show that with high
probability, essentially only the optimal branches is indefinitely developed.
We illustrate these methods on a global optimization problem of a Lipschitz
function, given noisy data.
"
cs.LG,Intrinsic dimension of a dataset: what properties does one expect?,"  We propose an axiomatic approach to the concept of an intrinsic dimension of
a dataset, based on a viewpoint of geometry of high-dimensional structures. Our
first axiom postulates that high values of dimension be indicative of the
presence of the curse of dimensionality (in a certain precise mathematical
sense). The second axiom requires the dimension to depend smoothly on a
distance between datasets (so that the dimension of a dataset and that of an
approximating principal manifold would be close to each other). The third axiom
is a normalization condition: the dimension of the Euclidean $n$-sphere $\s^n$
is $\Theta(n)$. We give an example of a dimension function satisfying our
axioms, even though it is in general computationally unfeasible, and discuss a
computationally cheap function satisfying most but not all of our axioms (the
``intrinsic dimensionality'' of Ch\'avez et al.)
"
cs.LG,Parametric Learning and Monte Carlo Optimization,"  This paper uncovers and explores the close relationship between Monte Carlo
Optimization of a parametrized integral (MCO), Parametric machine-Learning
(PL), and `blackbox' or `oracle'-based optimization (BO). We make four
contributions. First, we prove that MCO is mathematically identical to a broad
class of PL problems. This identity potentially provides a new application
domain for all broadly applicable PL techniques: MCO. Second, we introduce
immediate sampling, a new version of the Probability Collectives (PC) algorithm
for blackbox optimization. Immediate sampling transforms the original BO
problem into an MCO problem. Accordingly, by combining these first two
contributions, we can apply all PL techniques to BO. In our third contribution
we validate this way of improving BO by demonstrating that cross-validation and
bagging improve immediate sampling. Finally, conventional MC and MCO procedures
ignore the relationship between the sample point locations and the associated
values of the integrand; only the values of the integrand at those locations
are considered. We demonstrate that one can exploit the sample location
information using PL techniques, for example by forming a fit of the sample
locations to the associated values of the integrand. This provides an
additional way to apply PL techniques to improve MCO.
"
cs.LG,Supervised Feature Selection via Dependence Estimation,"  We introduce a framework for filtering features that employs the
Hilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence
between the features and the labels. The key idea is that good features should
maximise such dependence. Feature selection for various supervised learning
problems (including classification and regression) is unified under this
framework, and the solutions can be approximated using a backward-elimination
algorithm. We demonstrate the usefulness of our method on both artificial and
real world datasets.
"
cs.LG,"HMM Speaker Identification Using Linear and Non-linear Merging
  Techniques","  Speaker identification is a powerful, non-invasive and in-expensive biometric
technique. The recognition accuracy, however, deteriorates when noise levels
affect a specific band of frequency. In this paper, we present a sub-band based
speaker identification that intends to improve the live testing performance.
Each frequency sub-band is processed and classified independently. We also
compare the linear and non-linear merging techniques for the sub-bands
recognizer. Support vector machines and Gaussian Mixture models are the
non-linear merging techniques that are investigated. Results showed that the
sub-band based method used with linear merging techniques enormously improved
the performance of the speaker identification over the performance of wide-band
recognizers when tested live. A live testing improvement of 9.78% was achieved
"
cs.LG,"Scale-sensitive Psi-dimensions: the Capacity Measures for Classifiers
  Taking Values in R^Q","  Bounds on the risk play a crucial role in statistical learning theory. They
usually involve as capacity measure of the model studied the VC dimension or
one of its extensions. In classification, such ""VC dimensions"" exist for models
taking values in {0, 1}, {1,..., Q} and R. We introduce the generalizations
appropriate for the missing case, the one of models with values in R^Q. This
provides us with a new guaranteed risk for M-SVMs which appears superior to the
existing one.
"
cs.LG,Consistency of the group Lasso and multiple kernel learning,"  We consider the least-square regression problem with regularization by a
block 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger
than one. This problem, referred to as the group Lasso, extends the usual
regularization by the 1-norm where all spaces have dimension one, where it is
commonly referred to as the Lasso. In this paper, we study the asymptotic model
consistency of the group Lasso. We derive necessary and sufficient conditions
for the consistency of group Lasso under practical assumptions, such as model
misspecification. When the linear predictors and Euclidean norms are replaced
by functions and reproducing kernel Hilbert norms, the problem is usually
referred to as multiple kernel learning and is commonly used for learning from
heterogeneous data sources and for non linear variable selection. Using tools
from functional analysis, and in particular covariance operators, we extend the
consistency results to this infinite dimensional case and also propose an
adaptive scheme to obtain a consistent model estimate, even when the necessary
condition required for the non adaptive scheme is not satisfied.
"
cs.LG,"Cost-minimising strategies for data labelling : optimal stopping and
  active learning","  Supervised learning deals with the inference of a distribution over an output
or label space $\CY$ conditioned on points in an observation space $\CX$, given
a training dataset $D$ of pairs in $\CX \times \CY$. However, in a lot of
applications of interest, acquisition of large amounts of observations is easy,
while the process of generating labels is time-consuming or costly. One way to
deal with this problem is {\em active} learning, where points to be labelled
are selected with the aim of creating a model with better performance than that
of an model trained on an equal number of randomly sampled points. In this
paper, we instead propose to deal with the labelling cost directly: The
learning goal is defined as the minimisation of a cost which is a function of
the expected model performance and the total cost of the labels used. This
allows the development of general strategies and specific algorithms for (a)
optimal stopping, where the expected cost dictates whether label acquisition
should continue (b) empirical evaluation, where the cost is used as a
performance metric for a given combination of inference, stopping and sampling
methods. Though the main focus of the paper is optimal stopping, we also aim to
provide the background for further developments and discussion in the related
field of active learning.
"
cs.LG,Defensive forecasting for optimal prediction with expert advice,"  The method of defensive forecasting is applied to the problem of prediction
with expert advice for binary outcomes. It turns out that defensive forecasting
is not only competitive with the Aggregating Algorithm but also handles the
case of ""second-guessing"" experts, whose advice depends on the learner's
prediction; this paper assumes that the dependence on the learner's prediction
is continuous.
"
cs.LG,Continuous and randomized defensive forecasting: unified view,"  Defensive forecasting is a method of transforming laws of probability (stated
in game-theoretic terms as strategies for Sceptic) into forecasting algorithms.
There are two known varieties of defensive forecasting: ""continuous"", in which
Sceptic's moves are assumed to depend on the forecasts in a (semi)continuous
manner and which produces deterministic forecasts, and ""randomized"", in which
the dependence of Sceptic's moves on the forecasts is arbitrary and
Forecaster's moves are allowed to be randomized. This note shows that the
randomized variety can be obtained from the continuous variety by smearing
Sceptic's moves to make them continuous.
"
cs.LG,Filtering Additive Measurement Noise with Maximum Entropy in the Mean,"  The purpose of this note is to show how the method of maximum entropy in the
mean (MEM) may be used to improve parametric estimation when the measurements
are corrupted by large level of noise. The method is developed in the context
on a concrete example: that of estimation of the parameter in an exponential
distribution. We compare the performance of our method with the bayesian and
maximum likelihood approaches.
"
cs.LG,Prediction with expert advice for the Brier game,"  We show that the Brier game of prediction is mixable and find the optimal
learning rate and substitution function for it. The resulting prediction
algorithm is applied to predict results of football and tennis matches. The
theoretical performance guarantee turns out to be rather tight on these data
sets, especially in the case of the more extensive tennis data.
"
cs.LG,Consistency of trace norm minimization,"  Regularization by the sum of singular values, also referred to as the trace
norm, is a popular technique for estimating low rank rectangular matrices. In
this paper, we extend some of the consistency results of the Lasso to provide
necessary and sufficient conditions for rank consistency of trace norm
minimization with the square loss. We also provide an adaptive version that is
rank consistent even when the necessary condition for the non adaptive version
is not fulfilled.
"
cs.LG,Clustering with Transitive Distance and K-Means Duality,"  Recent spectral clustering methods are a propular and powerful technique for
data clustering. These methods need to solve the eigenproblem whose
computational complexity is $O(n^3)$, where $n$ is the number of data samples.
In this paper, a non-eigenproblem based clustering method is proposed to deal
with the clustering problem. Its performance is comparable to the spectral
clustering algorithms but it is more efficient with computational complexity
$O(n^2)$. We show that with a transitive distance and an observed property,
called K-means duality, our algorithm can be used to handle data sets with
complex cluster shapes, multi-scale clusters, and noise. Moreover, no
parameters except the number of clusters need to be set in our algorithm.
"
cs.LG,Covariance and PCA for Categorical Variables,"  Covariances from categorical variables are defined using a regular simplex
expression for categories. The method follows the variance definition by Gini,
and it gives the covariance as a solution of simultaneous equations. The
calculated results give reasonable values for test data. A method of principal
component analysis (RS-PCA) is also proposed using regular simplex expressions,
which allows easy interpretation of the principal components. The proposed
methods apply to variable selection problem of categorical data USCensus1990
data. The proposed methods give appropriate criterion for the variable
selection problem of categorical
"
cs.LG,On the Relationship between the Posterior and Optimal Similarity,"  For a classification problem described by the joint density $P(\omega,x)$,
models of $P(\omega\eq\omega'|x,x')$ (the ``Bayesian similarity measure'') have
been shown to be an optimal similarity measure for nearest neighbor
classification. This paper analyzes demonstrates several additional properties
of that conditional distribution. The paper first shows that we can
reconstruct, up to class labels, the class posterior distribution $P(\omega|x)$
given $P(\omega\eq\omega'|x,x')$, gives a procedure for recovering the class
labels, and gives an asymptotically Bayes-optimal classification procedure. It
also shows, given such an optimal similarity measure, how to construct a
classifier that outperforms the nearest neighbor classifier and achieves
Bayes-optimal classification rates. The paper then analyzes Bayesian similarity
in a framework where a classifier faces a number of related classification
tasks (multitask learning) and illustrates that reconstruction of the class
posterior distribution is not possible in general. Finally, the paper
identifies a distinct class of classification problems using
$P(\omega\eq\omega'|x,x')$ and shows that using $P(\omega\eq\omega'|x,x')$ to
solve those problems is the Bayes optimal solution.
"
cs.LG,Equations of States in Singular Statistical Estimation,"  Learning machines which have hierarchical structures or hidden variables are
singular statistical models because they are nonidentifiable and their Fisher
information matrices are singular. In singular statistical models, neither the
Bayes a posteriori distribution converges to the normal distribution nor the
maximum likelihood estimator satisfies asymptotic normality. This is the main
reason why it has been difficult to predict their generalization performances
from trained states. In this paper, we study four errors, (1) Bayes
generalization error, (2) Bayes training error, (3) Gibbs generalization error,
and (4) Gibbs training error, and prove that there are mathematical relations
among these errors. The formulas proved in this paper are equations of states
in statistical estimation because they hold for any true distribution, any
parametric model, and any a priori distribution. Also we show that Bayes and
Gibbs generalization errors are estimated by Bayes and Gibbs training errors,
and propose widely applicable information criteria which can be applied to both
regular and singular statistical models.
"
cs.LG,Density estimation in linear time,"  We consider the problem of choosing a density estimate from a set of
distributions F, minimizing the L1-distance to an unknown distribution
(Devroye, Lugosi 2001). Devroye and Lugosi analyze two algorithms for the
problem: Scheffe tournament winner and minimum distance estimate. The Scheffe
tournament estimate requires fewer computations than the minimum distance
estimate, but has strictly weaker guarantees than the latter.
  We focus on the computational aspect of density estimation. We present two
algorithms, both with the same guarantee as the minimum distance estimate. The
first one, a modification of the minimum distance estimate, uses the same
number (quadratic in |F|) of computations as the Scheffe tournament. The second
one, called ``efficient minimum loss-weight estimate,'' uses only a linear
number of computations, assuming that F is preprocessed.
  We also give examples showing that the guarantees of the algorithms cannot be
improved and explore randomized algorithms for density estimation.
"
cs.LG,Graph kernels between point clouds,"  Point clouds are sets of points in two or three dimensions. Most kernel
methods for learning on sets of points have not yet dealt with the specific
geometrical invariances and practical constraints associated with point clouds
in computer vision and graphics. In this paper, we present extensions of graph
kernels for point clouds, which allow to use kernel methods for such ob jects
as shapes, line drawings, or any three-dimensional point clouds. In order to
design rich and numerically efficient kernels with as few free parameters as
possible, we use kernels between covariance matrices and their factorizations
on graphical models. We derive polynomial time dynamic programming recursions
and present applications to recognition of handwritten digits and Chinese
characters from few training examples.
"
cs.LG,Online variants of the cross-entropy method,"  The cross-entropy method is a simple but efficient method for global
optimization. In this paper we provide two online variants of the basic CEM,
together with a proof of convergence.
"
cs.LG,The optimal assignment kernel is not positive definite,"  We prove that the optimal assignment kernel, proposed recently as an attempt
to embed labeled graphs and more generally tuples of basic data to a Hilbert
space, is in fact not always positive definite.
"
cs.LG,New Estimation Procedures for PLS Path Modelling,"  Given R groups of numerical variables X1, ... XR, we assume that each group
is the result of one underlying latent variable, and that all latent variables
are bound together through a linear equation system. Moreover, we assume that
some explanatory latent variables may interact pairwise in one or more
equations. We basically consider PLS Path Modelling's algorithm to estimate
both latent variables and the model's coefficients. New ""external"" estimation
schemes are proposed that draw latent variables towards strong group structures
in a more flexible way. New ""internal"" estimation schemes are proposed to
enable PLSPM to make good use of variable group complementarity and to deal
with interactions. Application examples are given.
"
cs.LG,"A New Approach to Collaborative Filtering: Operator Estimation with
  Spectral Regularization","  We present a general approach for collaborative filtering (CF) using spectral
regularization to learn linear operators from ""users"" to the ""objects"" they
rate. Recent low-rank type matrix completion approaches to CF are shown to be
special cases. However, unlike existing regularization based CF methods, our
approach can be used to also incorporate information such as attributes of the
users or the objects -- a limitation of existing regularization based CF
methods. We then provide novel representer theorems that we use to develop new
estimation methods. We provide learning algorithms based on low-rank
decompositions, and test them on a standard CF dataset. The experiments
indicate the advantages of generalizing the existing regularization based CF
methods to incorporate related information about users and objects. Finally, we
show that certain multi-task learning methods can be also seen as special cases
of our proposed approach.
"
cs.LG,Multiple Random Oracles Are Better Than One,"  We study the problem of learning k-juntas given access to examples drawn from
a number of different product distributions. Thus we wish to learn a function f
: {-1,1}^n -> {-1,1} that depends on k (unknown) coordinates. While the best
known algorithms for the general problem of learning a k-junta require running
time of n^k * poly(n,2^k), we show that given access to k different product
distributions with biases separated by \gamma>0, the functions may be learned
in time poly(n,2^k,\gamma^{-k}). More generally, given access to t <= k
different product distributions, the functions may be learned in time n^{k/t} *
poly(n,2^k,\gamma^{-k}). Our techniques involve novel results in Fourier
analysis relating Fourier expansions with respect to different biases and a
generalization of Russo's formula.
"
cs.LG,Introduction to Relational Networks for Classification,"  The use of computational intelligence techniques for classification has been
used in numerous applications. This paper compares the use of a Multi Layer
Perceptron Neural Network and a new Relational Network on classifying the HIV
status of women at ante-natal clinics. The paper discusses the architecture of
the relational network and its merits compared to a neural network and most
other computational intelligence classifiers. Results gathered from the study
indicate comparable classification accuracies as well as revealed relationships
between data features in the classification data. Much higher classification
accuracies are recommended for future research in the area of HIV
classification as well as missing data estimation.
"
cs.LG,"The Effect of Structural Diversity of an Ensemble of Classifiers on
  Classification Accuracy","  This paper aims to showcase the measure of structural diversity of an
ensemble of 9 classifiers and then map a relationship between this structural
diversity and accuracy. The structural diversity was induced by having
different architectures or structures of the classifiers The Genetical
Algorithms (GA) were used to derive the relationship between diversity and the
classification accuracy by evolving the classifiers and then picking 9
classifiers out on an ensemble of 60 classifiers. It was found that as the
ensemble became diverse the accuracy improved. However at a certain diversity
measure the accuracy began to drop. The Kohavi-Wolpert variance method is used
to measure the diversity of the ensemble. A method of voting is used to
aggregate the results from each classifier. The lowest error was observed at a
diversity measure of 0.16 with a mean square error of 0.274, when taking 0.2024
as maximum diversity measured. The parameters that were varied were: the number
of hidden nodes, learning rate and the activation function.
"
cs.LG,A Quadratic Loss Multi-Class SVM,"  Using a support vector machine requires to set two types of hyperparameters:
the soft margin parameter C and the parameters of the kernel. To perform this
model selection task, the method of choice is cross-validation. Its
leave-one-out variant is known to produce an estimator of the generalization
error which is almost unbiased. Its major drawback rests in its time
requirement. To overcome this difficulty, several upper bounds on the
leave-one-out error of the pattern recognition SVM have been derived. Among
those bounds, the most popular one is probably the radius-margin bound. It
applies to the hard margin pattern recognition SVM, and by extension to the
2-norm SVM. In this report, we introduce a quadratic loss M-SVM, the M-SVM^2,
as a direct extension of the 2-norm SVM to the multi-class case. For this
machine, a generalized radius-margin bound is then established.
"
cs.LG,On Recovery of Sparse Signals via $\ell_1$ Minimization,"  This article considers constrained $\ell_1$ minimization methods for the
recovery of high dimensional sparse signals in three settings: noiseless,
bounded error and Gaussian noise. A unified and elementary treatment is given
in these noise settings for two $\ell_1$ minimization methods: the Dantzig
selector and $\ell_1$ minimization with an $\ell_2$ constraint. The results of
this paper improve the existing results in the literature by weakening the
conditions and tightening the error bounds. The improvement on the conditions
shows that signals with larger support can be recovered accurately. This paper
also establishes connections between restricted isometry property and the
mutual incoherence property. Some results of Candes, Romberg and Tao (2006) and
Donoho, Elad, and Temlyakov (2006) are extended.
"
cs.LG,The Margitron: A Generalised Perceptron with Margin,"  We identify the classical Perceptron algorithm with margin as a member of a
broader family of large margin classifiers which we collectively call the
Margitron. The Margitron, (despite its) sharing the same update rule with the
Perceptron, is shown in an incremental setting to converge in a finite number
of updates to solutions possessing any desirable fraction of the maximum
margin. Experiments comparing the Margitron with decomposition SVMs on tasks
involving linear kernels and 2-norm soft margin are also reported.
"
cs.LG,Sample Selection Bias Correction Theory,"  This paper presents a theoretical analysis of sample selection bias
correction. The sample bias correction technique commonly used in machine
learning consists of reweighting the cost of an error on each training point of
a biased sample to more closely reflect the unbiased distribution. This relies
on weights derived by various estimation techniques based on finite samples. We
analyze the effect of an error in that estimation on the accuracy of the
hypothesis returned by the learning algorithm for two estimation techniques: a
cluster-based estimation technique and kernel mean matching. We also report the
results of sample bias correction experiments with several data sets using
these techniques. Our analysis is based on the novel concept of distributional
stability which generalizes the existing concept of point-based stability. Much
of our work and proof techniques can be used to analyze other importance
weighting techniques and their effect on accuracy when using a distributionally
stable algorithm.
"
cs.LG,From Data Topology to a Modular Classifier,"  This article describes an approach to designing a distributed and modular
neural classifier. This approach introduces a new hierarchical clustering that
enables one to determine reliable regions in the representation space by
exploiting supervised information. A multilayer perceptron is then associated
with each of these detected clusters and charged with recognizing elements of
the associated cluster while rejecting all others. The obtained global
classifier is comprised of a set of cooperating neural networks and completed
by a K-nearest neighbor classifier charged with treating elements rejected by
all the neural networks. Experimental results for the handwritten digit
recognition problem and comparison with neural and statistical nonmodular
classifiers are given.
"
cs.LG,"Utilisation des grammaires probabilistes dans les tches de
  segmentation et d'annotation prosodique","  Nous pr\'esentons dans cette contribution une approche \`a la fois symbolique
et probabiliste permettant d'extraire l'information sur la segmentation du
signal de parole \`a partir d'information prosodique. Nous utilisons pour ce
faire des grammaires probabilistes poss\'edant une structure hi\'erarchique
minimale. La phase de construction des grammaires ainsi que leur pouvoir de
pr\'ediction sont \'evalu\'es qualitativement ainsi que quantitativement.
  -----
  Methodologically oriented, the present work sketches an approach for prosodic
information retrieval and speech segmentation, based on both symbolic and
probabilistic information. We have recourse to probabilistic grammars, within
which we implement a minimal hierarchical structure. Both the stages of
probabilistic grammar building and its testing in prediction are explored and
quantitatively and qualitatively evaluated.
"
cs.LG,Statistical Learning of Arbitrary Computable Classifiers,"  Statistical learning theory chiefly studies restricted hypothesis classes,
particularly those with finite Vapnik-Chervonenkis (VC) dimension. The
fundamental quantity of interest is the sample complexity: the number of
samples required to learn to a specified level of accuracy. Here we consider
learning over the set of all computable labeling functions. Since the
VC-dimension is infinite and a priori (uniform) bounds on the number of samples
are impossible, we let the learning algorithm decide when it has seen
sufficient samples to have learned. We first show that learning in this setting
is indeed possible, and develop a learning algorithm. We then show, however,
that bounding sample complexity independently of the distribution is
impossible. Notably, this impossibility is entirely due to the requirement that
the learning algorithm be computable, and not due to the statistical nature of
the problem.
"
cs.LG,Agnostically Learning Juntas from Random Walks,"  We prove that the class of functions g:{-1,+1}^n -> {-1,+1} that only depend
on an unknown subset of k<<n variables (so-called k-juntas) is agnostically
learnable from a random walk in time polynomial in n, 2^{k^2}, epsilon^{-k},
and log(1/delta). In other words, there is an algorithm with the claimed
running time that, given epsilon, delta > 0 and access to a random walk on
{-1,+1}^n labeled by an arbitrary function f:{-1,+1}^n -> {-1,+1}, finds with
probability at least 1-delta a k-junta that is (opt(f)+epsilon)-close to f,
where opt(f) denotes the distance of a closest k-junta to f.
"
cs.LG,"Computationally Efficient Estimators for Dimension Reductions Using
  Stable Random Projections","  The method of stable random projections is a tool for efficiently computing
the $l_\alpha$ distances using low memory, where $0<\alpha \leq 2$ is a tuning
parameter. The method boils down to a statistical estimation task and various
estimators have been proposed, based on the geometric mean, the harmonic mean,
and the fractional power etc.
  This study proposes the optimal quantile estimator, whose main operation is
selecting, which is considerably less expensive than taking fractional power,
the main operation in previous estimators. Our experiments report that the
optimal quantile estimator is nearly one order of magnitude more
computationally efficient than previous estimators. For large-scale learning
tasks in which storing and computing pairwise distances is a serious
bottleneck, this estimator should be desirable.
  In addition to its computational advantages, the optimal quantile estimator
exhibits nice theoretical properties. It is more accurate than previous
estimators when $\alpha>1$. We derive its theoretical error bounds and
establish the explicit (i.e., no hidden constants) sample complexity bound.
"
cs.LG,On Approximating the Lp Distances for p>2,"  Applications in machine learning and data mining require computing pairwise
Lp distances in a data matrix A. For massive high-dimensional data, computing
all pairwise distances of A can be infeasible. In fact, even storing A or all
pairwise distances of A in the memory may be also infeasible. This paper
proposes a simple method for p = 2, 4, 6, ... We first decompose the l_p (where
p is even) distances into a sum of 2 marginal norms and p-1 ``inner products''
at different orders. Then we apply normal or sub-Gaussian random projections to
approximate the resultant ``inner products,'' assuming that the marginal norms
can be computed exactly by a linear scan. We propose two strategies for
applying random projections. The basic projection strategy requires only one
projection matrix but it is more difficult to analyze, while the alternative
projection strategy requires p-1 projection matrices but its theoretical
analysis is much easier. In terms of the accuracy, at least for p=4, the basic
strategy is always more accurate than the alternative strategy if the data are
non-negative, which is common in reality.
"
cs.LG,Graph Kernels,"  We present a unified framework to study graph kernels, special cases of which
include the random walk graph kernel \citep{GaeFlaWro03,BorOngSchVisetal05},
marginalized graph kernel \citep{KasTsuIno03,KasTsuIno04,MahUedAkuPeretal04},
and geometric kernel on graphs \citep{Gaertner02}. Through extensions of linear
algebra to Reproducing Kernel Hilbert Spaces (RKHS) and reduction to a
Sylvester equation, we construct an algorithm that improves the time complexity
of kernel computation from $O(n^6)$ to $O(n^3)$. When the graphs are sparse,
conjugate gradient solvers or fixed-point iterations bring our algorithm into
the sub-cubic domain. Experiments on graphs from bioinformatics and other
application domains show that it is often more than a thousand times faster
than previous approaches. We then explore connections between diffusion kernels
\citep{KonLaf02}, regularization on graphs \citep{SmoKon03}, and graph kernels,
and use these connections to propose new graph kernels. Finally, we show that
rational kernels \citep{CorHafMoh02,CorHafMoh03,CorHafMoh04} when specialized
to graphs reduce to the random walk graph kernel.
"
cs.LG,"On Probability Distributions for Trees: Representations, Inference and
  Learning","  We study probability distributions over free algebras of trees. Probability
distributions can be seen as particular (formal power) tree series [Berstel et
al 82, Esik et al 03], i.e. mappings from trees to a semiring K . A widely
studied class of tree series is the class of rational (or recognizable) tree
series which can be defined either in an algebraic way or by means of
multiplicity tree automata. We argue that the algebraic representation is very
convenient to model probability distributions over a free algebra of trees.
First, as in the string case, the algebraic representation allows to design
learning algorithms for the whole class of probability distributions defined by
rational tree series. Note that learning algorithms for rational tree series
correspond to learning algorithms for weighted tree automata where both the
structure and the weights are learned. Second, the algebraic representation can
be easily extended to deal with unranked trees (like XML trees where a symbol
may have an unbounded number of children). Both properties are particularly
relevant for applications: nondeterministic automata are required for the
inference problem to be relevant (recall that Hidden Markov Models are
equivalent to nondeterministic string automata); nowadays applications for Web
Information Extraction, Web Services and document processing consider unranked
trees.
"
cs.LG,"Positive factor networks: A graphical framework for modeling
  non-negative sequential data","  We present a novel graphical framework for modeling non-negative sequential
data with hierarchical structure. Our model corresponds to a network of coupled
non-negative matrix factorization (NMF) modules, which we refer to as a
positive factor network (PFN). The data model is linear, subject to
non-negativity constraints, so that observation data consisting of an additive
combination of individually representable observations is also representable by
the network. This is a desirable property for modeling problems in
computational auditory scene analysis, since distinct sound sources in the
environment are often well-modeled as combining additively in the corresponding
magnitude spectrogram. We propose inference and learning algorithms that
leverage existing NMF algorithms and that are straightforward to implement. We
present a target tracking example and provide results for synthetic observation
data which serve to illustrate the interesting properties of PFNs and motivate
their potential usefulness in applications such as music transcription, source
separation, and speech recognition. We show how a target process characterized
by a hierarchical state transition model can be represented as a PFN. Our
results illustrate that a PFN which is defined in terms of a single target
observation can then be used to effectively track the states of multiple
simultaneous targets. Our results show that the quality of the inferred target
states degrades gradually as the observation noise is increased. We also
present results for an example in which meaningful hierarchical features are
extracted from a spectrogram. Such a hierarchical representation could be
useful for music transcription and source separation applications. We also
propose a network for language modeling.
"
cs.LG,When is there a representer theorem? Vector versus matrix regularizers,"  We consider a general class of regularization methods which learn a vector of
parameters on the basis of linear measurements. It is well known that if the
regularizer is a nondecreasing function of the inner product then the learned
vector is a linear combination of the input data. This result, known as the
{\em representer theorem}, is at the basis of kernel-based methods in machine
learning. In this paper, we prove the necessity of the above condition, thereby
completing the characterization of kernel methods based on regularization. We
further extend our analysis to regularization methods which learn a matrix, a
problem which is motivated by the application to multi-task learning. In this
context, we study a more general representer theorem, which holds for a larger
class of regularizers. We provide a necessary and sufficient condition for
these class of matrix regularizers and highlight them with some concrete
examples of practical importance. Our analysis uses basic principles from
matrix theory, especially the useful notion of matrix nondecreasing function.
"
cs.LG,Clustered Multi-Task Learning: A Convex Formulation,"  In multi-task learning several related tasks are considered simultaneously,
with the hope that by an appropriate sharing of information across tasks, each
task may benefit from the others. In the context of learning linear functions
for supervised classification or regression, this can be achieved by including
a priori information about the weight vectors associated with the tasks, and
how they are expected to be related to each other. In this paper, we assume
that tasks are clustered into groups, which are unknown beforehand, and that
tasks within a group have similar weight vectors. We design a new spectral norm
that encodes this a priori assumption, without the prior knowledge of the
partition of tasks into groups, resulting in a new convex optimization
formulation for multi-task learning. We show in simulations on synthetic
examples and on the IEDB MHC-I binding dataset, that our approach outperforms
well-known convex methods for multi-task learning, as well as related non
convex methods dedicated to the same problem.
"
cs.LG,Surrogate Learning - An Approach for Semi-Supervised Classification,"  We consider the task of learning a classifier from the feature space
$\mathcal{X}$ to the set of classes $\mathcal{Y} = \{0, 1\}$, when the features
can be partitioned into class-conditionally independent feature sets
$\mathcal{X}_1$ and $\mathcal{X}_2$. We show the surprising fact that the
class-conditional independence can be used to represent the original learning
task in terms of 1) learning a classifier from $\mathcal{X}_2$ to
$\mathcal{X}_1$ and 2) learning the class-conditional distribution of the
feature set $\mathcal{X}_1$. This fact can be exploited for semi-supervised
learning because the former task can be accomplished purely from unlabeled
samples. We present experimental evaluation of the idea in two real world
applications.
"
cs.LG,Learning Isometric Separation Maps,"  Maximum Variance Unfolding (MVU) and its variants have been very successful
in embedding data-manifolds in lower dimensional spaces, often revealing the
true intrinsic dimension. In this paper we show how to also incorporate
supervised class information into an MVU-like method without breaking its
convexity. We call this method the Isometric Separation Map and we show that
the resulting kernel matrix can be used as a binary/multiclass Support Vector
Machine-like method in a semi-supervised (transductive) framework. We also show
that the method always finds a kernel matrix that linearly separates the
training data exactly without projecting them in infinite dimensional spaces.
In traditional SVMs we choose a kernel and hope that the data become linearly
separable in the kernel space. In this paper we show how the hyperplane can be
chosen ad-hoc and the kernel is trained so that data are always linearly
separable. Comparisons with Large Margin SVMs show comparable performance.
"
cs.LG,"Entropy, Perception, and Relativity","  In this paper, I expand Shannon's definition of entropy into a new form of
entropy that allows integration of information from different random events.
Shannon's notion of entropy is a special case of my more general definition of
entropy. I define probability using a so-called performance function, which is
de facto an exponential distribution. Assuming that my general notion of
entropy reflects the true uncertainty about a probabilistic event, I understand
that our perceived uncertainty differs. I claim that our perception is the
result of two opposing forces similar to the two famous antagonists in Chinese
philosophy: Yin and Yang. Based on this idea, I show that our perceived
uncertainty matches the true uncertainty in points determined by the golden
ratio. I demonstrate that the well-known sigmoid function, which we typically
employ in artificial neural networks as a non-linear threshold function,
describes the actual performance. Furthermore, I provide a motivation for the
time dilation in Einstein's Special Relativity, basically claiming that
although time dilation conforms with our perception, it does not correspond to
reality. At the end of the paper, I show how to apply this theoretical
framework to practical applications. I present recognition rates for a pattern
recognition problem, and also propose a network architecture that can take
advantage of general entropy to solve complex decision problems.
"
cs.LG,Stability Bound for Stationary Phi-mixing and Beta-mixing Processes,"  Most generalization bounds in learning theory are based on some measure of
the complexity of the hypothesis class used, independently of any algorithm. In
contrast, the notion of algorithmic stability can be used to derive tight
generalization bounds that are tailored to specific learning algorithms by
exploiting their particular properties. However, as in much of learning theory,
existing stability analyses and bounds apply only in the scenario where the
samples are independently and identically distributed. In many machine learning
applications, however, this assumption does not hold. The observations received
by the learning algorithm often have some inherent temporal dependence.
  This paper studies the scenario where the observations are drawn from a
stationary phi-mixing or beta-mixing sequence, a widely adopted assumption in
the study of non-i.i.d. processes that implies a dependence between
observations weakening over time. We prove novel and distinct stability-based
generalization bounds for stationary phi-mixing and beta-mixing sequences.
These bounds strictly generalize the bounds given in the i.i.d. case and apply
to all stable learning algorithms, thereby extending the use of
stability-bounds to non-i.i.d. scenarios.
  We also illustrate the application of our phi-mixing generalization bounds to
general classes of learning algorithms, including Support Vector Regression,
Kernel Ridge Regression, and Support Vector Machines, and many other kernel
regularization-based and relative entropy-based regularization algorithms.
These novel bounds can thus be viewed as the first theoretical basis for the
use of these algorithms in non-i.i.d. scenarios.
"
cs.LG,Land Cover Mapping Using Ensemble Feature Selection Methods,"  Ensemble classification is an emerging approach to land cover mapping whereby
the final classification output is a result of a consensus of classifiers.
Intuitively, an ensemble system should consist of base classifiers which are
diverse i.e. classifiers whose decision boundaries err differently. In this
paper ensemble feature selection is used to impose diversity in ensembles. The
features of the constituent base classifiers for each ensemble were created
through an exhaustive search algorithm using different separability indices.
For each ensemble, the classification accuracy was derived as well as a
diversity measure purported to give a measure of the inensemble diversity. The
correlation between ensemble classification accuracy and diversity measure was
determined to establish the interplay between the two variables. From the
findings of this paper, diversity measures as currently formulated do not
provide an adequate means upon which to constitute ensembles for land cover
mapping.
"
cs.LG,A Novel Clustering Algorithm Based on Quantum Random Walk,"  The enormous successes have been made by quantum algorithms during the last
decade. In this paper, we combine the quantum random walk (QRW) with the
problem of data clustering, and develop two clustering algorithms based on the
one dimensional QRW. Then, the probability distributions on the positions
induced by QRW in these algorithms are investigated, which also indicates the
possibility of obtaining better results. Consequently, the experimental results
have demonstrated that data points in datasets are clustered reasonably and
efficiently, and the clustering algorithms are of fast rates of convergence.
Moreover, the comparison with other algorithms also provides an indication of
the effectiveness of the proposed approach.
"
cs.LG,Convex Sparse Matrix Factorizations,"  We present a convex formulation of dictionary learning for sparse signal
decomposition. Convexity is obtained by replacing the usual explicit upper
bound on the dictionary size by a convex rank-reducing term similar to the
trace norm. In particular, our formulation introduces an explicit trade-off
between size and sparsity of the decomposition of rectangular matrices. Using a
large set of synthetic examples, we compare the estimation abilities of the
convex and non-convex approaches, showing that while the convex formulation has
a single local minimum, this may lead in some cases to performance which is
inferior to the local minima of the non-convex formulation.
"
cs.LG,Binary Classification Based on Potentials,"  We introduce a simple and computationally trivial method for binary
classification based on the evaluation of potential functions. We demonstrate
that despite the conceptual and computational simplicity of the method its
performance can match or exceed that of standard Support Vector Machine
methods.
"
q-bio.NC,"Synchronization and oscillatory dynamics in heterogeneous mutually
  inhibited neurons","  We study some mechanisms responsible for synchronous oscillations and loss of
synchrony at physiologically relevant frequencies (10-200 Hz) in a network of
heterogeneous inhibitory neurons. We focus on the factors that determine the
level of synchrony and frequency of the network response, as well as the
effects of mild heterogeneity on network dynamics. With mild heterogeneity,
synchrony is never perfect and is relatively fragile. In addition, the effects
of inhibition are more complex in mildly heterogeneous networks than in
homogeneous ones. In the former, synchrony is broken in two distinct ways,
depending on the ratio of the synaptic decay time to the period of repetitive
action potentials ($\tau_s/T$), where $T$ can be determined either from the
network or from a single, self-inhibiting neuron. With $\tau_s/T > 2$,
corresponding to large applied current, small synaptic strength or large
synaptic decay time, the effects of inhibition are largely tonic and
heterogeneous neurons spike relatively independently. With $\tau_s/T < 1$,
synchrony breaks when faster cells begin to suppress their less excitable
neighbors; cells that fire remain nearly synchronous. We show numerically that
the behavior of mildly heterogeneous networks can be related to the behavior of
single, self-inhibiting cells, which can be studied analytically.
"
q-bio.NC,Frequency control in synchronized networks of inhibitory neurons,"  We analyze the control of frequency for a synchronized inhibitory neuronal
network. The analysis is done for a reduced membrane model with a
biophysically-based synaptic influence. We argue that such a reduced model can
quantitatively capture the frequency behavior of a larger class of neuronal
models. We show that in different parameter regimes, the network frequency
depends in different ways on the intrinsic and synaptic time constants. Only in
one portion of the parameter space, called `phasic', is the network period
proportional to the synaptic decay time. These results are discussed in
connection with previous work of the authors, which showed that for mildly
heterogeneous networks, the synchrony breaks down, but coherence is preserved
much more for systems in the phasic regime than in the other regimes. These
results imply that for mildly heterogeneous networks, the existence of a
coherent rhythm implies a linear dependence of the network period on synaptic
decay time, and a much weaker dependence on the drive to the cells. We give
experimental evidence for this conclusion.
"
q-bio.NC,Coding Strategies in Monkey V1 and Inferior Temporal Cortices,"  We would like to know whether the statistics of neuronal responses vary
across cortical areas. We examined stimulus-elicited spike count response
distributions in V1 and IT cortices of awake monkeys. In both areas the
distribution of spike counts for each stimulus was well-described by a
Gaussian, with the log of the variance in the spike count linearly related to
the log of the mean spike count. Two significant differences in response
characteristics were found: both the range of spike counts and the slope of the
log(variance) vs. log(mean) regression were larger in V1 than in IT. However,
neurons in the two areas transmitted approximately the same amount of
information about the stimuli, and had about the same channel capacity (the
maximum possible transmitted information given noise in the responses). These
results suggest that neurons in V1 use more variable signals over a larger
dynamic range than neurons in IT, which use less variable signals over a
smaller dynamic range. The two coding strategies are approximately as effective
in transmitting information.
"
q-bio.NC,Measuring the dynamics of neural responses in primary auditory cortex,"  We review recent developments in the measurement of the dynamics of the
response properties of auditory cortical neurons to broadband sounds, which is
closely related to the perception of timbre. The emphasis is on a method that
characterizes the spectro-temporal properties of single neurons to dynamic,
broadband sounds, akin to the drifting gratings used in vision. The method
treats the spectral and temporal aspects of the response on an equal footing.
"
q-bio.NC,Synaptic Transmission: An Information-Theoretic Perspective,"  Here we analyze synaptic transmission from an information-theoretic
perspective. We derive closed-form expressions for the lower-bounds on the
capacity of a simple model of a cortical synapse under two explicit coding
paradigms. Under the ``signal estimation'' paradigm, we assume the signal to be
encoded in the mean firing rate of a Poisson neuron. The performance of an
optimal linear estimator of the signal then provides a lower bound on the
capacity for signal estimation. Under the ``signal detection'' paradigm, the
presence or absence of the signal has to be detected. Performance of the
optimal spike detector allows us to compute a lower bound on the capacity for
signal detection. We find that single synapses (for empirically measured
parameter values) transmit information poorly but significant improvement can
be achieved with a small amount of redundancy.
"
q-bio.NC,Sensory Coding with Dynamically Competitive Networks,"  Studies of insect olfactory processing indicate that odors are represented by
rich spatio-temporal patterns of neural activity. These patterns are very
difficult to predict a priori, yet they are stimulus specific and reliable upon
repeated stimulation with the same input. We formulate here a theoretical
framework in which we can interpret these experimental results. We propose a
paradigm of ``dynamic competition'' in which inputs (odors) are represented by
internally competing neural assemblies. Each pattern is the result of dynamical
motion within the network and does not involve a ``winner'' among competing
possibilities. The model produces spatio-temporal patterns with strong
resemblance to those observed experimentally and possesses many of the general
features one desires for pattern classifiers: large information capacity,
reliability, specific responses to specific inputs, and reduced sensitivity to
initial conditions or influence of noise. This form of neural processing may
thus describe the organizational principles of neural information processing in
sensory systems and go well beyond the observations on insect olfactory
processing which motivated its development.
"
q-bio.NC,"Temporal structure in neuronal activity during working memory in Macaque
  parietal cortex","  A number of cortical structures are reported to have elevated single unit
firing rates sustained throughout the memory period of a working memory task.
How the nervous system forms and maintains these memories is unknown but
reverberating neuronal network activity is thought to be important. We studied
the temporal structure of single unit (SU) activity and simultaneously recorded
local field potential (LFP) activity from area LIP in the inferior parietal
lobe of two awake macaques during a memory-saccade task. Using multitaper
techniques for spectral analysis, which play an important role in obtaining the
present results, we find elevations in spectral power in a 50--90 Hz (gamma)
frequency band during the memory period in both SU and LFP activity. The
activity is tuned to the direction of the saccade providing evidence for
temporal structure that codes for movement plans during working memory. We also
find SU and LFP activity are coherent during the memory period in the 50--90 Hz
gamma band and no consistent relation is present during simple fixation.
Finally, we find organized LFP activity in a 15--25 Hz frequency band that may
be related to movement execution and preparatory aspects of the task. Neuronal
activity could be used to control a neural prosthesis but SU activity can be
hard to isolate with cortical implants. As the LFP is easier to acquire than SU
activity, our finding of rich temporal structure in LFP activity related to
movement planning and execution may accelerate the development of this medical
application.
"
q-bio.NC,Structured psychosocial stress and therapeutic failure,"  Generalized language-of-thought arguments appropriate to interacting
cognitive modules permit exploration of how disease states interact with
medical treatment. The interpenetrating feedback between treatment and response
to it creates a kind of idiotypic hall-of-mirrors generating a synergistic
pattern of efficacy, treatment failure, adverse reactions, and patient
noncompliance which, from a Rate Distortion perspective, embodies a distorted
image of externally-imposed structured psychosocial stress. For the US,
accelerating spatial and social diffusion of such stress enmeshes both dominant
and subordinate populations in a linked system which will express itself, not
only in an increasingly unhealthy society, but in the diffusion of therapeutic
failure, including, but not limited to, drug-based treatments.
"
q-bio.NC,"Different ocular dominance map formation by influence of orientation
  columns in visual cortices","  In animal experiments, the observed orientation preference (OP) and ocular
dominance (OD) columns in the visual cortex of the brain show various pattern
types. Here, we show that the different visual map formations in various
species are due to the crossover behavior in anisotropic systems composed of
orientational and scalar components such as easy-plane Heisenberg models. We
predict the transition boundary between different pattern types with the
anisotropy as a main bifurcation parameter, which is consistent with
experimental observations.
"
q-bio.NC,What do neural nets and quantum theory tell us about mind and reality?,"  This paper proposes an approach to framing and answering fundamental
questions about consciousness. It argues that many of the more theoretical
debates about consciousness, such as debates about ""when does it begin?"", are
misplaced and meaningless, in part because ""consciousness"" as a word has many
valid and interesting definitions, and in part because consciousness qua mind
or intelligence (the main focus here)is a matter of degree or level, not a
binary variable. It proposes that new mathematical work related to functional
neural network designs -- designs so functional that they can be used in
engineering -- is essential to a functional understanding of intelligence as
such, and outlines some key mathematics as of 1999, citing earlier work for
more details. Quantum theory is relevant, but not in the simple ways proposed
in more popular philosophies.
"
q-bio.NC,"Brain neurons as quantum computers: {\it in vivo} support of background
  physics","  The question: whether quantum coherent states can sustain decoherence,
heating and dissipation over time scales comparable to the dynamical timescales
of the brain neurons, is actively discussed in the last years. Positive answer
on this question is crucial, in particular, for consideration of brain neurons
as quantum computers. This discussion was mainly based on theoretical
arguments. In present paper nonlinear statistical properties of the Ventral
Tegmental Area (VTA) of genetically depressive limbic brain are studied {\it in
vivo} on the Flinders Sensitive Line of rats (FSL). VTA plays a key role in
generation of pleasure and in development of psychological drug addiction. We
found that the FSL VTA (dopaminergic) neuron signals exhibit multifractal
properties for interspike frequencies on the scales where healthy VTA
dopaminergic neurons exhibit bursting activity. For high moments the observed
multifractal (generalized dimensions) spectrum coincides with the generalized
dimensions spectrum calculated for a spectral measure of a {\it quantum} system
(so-called kicked Harper model, actively used as a model of quantum chaos).
This observation can be considered as a first experimental ({\it in vivo})
indication in the favour of the quantum (at least partially) nature of the
brain neurons activity.
"
q-bio.NC,"Ocular dominance patterns and the wire length minimization: a numerical
  study","  We study a mathematical model for ocular dominance patterns (ODPs) in primary
visual cortex. This model is based on the premise that ODP is an adaptation to
minimize the length of intra-cortical wiring. Thus we attempt to understand the
existing ODPs by solving a wire length minimization problem. We divide all the
neurons into two classes: left- and right-eye dominated. We find that
segregation of neurons into monocular regions reduces wire length if the number
of connections to the neurons of the same class (intraocular) differs from the
number of interocular connections. The shape of the regions depends on the
relative fraction of neurons in the two classes. We find that if both classes
are almost equally represented, the optimal ODP consists of interdigitating
stripes. If one class is less numerous than the other, the optimal ODP consists
of patches of the less abundant class surrounded by the neurons of the other
class. We predict that the transition from stripes to patches occurs when the
fraction of neurons dominated by the underrepresented eye is about 40%. This
prediction agrees with the data in macaque and Cebus monkeys. We also study the
dependence of the periodicity of ODP on the parameters of our model.
"
q-bio.NC,"Comparation based bottom-up and top-down filtering model of the
  hippocampus and its environment","  Two rate code models -- a reconstruction network model and a control model --
of the hippocampal-entorhinal loop are merged. The hippocampal-entorhinal loop
plays a double role in the unified model, it is part of a reconstruction
network and a controller, too. This double role turns the bottom-up information
flow into top-down control like signals. The role of bottom-up filtering is
information maximization, noise filtering, temporal integration and prediction,
whereas the role of top-down filtering is emphasizing, i.e., highlighting or
`paving of the way' as well as context based pattern completion. In the joined
model, the control task is performed by cortical areas, whereas reconstruction
networks can be found between cortical areas. While the controller is highly
non-linear, the reconstruction network is an almost linear architecture, which
is optimized for noise estimation and noise filtering. A conjecture of the
reconstruction network model -- that the long-term memory of the visual stream
is the linear feedback connections between neocortical areas -- is reinforced
by the joined model. Falsifying predictions are presented; some of them have
recent experimental support. Connections to attention and to awareness are
made.
"
q-bio.NC,Fast Computation with Neural Oscillators,"  Artificial spike-based computation, inspired by models of computations in the
central nervous system, may present significant performance advantages over
traditional methods for specific types of large scale problems. In this paper,
we study new models for two common instances of such computation,
winner-take-all and coincidence detection. In both cases, very fast convergence
is achieved independent of initial conditions, and network complexity is linear
in the number of inputs.
"
q-bio.NC,Random Walks for Spike-Timing Dependent Plasticity,"  Random walk methods are used to calculate the moments of negative image
equilibrium distributions in synaptic weight dynamics governed by spike-timing
dependent plasticity (STDP). The neural architecture of the model is based on
the electrosensory lateral line lobe (ELL) of mormyrid electric fish, which
forms a negative image of the reafferent signal from the fish's own electric
discharge to optimize detection of sensory electric fields. Of particular
behavioral importance to the fish is the variance of the equilibrium
postsynaptic potential in the presence of noise, which is determined by the
variance of the equilibrium weight distribution. Recurrence relations are
derived for the moments of the equilibrium weight distribution, for arbitrary
postsynaptic potential functions and arbitrary learning rules. For the case of
homogeneous network parameters, explicit closed form solutions are developed
for the covariances of the synaptic weight and postsynaptic potential
distributions.
"
q-bio.NC,K-Winners-Take-All Computation with Neural Oscillators,"  Artificial spike-based computation, inspired by models of computation in the
central nervous system, may present significant performance advantages over
traditional methods for specific types of large scale problems. This paper
describes very simple network architectures for k-winners-take-all and
soft-winner-take-all computation using neural oscillators. Fast convergence is
achieved from arbitrary initial conditions, which makes the networks
particularly suitable to track time-varying inputs.
"
q-bio.NC,"Graded persisting activity of heterogeneous neuron ensembles subject to
  white noises","  Effects of distractions such as noises and parameter heterogeneity have been
studied on the firing activity of ensemble neurons, each of which is described
by the extended Morris-Lecar model showing the graded persisting firings with
the aid of an included ${\rm Ca}^{2+}$-dependent cation current. Although the
sustained activity of {\it single} neurons is rather robust in a sense that the
activity is realized even in the presence of the distractions, the graded
frequency of sustained firings is vulnerable to them. It has been shown,
however, that the graded persisting activity of {\it ensemble} neurons becomes
much robust to the distractions by the pooling (ensemble) effect. When the
coupling is introduced, the synchronization of firings in ensemble neurons is
enhanced, which is beneficial to firings of target neurons.
"
q-bio.NC,"Spectra and waiting-time densities in firing resonant and nonresonant
  neurons","  The response of a neural cell to an external stimulus can follow one of the
two patterns: Nonresonant neurons monotonously relax to the resting state after
excitation while resonant ones show subthreshold oscillations. We investigate
how do these subthreshold properties of neurons affect their suprathreshold
response. Vice versa we ask: Can we distinguish between both types of neuronal
dynamics using suprathreshold spike trains? The dynamics of neurons is given by
stochastic FitzHugh-Nagumo and Morris-Lecar models with either having a focus
or a node as the stable fixpoint. We determine numerically the spectral power
density as well as the interspike interval density in response to a random
(noise-like) signals. We show that the information about the type of dynamics
obtained from power spectra is of limited validity. In contrast, the interspike
interval density gives a very sensitive instrument for the diagnostics of
whether the dynamics has resonant or nonresonant properties. For the latter
value we formulate a fit formula and use it to reconstruct theoretically the
spectral power density, which coincides with the numerically obtained spectra.
We underline that the renewal theory is applicable to analysis of
suprathreshold responses even of resonant neurons.
"
q-bio.NC,"Quantum Physics in Neuroscience and Psychology: A New Theory With
  Respect to Mind/Brain Interaction","  The cognitive frame in which most neuropsychological research on the neural
basis of behavior is conducted contains the assumption that brain mechanisms
per se fully suffice to explain all psychologically described phenomena. This
assumption stems from the idea that the brain is made up entirely of material
particles and fields, and that all causal mechanisms must therefore be
formulated solely in terms of properties of these elements. One consequence of
this stance is that psychological terms having intrinsic mentalistic and/or
experiential content (terms such as ""feeling,"" ""knowing,"" and ""effort"") have
not been included as primary causal factors in neuropsychological research:
insofar as properties are not described in material terms they are deemed
irrelevant to the causal mechanisms underlying brain function. However, the
origin of this demand that experiential realities be excluded from the causal
base is a theory of nature that has been known for more that three quarters of
a century to be fundamentally incorrect. It is explained here why it is
consequently scientifically unwarranted to assume that material factors alone
can in principle explain all causal mechanisms relevant to neuroscience. More
importantly, it is explained how a key quantum effect can be introduced into
brain dynamics in a simple and practical way that provides a rationally
coherent, causally formulated, physics-based way of understanding and using the
psychological and physical data derived from the growing set of studies of the
capacity of directed attention and mental effort to systematically alter brain
function.
"
q-bio.NC,Response variability in balanced cortical networks,"  We study the spike statistics of neurons in a network with dynamically
balanced excitation and inhibition. Our model, intended to represent a generic
cortical column, comprises randomly connected excitatory and inhibitory leaky
integrate-and-fire neurons, driven by excitatory input from an external
population. The high connectivity permits a mean-field description in which
synaptic currents can be treated as Gaussian noise, the mean and
autocorrelation function of which are calculated self-consistently from the
firing statistics of single model neurons. Within this description, we find
that the irregularity of spike trains is controlled mainly by the strength of
the synapses relative to the difference between the firing threshold and the
post-firing reset level of the membrane potential. For moderately strong
synapses we find spike statistics very similar to those observed in primary
visual cortex.
"
q-bio.NC,Mean field methods for cortical network dynamics,"  We review the use of mean field theory for describing the dynamics of dense,
randomly connected cortical circuits. For a simple network of excitatory and
inhibitory leaky integrate-and-fire neurons, we can show how the firing
irregularity, as measured by the Fano factor, increases with the strength of
the synapses in the network and with the value to which the membrane potential
is reset after a spike. Generalizing the model to include conductance-based
synapses gives insight into the connection between the firing statistics and
the high-conductance state observed experimentally in visual cortex. Finally,
an extension of the model to describe an orientation hypercolumn provides
understanding of how cortical interactions sharpen orientation tuning, in a way
that is consistent with observed firing statistics.
"
q-bio.NC,High conductance states in a mean field cortical network model,"  Measured responses from visual cortical neurons show that spike times tend to
be correlated rather than exactly Poisson distributed. Fano factors vary and
are usually greater than 1 due to the tendency of spikes being clustered into
bursts. We show that this behavior emerges naturally in a balanced cortical
network model with random connectivity and conductance-based synapses. We
employ mean field theory with correctly colored noise to describe temporal
correlations in the neuronal activity. Our results illuminate the connection
between two independent experimental findings: high conductance states of
cortical neurons in their natural environment, and variable non-Poissonian
spike statistics with Fano factors greater than 1.
"
q-bio.NC,Fine Discrimination of Analog Patterns by Nonlinear Dendritic Inhibition,"  Recent experiments revealed that a certain class of inhibitory neurons in the
cerebral cortex make synapses not onto cell bodies but at distal parts of
dendrites of the target neurons, mediating highly nonlinear dendritic
inhibition. We propose a novel form of competitive neural network model that
realizes such dendritic inhibition. Contrary to the conventional lateral
inhibition in neural networks, our dendritic inhibition models don't always
show winner-take-all behaviors; instead, they converge to ""I don't know"" states
when unknown input patterns are presented. We derive reduced two-dimensional
dynamics for the network, showing that a drastic shift of the fixed point from
a winner-take-all state to an ""I don't know"" state occurs in accordance with
the increase in noise added to the stored patterns. By preventing
misrecognition in such a way, dendritic inhibition networks achieve fine
pattern discrimination, which could be one of the basic computations by
inhibitory connected recurrent neural networks in the brain.
"
q-bio.NC,Performance of networks of artificial neurons: The role of clustering,"  The performance of the Hopfield neural network model is numerically studied
on various complex networks, such as the Watts-Strogatz network, the
Barab{\'a}si-Albert network, and the neuronal network of the C. elegans.
Through the use of a systematic way of controlling the clustering coefficient,
with the degree of each neuron kept unchanged, we find that the networks with
the lower clustering exhibit much better performance. The results are discussed
in the practical viewpoint of application, and the biological implications are
also suggested.
"
q-bio.NC,"The perception of melodic consonance: an acoustical and
  neurophysiological explanation based on the overtone series","  The melodic consonance of a sequence of tones is explained using the overtone
series: the overtones form ""flow lines"" that link the tones melodically; the
strength of these flow lines determines the melodic consonance. This hypothesis
admits of psychoacoustical and neurophysiological interpretations that fit well
with the place theory of pitch perception. The hypothesis is used to create a
model for how the auditory system judges melodic consonance, which is used to
algorithmically construct melodic sequences of tones.
"
q-bio.NC,"Mean field theory for a balanced hypercolumn model of orientation
  selectivity in primary visual cortex","  We present a complete mean field theory for a balanced state of a simple
model of an orientation hypercolumn. The theory is complemented by a
description of a numerical procedure for solving the mean-field equations
quantitatively. With our treatment, we can determine self-consistently both the
firing rates and the firing correlations, without being restricted to specific
neuron models. Here, we solve the analytically derived mean-field equations
numerically for integrate-and-fire neurons. Several known key properties of
orientation selective cortical neurons emerge naturally from the description:
Irregular firing with statistics close to -- but not restricted to -- Poisson
statistics; an almost linear gain function (firing frequency as a function of
stimulus contrast) of the neurons within the network; and a contrast-invariant
tuning width of the neuronal firing. We find that the irregularity in firing
depends sensitively on synaptic strengths. If Fano factors are bigger than 1,
then they are so for all stimulus orientations that elicit firing. We also find
that the tuning of the noise in the input current is the same as the tuning of
the external input, while that for the mean input current depends on both the
external input and the intracortical connectivity.
"
q-bio.NC,Pulse-coupled resonate-and-fire models,"  We analyze two pulse-coupled resonate-and-fire neurons. Numerical simulation
reveals that an anti-phase state is an attractor of this model. We can
analytically explain the stability of anti-phase states by means of a return
map of firing times, which we propose in this paper. The resultant stability
condition turns out to be quite simple. The phase diagram based on our theory
shows that there are two types of anti-phase states. One of these cannot be
seen in coupled integrate-and-fire models and is peculiar to resonate-and-fire
models. The results of our theory coincide with those of numerical simulations.
"
q-bio.NC,A toy model of the brain,"  We have designed a toy brain and have written computer code that simulates
it. This toy brain is flexible, modular, has hierarchical learning and
recognition, has short and long term memory, is distributed (i.e. has no
central control), is asynchronous, and includes parallel and series processing.
We have simulated the neurons calculating their internal voltages as a function
of time. We include in the simulation the ion pumps of the neurons, the
synapses with glutamate or GABA neurotransmitters, and the delays of the action
pulses in axons and dendrites. We have used known or plausible circuits of real
brains. The toy brain reads books and learns languages using the Hebb
mechanism. Finally, we have related the toy brain with what might be occurring
in a real brain.
"
q-bio.NC,"General representation of collective neural dynamics with columnar
  modularity","  We exhibit a mathematical framework to represent the neural dynamics at
cortical level. Our description of neural dynamics with columnar and functional
modularity, named fibre bundle representation (FBM) method, is based both on
neuroscience and informatics, whereas they correspond with the conventional
formulas in statistical physics. In spite of complex interactions in neural
circuitry and various cortical modification rules per models, some significant
factors determine the typical phenomena in cortical dynamics. The FBM
representation method reveals them plainly and gives profit in building or
analyzing the cortical dynamic models. Not only the similarity in formulas, the
cortical dynamics can share the statistical properties with other physical
systems, which validated in primary visual maps. We apply our method to
proposed models in visual map formations, in addition our suggestion using the
lateral interaction scheme. In this paper, we will show that the neural dynamic
procedures can be treated through conventional physics expressions and
theories.
"
q-bio.NC,"Cortical Dynamics and Awareness State: An Interpretation of Observed
  Interstimulus Interval Dependence in Apparent Motion","  In a recent paper on Cortical Dynamics, Francis and Grossberg raise the
question how visual forms and motion information are integrated to generate a
coherent percept of moving forms? In their investigation of illusory contours
(which are, like Kanizsa squares, mental constructs rather than stimuli on the
retina) they quantify the subjective impression of apparent motion between
illusory contours that are formed by two subsequent stimuli with delay times of
about 0.2 second (called the interstimulus interval ISI). The impression of
apparent motion is due to a back referral of a later experience to an earlier
time in the conscious representation. A model is developed which describes the
state of awareness in the observer in terms of a time dependent Schroedinger
equation to which a second order time derivative is added. This addition
requires as boundary conditions the values of the solution both at the
beginning and after the process. Satisfactory quantitative agreement is found
between the results of the model and the experimental results. We recall that
in the von Neumann interpretation of the collapse of the quantum mechanical
wave-function, the collapse was associated with an observer's awareness. Some
questions of causality and determinism that arise from later-time boundary
conditions are touched upon.
"
q-bio.NC,"Predicting spike times of a detailed conductance-based neuron model
  driven by stochastic spike arrival","  Reduced models of neuronal activity such as Integrate-and-Fire models allow a
description of neuronal dynamics in simple, intuitive terms and are easy to
simulate numerically. We present a method to fit an Integrate-and-Fire-type
model of neuronal activity, namely a modified version of the Spike Response
Model, to a detailed Hodgkin-Huxley-type neuron model driven by stochastic
spike arrival. In the Hogkin-Huxley model, spike arrival at the synapse is
modeled by a change of synaptic conductance. For such conductance spike input,
more than 70% of the postsynaptic action potentials can be predicted with the
correct timing by the Integrate-and-Fire-type model. The modified Spike
Response Model is based upon a linearized theory of conductance-driven
Integrate-and-Fire neuron.
"
q-bio.NC,"A subjective distance between stimuli: quantifying the metric structure
  of representations","  As subjects perceive the sensory world, different stimuli elicit a number of
neural representations. Here, a subjective distance between stimuli is defined,
measuring the degree of similarity between the underlying representations. As
an example, the subjective distance between different locations in space is
calculated from the activity of rodent hippocampal place cells, and lateral
septal cells. Such a distance is compared to the real distance, between
locations. As the number of sampled neurons increases, the subjective distance
shows a tendency to resemble the metrics of real space.
"
q-bio.NC,How to compute using globally coupled oscillators,"  Synchronization is known to play a vital role within many highly connected
neural systems such as the olfactory systems of fish and insects. In this paper
we show how one can robustly and effectively perform practical computations
using small perturbations to a very simple globally coupled network of coupled
oscillators. Computations are performed by exploiting the spatio-temporal
dynamics of a robust attracting heteroclinic network (also referred to as
`winnerless competition' dynamics). We use different cluster synchronization
states to encode memory states and use this to design a simple multi-base
counter. The simulations indicate that this gives a robust computational system
exploiting the natural dynamics of the system.
"
q-bio.NC,Can Neural Networks Recognize Parts?,"  We have demonstrated neural networks can recognize parts by visual images.
Input signals are gray scale photographs of objects consisting of some parts
and output signals are their shapes. By training neural networks by a few set
of images, without any supervision they become to be able to recognize the
boundary between parts.
"
q-bio.NC,Theory of localized synfire chain,"  Neuron is a noisy information processing unit and conventional view is that
information in the cortex is carried on the rate of neurons spike emission.
More recent studies on the activity propagation through the homogeneous network
have demonstrated that signals can be transmitted with millisecond fidelity;
this model is called the Synfire chain and suggests the possibility of the
spatio-temporal coding. However, the more biologically realistic, structured
feedforward network generates spatially distributed inputs. It results in the
difference of spike timing. This poses a question on how the spatial structure
of a network effect the stability of spatio-temporal spike patterns, and the
speed of a spike packet propagation. By formulating the Fokker-Planck equation
for the feedforwardly coupled network with Mexican-Hat type connectivity, we
show the stability of localized spike packet and existence of Multi-stable
phase where both uniform and localized spike packets are stable depending on
the initial input structure. The Multi-stable phase enables us to show that a
spike pattern, or the information of its own, determines the propagation speed.
"
q-bio.NC,Stimulus competition by inhibitory interference,"  When two stimuli are present in the receptive field of a V4 neuron, the
firing rate response is between the weakest and strongest response elicited by
each of the stimuli alone (Reynolds et al, 1999, Journal of Neuroscience
19:1736-1753). When attention is directed towards the stimulus eliciting the
strongest response (the preferred stimulus), the response to the pair is
increased, whereas the response decreases when attention is directed to the
other stimulus (the poor stimulus). These experimental results were reproduced
in a model of a V4 neuron under the assumption that attention modulates the
activity of local interneuron networks. The V4 model neuron received
stimulus-specific asynchronous excitation from V2 and synchronous inhibitory
inputs from two local interneuron networks in V4. Each interneuron network was
driven by stimulus-specific excitatory inputs from V2 and was modulated by a
projection from the frontal eye fields. Stimulus competition was present
because of a delay in arrival time of synchronous volleys from each interneuron
network. For small delays, the firing rate was close to the rate elicited by
the preferred stimulus alone, whereas for larger delays it approached the
firing rate of the poor stimulus. When either stimulus was presented alone the
neuron's response was not altered by the change in delay. The model suggests
that top-down attention biases the competition between V2 columns for control
of V4 neurons by changing the relative timing of inhibition rather than by
changes in the degree of synchrony of interneuron networks. The mechanism
proposed here for attentional modulation of firing rate - gain modulation by
inhibitory interference - is likely to have more general applicability to
cortical information processing.
"
q-bio.NC,"Effect of Synchronous Incoming Spikes on Activity Pattern in A Network
  of Spiking Neurons","  Although recent neurophysiological experiments suggest that synchronous
neural activity is involved in some perceptual and cognitive processes, the
functional role of such coherent neuronal behavior is not well understood. As a
first step in clarifying this role, we investigate how the temporal coherence
of certain neuronal activity affects the activity pattern in a neural network.
Using a simple network of leaky integrate-and-fire neurons, we study the
effects of synchronized incoming spikes on the functioning of two mechanisms
typically used in model neural systems, winner-take-all competition and
associative memory. We demonstrate that a pair of switches undergone by the
incoming spikes, from asynchronous to synchronous and then back to
asynchronous, triggers a transition of the network from one state to another
state. In the case of associative memory, for example, this switching controls
the timing of the next recalling, whereas the firing rate pattern in the
asynchronous state prepares the network for the next retrieval pattern.
"
q-bio.NC,"Neurokinematic Modeling of Complex Swimming Patterns of the Larval
  Zebrafish","  Larval zebrafish exhibit a variety of complex undulatory swimming patterns.
This repertoire is controlled by the 300 neurons projecting from brain into
spinal cord. Understanding how descending control signals shape the output of
spinal circuits, however, is nontrivial. We have therefore developed a
segmental oscillator model (using NEURON) to investigate this system. We found
that adjusting the strength of NMDA and glycinergic synapses enabled the
generation of oscillation (tail-beat) frequencies over the range exhibited in
different larval swim patterns. In addition, we developed a kinematic model to
visualize the more complex axial bending patterns used during prey capture.
"
q-bio.NC,Activity-dependent brain model explaining EEG spectra,"  Most brain models focus on associative memory or calculation capability,
experimentally inaccessible using physiological methods. Here we present a
model explaining a basic feature of electroencephalograms (EEG). Our model is
based on an electrical network with threshold firing and plasticity of synapses
that reproduces very robustly the measured exponent 0.8 of the medical EEG
spectra, a solid evidence for self-organized criticality. Our result are also
valid on small-world lattices. We propose that an universal scaling behaviour
characterizes many physiological signal spectra for brain controlled
activities.
"
q-bio.NC,"Cognitive styles sex the brain, compete neurally, and quantify deficits
  in autism","  Introduction: Two key dimensions of the mind are understanding and responding
to another's mental state (empathizing), and analysing lawful behaviour
(systemizing).
  Methods: Two questionnaires, the Systemizing Quotient (SQ) and the Empathy
Quotient (EQ), were administered to a normal control group and a group of
individuals with Asperger Syndrome (AS) or High-Functioning Autism (HFA). The
multivariate correlations of the joint scores were analysed using principal
components analysis.
  Results: The principal components were well-approximated by the sums and
differences of the SQ and EQ scores. The differences in the scores corresponded
to sex differences within the control group and also separated out the AS/HFA
group, which showed stronger systemizing than the control group, but
below-average empathy. The sums of the scores did not show sex differences, but
did distinguish the AS/HFA group.
  Conclusions: These tests reliably sex the brain, and their correlations show
that empathizing and systemizing are not independent, but compete neurally.
Their combined score (EQ + SQ) quantifies the deficit in autism spectrum
conditions.
"
q-bio.NC,Computational Models of Adult Neurogenesis,"  Experimental results in recent years have shown that adult neurogenesis is a
significant phenomenon in the mammalian brain. Little is known, however, about
the functional role played by the generation and destruction of neurons in the
context of and adult brain. Here we propose two models where new projection
neurons are incorporated. We show that in both models, using incorporation and
removal of neurons as a computational tool, it is possible to achieve a higher
computational efficiency that in purely static, synapse-learning driven
networks. We also discuss the implication for understanding the role of adult
neurogenesis in specific brain areas.
"
q-bio.NC,"Comment on: ""Characterization of subthreshold voltage fluctuations in
  neuronal membranes"" by M. Rudolph and A. Destexhe","  In two recent papers, Rudolph and Destexhe (Neural Comp. {\bf 15},
  2577-2618, 2003; Neural Comp. in press, 2005) studied a leaky integrator
model (i.e. an RC-circuit) driven by correlated (``colored'') Gaussian
conductance noise and Gaussian current noise. In the first paper they derived
an expression for the stationary probability density of the membrane voltage;
in the second paper this expression was modified to cover a larger parameter
regime. Here we show by standard analysis of solvable limit cases (white-noise
limit of additive and multiplicative noise sources; only slow multiplicative
noise; only additive noise) and by numerical simulations that their first
result does not hold for the general colored-noise case and uncover the errors
made in the derivation of a Fokker-Planck equation for the probability density.
Furthermore, we demonstrate analytically (including an exact integral
expression for the time-dependent mean value of the voltage) and by comparison
to simulation results, that the extended expression for the probability density
works much better but still does not solve exactly the full colored-noise
problem. We also show that at stronger synaptic input the stationary mean value
of the linear voltage model may diverge and give an exact condition relating
the system parameters for which this takes place.
"
q-bio.NC,"Autoassociative Memory Retrieval and Spontaneous Activity Bumps in
  Small-World Networks of Integrate-and-Fire Neurons","  Qualitatively, some real networks in the brain could be characterized as
'small worlds', in the sense that the structure of their connections is
intermediate between the extremes of an orderly geometric arrangement and of a
geometry-independent random mesh. Small worlds can be defined more precisely in
terms of their mean path length and clustering coefficient; but is such a
precise description useful to better understand how the type of connectivity
affects memory retrieval?
  We have simulated an autoassociative memory network of integrate-and-fire
units, positioned on a ring, with the network connectivity varied
parametrically between ordered and random. We find that the network retrieves
when the connectivity is close to random, and displays the characteristic
behavior of ordered nets (localized 'bumps' of activity) when the connectivity
is close to ordered. Recent analytical work shows that these two behaviours can
coexist in a network of simple threshold-linear units, leading to localized
retrieval states. We find that they tend to be mutually exclusive behaviours,
however, with our integrate-and-fire units. Moreover, the transition between
the two occurs for values of the connectivity parameter which are not simply
related to the notion of small worlds.
"
q-bio.NC,"Transient Information Flow in a Network of Excitatory and Inhibitory
  Model Neurons: Role of Noise and Signal Autocorrelation","  We investigate the performance of sparsely-connected networks of
integrate-and-fire neurons for ultra-short term information processing. We
exploit the fact that the population activity of networks with balanced
excitation and inhibition can switch from an oscillatory firing regime to a
state of asynchronous irregular firing or quiescence depending on the rate of
external background spikes.
  We find that in terms of information buffering the network performs best for
a moderate, non-zero, amount of noise. Analogous to the phenomenon of
stochastic resonance the performance decreases for higher and lower noise
levels. The optimal amount of noise corresponds to the transition zone between
a quiescent state and a regime of stochastic dynamics. This provides a
potential explanation on the role of non-oscillatory population activity in a
simplified model of cortical micro-circuits.
"
q-bio.NC,"Signal buffering in random networks of spiking neurons: microscopic vs.
  macroscopic phenomena","  In randomly connected networks of pulse-coupled elements a time-dependent
input signal can be buffered over a short time. We studied the signal buffering
properties in simulated networks as a function of the networks state,
characterized by both the Lyapunov exponent of the microscopic dynamics and the
macroscopic activity derived from mean-field theory. If all network elements
receive the same signal, signal buffering over delays comparable to the
intrinsic time constant of the network elements can be explained by macroscopic
properties and works best at the phase transition to chaos. However, if only 20
percent of the network units receive a common time-dependent signal, signal
buffering properties improve and can no longer be attributed to the macroscopic
dynamics.
"
q-bio.NC,"Optimal Spike-Timing Dependent Plasticity for Precise Action Potential
  Firing","  In timing-based neural codes, neurons have to emit action potentials at
precise moments in time. We use a supervised learning paradigm to derive a
synaptic update rule that optimizes via gradient ascent the likelihood of
postsynaptic firing at one or several desired firing times. We find that the
optimal strategy of up- and downregulating synaptic efficacies can be described
by a two-phase learning window similar to that of Spike-Timing Dependent
Plasticity (STDP). If the presynaptic spike arrives before the desired
postsynaptic spike timing, our optimal learning rule predicts that the synapse
should become potentiated. The dependence of the potentiation on spike timing
directly reflects the time course of an excitatory postsynaptic potential. The
presence and amplitude of depression of synaptic efficacies for reversed spike
timing depends on how constraints are implemented in the optimization problem.
Two different constraints, i.e., control of postsynaptic rates or control of
temporal locality,are discussed.
"
q-bio.NC,Inhibitory synchrony as a mechanism for attentional gain modulation,"  Recordings from area V4 of monkeys have revealed that when the focus of
attention is on a visual stimulus within the receptive field of a cortical
neuron, two distinct changes can occur: The firing rate of the neuron can
change and there can be an increase in the coherence between spikes and the
local field potential in the gamma-frequency range (30-50 Hz). The hypothesis
explored here is that these observed effects of attention could be a
consequence of changes in the synchrony of local interneuron networks. We
performed computer simulations of a Hodgkin-Huxley type neuron driven by a
constant depolarizing current, I, representing visual stimulation and a
modulatory inhibitory input representing the effects of attention via local
interneuron networks. We observed that the neuron's firing rate and the
coherence of its output spike train with the synaptic inputs was modulated by
the degree of synchrony of the inhibitory inputs. The model suggest that the
observed changes in firing rate and coherence of neurons in the visual cortex
could be controlled by top-down inputs that regulated the coherence in the
activity of a local inhibitory network discharging at gamma frequencies.
"
q-bio.NC,Features and dimensions: Motion estimation in fly vision,"  We characterize the computation of motion in the fly visual system as a
mapping from the high dimensional space of signals in the retinal photodetector
array to the probability of generating an action potential in a motion
sensitive neuron. Our approach to this problem identifies a low dimensional
subspace of signals within which the neuron is most sensitive, and then samples
this subspace to visualize the nonlinear structure of the mapping. The results
illustrate the computational strategies predicted for a system that makes
optimal motion estimates given the physical noise sources in the detector
array. More generally, the hypothesis that neurons are sensitive to low
dimensional subspaces of their inputs formalizes the intuitive notion of
feature selectivity and suggests a strategy for characterizing the neural
processing of complex, naturalistic sensory inputs.
"
q-bio.NC,Noise-enhanced computation in a model of a cortical column,"  Varied sensory systems use noise in order to enhance detection of weak
signals. It has been conjectured in the literature that this effect, known as
stochastic resonance, may take place in central cognitive processes such as the
memory retrieval of arithmetical multiplication. We show in a simplified model
of cortical tissue, that complex arithmetical calculations can be carried out
and are enhanced in the presence of a stochastic background. The performance is
shown to be positively correlated to the susceptibility of the network, defined
as its sensitivity to a variation of the mean of its inputs. For nontrivial
arithmetic tasks such as multiplication, stochastic resonance is an emergent
property of the microcircuitry of the model network.
"
q-bio.NC,Wiring cost in the organization of a biological network,"  To find out the role of the wiring cost in the organization of the neural
network of the nematode \textit{Caenorhapditis elegans} (\textit{C. elegans}),
we build the neuronal map of \textit{C. elegans} based on geometrical positions
of neurons and define the cost as inter-neuronal Euclidean distance \textit{d}.
We show that the wiring probability decays exponentially as a function of
\textit{d}. Using the edge exchanging method and the component placement
optimization scheme, we show that positions of neurons are not randomly
distributed but organized to reduce the total wiring cost. Furthermore, we
numerically study the trade-off between the wiring cost and the performance of
the Hopfield model on the neural network.
"
q-bio.NC,"Dynamics of learning in coupled oscillators tutored with delayed
  reinforcements","  In this work we analyze the solutions of a simple system of coupled phase
oscillators in which the connectivity is learned dynamically. The model is
inspired in the process of learning of birdsong by oscine birds. An oscillator
acts as the generator of a basic rhythm, and drives slave oscillators which are
responsible for different motor actions. The driving signal arrives to each
driven oscillator through two different pathways. One of them is a ""direct""
pathway. The other one is a ""reinforcement"" pathway, through which the signal
arrives delayed. The coupling coefficients between the driving oscillator and
the slave ones evolve in time following a Hebbian-like rule. We discuss the
conditions under which a driven oscillator is capable of learning to lock to
the driver. The resulting phase difference and connectivity is a function of
the delay of the reinforcement. Around some specific delays, the system is
capable to generate dramatic changes in the phase difference between the driver
and the driven systems. We discuss the dynamical mechanism responsible for this
effect, and possible applications of this learning scheme.
"
q-bio.NC,"Search for optimal measure for discriminating spike trains with
  different randomness","  We wish to discriminate spike sequences based on the degree of irregularity.
For this purpose, we search for a rational expressions of quadratic functions
of consecutive interspike intervals that efficiently measures spiking
irregularity. Under natural assumptions, the functional form of the coefficient
can be parameterized by a single parameter. The parameter is determined so as
to maximize the mutual information between the distributions of coefficients
computed for spike sequences derived from different renewal point processes. We
find that the local variation of interspike intervals, LV (Neural Comput. Vol.
15, pp. 2823-42, 2003), is nearly optimal for whose intrinsic irregularity is
close to that of experimental data.
"
q-bio.NC,"Postsynaptic mechanisms for the induction of long-term potentiation and
  long-term depression of synaptic transmission in CA1 pyramidal neurons of the
  hippocampus","  The studies described in this dissertation have attempted to address the
cellular mechanisms of information storage by the brain. My work has focused
primarily on the postsynaptic events that occur during and after induction of
long-term potentiation (LTP) and long-term depression (LTD) at the Schaffer
collateral-CA1 synapse of the hippocampus. I have explored various aspects of
the role of postsynaptic Ca$^{2+}$ in the induction of LTP and LTD using both
extracellular and whole-cell recording techniques, as well as fluorescence
imaging of Ca$^{2+}$. I have also examined the possible role of modulation of
dendritic K$^{+}$ channels in the induction of LTP.
"
q-bio.NC,Traveling wave solutions of Fitzhugh model with cross-diffusion,"  The Fitzhugh-Nagumo equations have been used as a caricature of the
Hodgkin-Huxley equations of neuron firing to better understand the essential
dynamics of the interaction of the membrane potential and the restoring force
and to capture, qualitatively, the general properties of an excitable membrane.
Even though its simplicity allows very valuable insight to be gained, the
accuracy of reproducing real experimental results is limited. In this paper, we
utilize a modified version of the Fitzhugh-Nagumo equations to model the
spatial propagation of neuron firing; we assume that this propagation is (at
least, partially) caused by the cross-diffusion connection between the
potential and recovery variables. We show that the cross-diffusion version of
the model, besides giving rise to the typical fast traveling wave solution
exhibited in the original diffusion Fitzhugh-Nagumo equations, also gives rise
to a slow traveling wave solution. We analyze all possible traveling wave
solutions of the Fitzhugh-Nagumo equations with this cross-diffusion term and
show that there exists a threshold of the cross-diffusion coefficient (the
maximum value for a given speed of propagation), which bounds the area where
normal impulse propagation is possible.
"
q-bio.NC,Nonlinear aspects of the EEG during sleep in children,"  Electroencephalograph (EEG) analysis enables the neuronal behavior of a
section of the brain to be examined. If the behavior is nonlinear then
nonlinear tools can be used to glean information on brain behavior, and aid in
the diagnosis of sleep abnormalities such as obstructive sleep apnea syndrome
(OSAS). In this paper the sleep EEGs of a set of normal and mild OSAS children
are evaluated for nonlinear behaviour. We consider how the behaviour of the
brain changes with sleep stage and between normal and OSAS children.
"
q-bio.NC,Effects of fast presynaptic noise in attractor neural networks,"  We study both analytically and numerically the effect of presynaptic noise on
the transmission of information in attractor neural networks. The noise occurs
on a very short-time scale compared to that for the neuron dynamics and it
produces short-time synaptic depression. This is inspired in recent
neurobiological findings that show that synaptic strength may either increase
or decrease on a short-time scale depending on presynaptic activity. We thus
describe a mechanism by which fast presynaptic noise enhances the neural
network sensitivity to an external stimulus. The reason for this is that, in
general, the presynaptic noise induces nonequilibrium behavior and,
consequently, the space of fixed points is qualitatively modified in such a way
that the system can easily scape from the attractor. As a result, the model
shows, in addition to pattern recognition, class identification and
categorization, which may be relevant to the understanding of some of the brain
complex tasks.
"
q-bio.NC,"Stimulus - response curves of a neuronal model for noisy subthreshold
  oscillations and related spike generation","  We investigate the stimulus-dependent tuning properties of a noisy ionic
conductance model for intrinsic subthreshold oscillations in membrane potential
and associated spike generation. On depolarization by an applied current, the
model exhibits subthreshold oscillatory activity with occasional spike
generation when oscillations reach the spike threshold. We consider how the
amount of applied current, the noise intensity, variation of maximum
conductance values and scaling to different temperature ranges alter the
responses of the model with respect to voltage traces, interspike intervals and
their statistics and the mean spike frequency curves. We demonstrate that
subthreshold oscillatory neurons in the presence of noise can sensitively and
also selectively be tuned by stimulus-dependent variation of model parameters.
"
q-bio.NC,Control of neural chaos by synaptic noise,"  We studied neural automata -or neurobiologically inspired cellular automata-
which exhibits chaotic itinerancy among the different stored patterns or
memories. This is a consequence of activity-dependent synaptic fluctuations,
which continuously destabilize the attractor and induce irregular hopping to
other possible attractors. The nature of the resulting irregularity depends on
the dynamic details, namely, on the intensity of the synaptic noise and on the
number of sites of the network that are synchronously updated at each time
step. Varying these details, different regimes occur from regular to chaotic.
In the absence of external agents, the chaotic behavior may turn regular after
tuning the noise intensity. It is argued that a similar mechanism might be at
the origin of the self-control of chaos in natural systems.
"
q-bio.NC,"Participatory Ecosystem Management Planning at Tuzla Lake (Turkey) Using
  Fuzzy Cognitive Mapping","  A participatory environmental management plan was prepared for Tuzla Lake,
Turkey. Fuzzy cognitive mapping approach was used to obtain stakeholder views
and desires. Cognitive maps were prepared with 44 stakeholders (villagers,
local decisionmakers, government and non-government organization (NGO)
officials). Graph theory indices, statistical methods and ""What-if"" simulations
were used in the analysis. The most mentioned variables were livelihood,
agriculture and animal husbandry. The most central variable was agriculture for
local people (villagers and local decisionmakers) and education for NGO &
Government officials. All the stakeholders agreed that livelihood was increased
by agriculture and animal husbandry while hunting decreased birds and wildlife.
Although local people focused on their livelihoods, NGO & Government officials
focused on conservation of Tuzla Lake and education of local people.
Stakeholders indicated that the conservation status of Tuzla Lake should be
strengthened to conserve the ecosystem and biodiversity, which may be
negatively impacted by agriculture and irrigation. Stakeholders mentioned salt
extraction, ecotourism, and carpet weaving as alternative economic activities.
Cognitive mapping provided an effective tool for the inclusion of the
stakeholders' views and ensured initial participation in environmental planning
and policy making.
"
q-bio.NC,Context-dependent selection of visuomotor maps,"  Behavior results from the integration of ongoing sensory signals and
contextual information in various forms, such as past experience, expectations,
current goals, etc. Thus, the response to a specific stimulus, say the ringing
of a doorbell, varies depending on whether you are at home or in someone else's
house. What is the neural basis of this flexibility? What mechanism is capable
of selecting, in a context-dependent way, an adequate response to a given
stimulus? One possibility is based on a nonlinear neural representation in
which context information regulates the gain of stimulus-evoked responses. Here
I explore the properties of this mechanism. By means of three hypothetical
visuomotor tasks, I study a class of neural network models in which any one of
several possible stimulus-response maps or rules can be selected according to
context. The underlying mechanism based on gain modulation has three key
features: (1) modulating the sensory responses is equivalent to switching on or
off different subpopulations of neurons, (2) context does not need to be
represented continuously, although this is advantageous for generalization, and
(3) context-dependent selection is independent of the discriminability of the
stimuli. In all cases, the contextual cues can quickly turn on or off a
sensory-motor map, effectively changing the functional connectivity between
inputs and outputs in the networks. The model predicts that sensory responses
that are nonlinearly modulated by arbitrary context signals should be found in
behavioral situations that involve choosing or switching between multiple
sensory-motor maps.
"
q-bio.NC,Stable Concurrent Synchronization in Dynamic System Networks,"  In a network of dynamical systems, concurrent synchronization is a regime
where multiple groups of fully synchronized elements coexist. In the brain,
concurrent synchronization may occur at several scales, with multiple
``rhythms'' interacting and functional assemblies combining neural oscillators
of many different types. Mathematically, stable concurrent synchronization
corresponds to convergence to a flow-invariant linear subspace of the global
state space. We derive a general condition for such convergence to occur
globally and exponentially. We also show that, under mild conditions, global
convergence to a concurrently synchronized regime is preserved under basic
system combinations such as negative feedback or hierarchies, so that stable
concurrently synchronized aggregates of arbitrary size can be constructed.
Robustnesss of stable concurrent synchronization to variations in individual
dynamics is also quantified. Simple applications of these results to classical
questions in systems neuroscience and robotics are discussed.
"
q-bio.NC,Temporal correlation based learning in neuron models,"  We study a learning rule based upon the temporal correlation (weighted by a
learning kernel) between incoming spikes and the internal state of the
postsynaptic neuron, building upon previous studies of spike timing dependent
synaptic plasticity (\cite{KGvHW,KGvH1,vH}). Our learning rule for the synaptic
weight $w_{ij}$ is $$ \dot w_{ij}(t)= \epsilon \int_{-\infty}^\infty
\frac{1}{T_l} \int_{t-T_l}^t \sum_\mu \delta(\tau+s-t_{j,\mu}) u(\tau) d\tau\
\Gamma(s)ds $$ where the $t_{j,\mu}$ are the arrival times of spikes from the
presynaptic neuron $j$ and the function $u(t)$ describes the state of the
postsynaptic neuron $i$. Thus, the spike-triggered average contained in the
inner integral is weighted by a kernel $\Gamma(s)$, the learning window,
positive for negative, negative for positive values of the time diffence $s$
between post- and presynaptic activity. An antisymmetry assumption for the
learning window enables us to derive analytical expressions for a general class
of neuron models and to study the changes in input-output relationships
following from synaptic weight changes. This is a genuinely non-linear effect
(\cite{SMA}).
"
q-bio.NC,"Attentional modulation of firing rate and synchrony in a model cortical
  network","  When attention is directed into the receptive field of a V4 neuron, its
contrast response curve is shifted to lower contrast values (Reynolds et al,
2000, Neuron 26:703). Attention also increases the coherence between neurons
responding to the same stimulus (Fries et al, 2001, Science 291:1560). We
studied how the firing rate and synchrony of a densely interconnected cortical
network varied with contrast and how they were modulated by attention. We found
that an increased driving current to the excitatory neurons increased the
overall firing rate of the network, whereas variation of the driving current to
inhibitory neurons modulated the synchrony of the network. We explain the
synchrony modulation in terms of a locking phenomenon during which the ratio of
excitatory to inhibitory firing rates is approximately constant for a range of
driving current values. We explored the hypothesis that contrast is represented
primarily as a drive to the excitatory neurons, whereas attention corresponds
to a reduction in driving current to the inhibitory neurons. Using this
hypothesis, the model reproduces the following experimental observations: (1)
the firing rate of the excitatory neurons increases with contrast; (2) for high
contrast stimuli, the firing rate saturates and the network synchronizes; (3)
attention shifts the contrast response curve to lower contrast values; (4)
attention leads to stronger synchronization that starts at a lower value of the
contrast compared with the attend-away condition. In addition, it predicts that
attention increases the delay between the inhibitory and excitatory synchronous
volleys produced by the network, allowing the stimulus to recruit more
downstream neurons.
"
q-bio.NC,Contextual Emergence of Mental States from Neurodynamics,"  The emergence of mental states from neural states by partitioning the neural
phase space is analyzed in terms of symbolic dynamics. Well-defined mental
states provide contexts inducing a criterion of structural stability for the
neurodynamics that can be implemented by particular partitions. This leads to
distinguished subshifts of finite type that are either cyclic or irreducible.
Cyclic shifts correspond to asymptotically stable fixed points or limit tori
whereas irreducible shifts are obtained from generating partitions of mixing
hyperbolic systems. These stability criteria are applied to the discussion of
neural correlates of consiousness, to the definition of macroscopic neural
states, and to aspects of the symbol grounding problem. In particular, it is
shown that compatible mental descriptions, topologically equivalent to the
neurodynamical description, emerge if the partition of the neural phase space
is generating. If this is not the case, mental descriptions are incompatible or
complementary. Consequences of this result for an integration or unification of
cognitive science or psychology, respectively, will be indicated.
"
q-bio.NC,Waveform sample method of excitable sensory neuron,"  We present a new interpretation for encoding information of the period of
input signals into spike-trains in individual sensory neuronal systems. The
spike-train could be described as the waveform sample of the input signal which
locks sample points to wave crests with randomness. Based on simulations of the
Hodgkin-Huxley (HH) neuron responding to periodic inputs, we demonstrate that
the random sampling is a proper encoding method in medium frequency region
since power spectra of the reconstructed spike-trains are identical to that of
neural signals.
"
q-bio.NC,Sleep as the solution to an optimization problem,"  This paper develops a highly simplified model with which to analyze the
phenomenon of sleep. Motivated by Crick's suggestion that sleep is the brain's
way of ``taking out the trash,'' a suggestion that is supported by emerging
evidence, we consider the problem of the filling and emptying of a tank. At any
given time, the tank may take in external resource, or fill, if resource is
available at that time, or it may empty. The filling phases correspond to
information input from the environment, or input of some material in general,
while the emptying phases correspond to the processing of the resource. Given a
resource-availablility profile over some time interval $ T $, we develop a
canonical algorithm for determining the fill-empty profile that produces the
maximum quantity of processed resource at the end of the time interval. From
this algorithm, it readily follows that for a periodically oscillating
resource-availability profile, the optimal fill-empty strategy is given by a
fill period when resource is available, followed by an empty period when
resource is not. This cycling behavior is analogous to the wake-sleep cycles in
organismal life, where the generally nocturnal sleep phase is a period where
the information collected from the day's activities may be processed. The sleep
cycle is then a mechanism for the organism to process a maximal amount of
information over a daily cycle. Our model can exhibit phenomena analogous to
``microsleeps,'' and other behavior associated with breakdown in sleep
patterns.
"
q-bio.NC,"Glial activation in white matter following ischemia in the neonatal P7
  rat brain","  This study examines cell death and proliferation in the white matter after
neonatal stroke. In post-natal day 7 injured rat, there was a marked reduction
in myelin basic protein (MBP) immunostaining mainly corresponding to numerous
pyknotic immature oligodendrocytes and TUNEL-positive astrocytes in the
ipsilateral external capsule. In contrast, a substantial restoration of MBP, as
indicated by the MBP ratio of left-toright, occurred in the cingulum at 48
(1.27 +- 0.12) and 72 (1.30 +- 0.18, p<0.05) hours of recovery as compared to
age-matched controls (1.03 +- 0.14). Ki-67 immunostaining revealed a first peak
of newly-generated cells in the dorsolateral hippocampal subventricular zone
and cingulum at 72 hours after reperfusion. Double immunofluorescence revealed
that most of the Ki-67-positive cells were astrocytes at 48 hours and NG2
pre-oligodendrocytes at 72 hours of recovery. Microglia infiltration occurs
over several days in the cingulum and a huge quantity of macrophages reached
the subcortical white matter where they engulfed immature oligodendrocytes. The
overall results suggest that the persistent activation of microglia involves a
chronic component of immunoinflammation, which overwhelms repair processes and
contributes to cystic growth in the developing brain.
"
q-bio.NC,"Gradient learning in spiking neural networks by dynamic perturbation of
  conductances","  We present a method of estimating the gradient of an objective function with
respect to the synaptic weights of a spiking neural network. The method works
by measuring the fluctuations in the objective function in response to dynamic
perturbation of the membrane conductances of the neurons. It is compatible with
recurrent networks of conductance-based model neurons with dynamic synapses.
The method can be interpreted as a biologically plausible synaptic learning
rule, if the dynamic perturbations are generated by a special class of
``empiric'' synapses driven by random spike trains from an external source.
"
q-bio.NC,Self-Organized Criticality model for Brain Plasticity,"  Networks of living neurons exhibit an avalanche mode of activity,
experimentally found in organotypic cultures. Here we present a model based on
self-organized criticality and taking into account brain plasticity, which is
able to reproduce the spectrum of electroencephalograms (EEG). The model
consists in an electrical network with threshold firing and activity-dependent
synapse strenghts. The system exhibits an avalanche activity power law
distributed. The analysis of the power spectra of the electrical signal
reproduces very robustly the power law behaviour with the exponent 0.8,
experimentally measured in EEG spectra. The same value of the exponent is found
on small-world lattices and for leaky neurons, indicating that universality
holds for a wide class of brain models.
"
q-bio.NC,"Concurrent encoding of frequency and amplitude modulation in human
  auditory cortex: MEG evidence","  A natural sound can be described by dynamic changes in envelope (amplitude)
and carrier (frequency), corresponding to amplitude modulation (AM) and
frequency modulation (FM) respectively. Although the neural responses to both
AM and FM sounds are extensively studied in both animals and humans, it is
uncertain how they are co-represented when changed simultaneously but
independently, as is typical for ecologically natural signals. This study
elucidates the neural coding of such sounds in human auditory cortex using
magnetoencephalography (MEG). Using stimuli with both sinusoidal modulated
envelope (f_{AM}, 37 Hz) and carrier frequency (f_{FM}, 0.3 - 8 Hz), it is
demonstrated that AM and FM stimulus dynamics are co-represented in the neural
code of human auditory cortex. The stimulus AM dynamics are represented
neurally with AM encoding, by the auditory Steady State Response (aSSR) at
f_{AM}. For sounds with slowly changing carrier frequency ((f_{FM} < 5 Hz), it
is shown that the stimulus FM dynamics are tracked by the phase of the aSSR,
demonstrating neural phase modulation (PM) encoding of the stimulus carrier
frequency. For sounds with faster carrier frequency change ((f_{FM} >= 5 Hz),
it is shown that modulation encoding of stimulus FM dynamics persists, but the
neural encoding is no longer purely PM. This result is consistent with the
recruitment of additional neural AM encoding over and above the original neural
PM encoding, indicating that both the amplitude and phase of the aSSR at f_{AM}
track the stimulus FM dynamics. A neural model is suggested to account for
these observations.
"
q-bio.NC,Grandmother cells and the storage capacity of the human brain,"  Quian Quiroga et al. [Nature 435, 1102 (2005)] have recently discovered
neurons that appear to have the characteristics of grandmother (GM) cells. Here
we quantitatively assess the compatibility of their data with the GM-cell
hypothesis. We show that, contrary to the general impression, a GM-cell
representation can be information-theoretically efficient, but that it must be
accompanied by cells giving a distributed coding of the input. We present a
general method to deduce the sparsity distribution of the whole neuronal
population from a sample, and use it to show there are two populations of
cells: a distributed-code population of less than about 5% of the cells, and a
much more sparsely responding population of putative GM cells. With an
allowance for the number of undetected silent cells, we find that the putative
GM cells can code for 10^5 or more categories, sufficient for them to be
classic GM cells, or to be GM-like cells coding for memories. We quantify the
strong biases against detection of GM cells, and show consistency of our
results with previous measurements that find only distributed coding. We
discuss the consequences for the architecture of neural systems and synaptic
connectivity, and for the statistics of neural firing.
"
q-bio.NC,Synchronization in Electrically Coupled Neural Networks,"  In this report, we investigate the synchronization of temporal activity in an
electrically coupled neural network model. The electrical coupling is
established by homotypic static gap-junctions (Connexin 43). Two distinct
network topologies, namely: {\em sparse random network, (SRN)} and {\em fully
connected network, (FCN)} are used to establish the connectivity. The strength
of connectivity in the FCN is governed by the {\em mean gap junctional
conductance} ($\mu$). In the case of the SRN, the overall strength of
connectivity is governed by the {\em density of connections} ($\delta$) and the
connection strength between two neurons ($S_0$). The synchronization of the
network with increasing gap junctional strength and varying population sizes is
investigated. It was observed that the network {\em abruptly} makes a
transition from a weakly synchronized to a well synchronized regime when
($\delta$) or ($\mu$) exceeds a critical value. It was also observed that the
($\delta$, $\mu$) values used to achieve synchronization decreases with
increasing network size.
"
q-bio.NC,Algorithms for identification and categorization,"  The main features of a family of efficient algorithms for recognition and
classification of complex patterns are briefly reviewed. They are inspired in
the observation that fast synaptic noise is essential for some of the
processing of information in the brain.
"
q-bio.NC,"Competition between synaptic depression and facilitation in attractor
  neural networks","  We study the effect of competition between short-term synaptic depression and
facilitation on the dynamical properties of attractor neural networks, using
Monte Carlo simulation and a mean field analysis. Depending on the balance
between depression, facilitation and the noise, the network displays different
behaviours, including associative memory and switching of the activity between
different attractors. We conclude that synaptic facilitation enhances the
attractor instability in a way that (i) intensifies the system adaptability to
external stimuli, which is in agreement with experiments, and (ii) favours the
retrieval of information with less error during short--time intervals.
"
q-bio.NC,Chaotic hopping between attractors in neural networks,"  We present a neurobiologically--inspired stochastic cellular automaton whose
state jumps with time between the attractors corresponding to a series of
stored patterns. The jumping varies from regular to chaotic as the model
parameters are modified. The resulting irregular behavior, which mimics the
state of attention in which a systems shows a great adaptability to changing
stimulus, is a consequence in the model of short--time presynaptic noise which
induces synaptic depression. We discuss results from both a mean--field
analysis and Monte Carlo simulations.
"
q-bio.NC,"Instability of attractors in autoassociative networks with bioinspired
  fast synaptic noise","  We studied autoassociative networks in which synapses are noisy on a time
scale much shorter that the one for the neuron dynamics. In our model a
presynaptic noise causes postsynaptic depression as recently observed in
neurobiological systems. This results in a nonequilibrium condition in which
the network sensitivity to an external stimulus is enhanced. In particular, the
fixed points are qualitatively modified, and the system may easily scape from
the attractors. As a result, in addition to pattern recognition, the model is
useful for class identification and categorization.
"
q-bio.NC,Combinatorial on/off Model for Olfactory Coding,"  We present a model for olfactory coding based on spatial representation of
glomerular responses. In this model distinct odorants activate specific subsets
of glomeruli, dependent upon the odorant's chemical identity and concentration.
The glomerular response specificities are understood statistically, based on
experimentally measured distributions of detection thresholds. A simple version
of the model, in which glomerular responses are binary (the on/off model),
allows us to account quantitatively for the following results of human/rodent
olfactory psychophysics: 1) just noticeable differences in the perceived
concentration of a single odor (Weber ratios) are dC/C ~ 0.04; 2) the number of
simultaneously perceived odors can be as high as 12; 3) extensive lesions of
the olfactory bulb do not lead to significant changes in detection or
discrimination thresholds. We conclude that a combinatorial code based on a
binary glomerular response is sufficient to account for the discrimination
capacity of the mammalian olfactory system.
"
q-bio.NC,"Synaptic plasticity of Inhibitory synapse promote synchrony in
  inhibitory network in presence of heterogeneity and noise","  Recently spike timing dependent plasticity was observed in inhibitory synapse
in the layer II of entorhinal cortex. The rule provides an interesting zero in
the region of $\Delta t=t_{post}-t_{pre}=0$ and in addition the dynamic range
of the rule lie in gamma frequency band. We propose a robust mechanism based on
this observed synaptic plasticity rule for inhibitory synapses for two mutually
coupled interneurons to phase lock in synchrony in the presence of intrisic
heterogeneity in firing. We study the stability of the phase locked solution by
defining a map for spike times dependent on the phase response curve for the
coupled neurons. Finally we present results on robustness of synchronization in
the presence of noise.
"
q-bio.NC,The role of synaptic facilitation in coincidence spike detection,"  Using a realistic model of activity dependent dynamical synapses and a
standard integrate and fire neuron model we study, both analytically and
numerically, the conditions in which a postsynaptic neuron efficiently detects
temporal coincidences of spikes arriving at certain frequency from N different
afferents. We extend a previous work that only considers synaptic depression as
the most important mechanism in the transmission of information through
synapses, to a more general situation including also synaptic facilitation. Our
study shows that: 1) facilitation enhances the detection of correlated signals
arriving from a subset of presynaptic excitatory neurons, with different
degrees of correlation among this subset, and 2) the presence of facilitation
allows for a better detection of firing rate changes. Finally, we also observed
that facilitation determines the existence of an optimal input frequency which
allows the best performance for a wide (maximum) range of the neuron firing
threshold. This optimal frequency can be controlled by means of facilitation
parameters.
"
q-bio.NC,"Chaotic itinerancy, temporal segmentation and spatio-temporal
  combinatorial codes","  We study a deterministic dynamics with two time scales in a continuous state
attractor network. To the usual (fast) relaxation dynamics towards point
attractors (``patterns'') we add a slow coupling dynamics that makes the
visited patterns to loose stability leading to an itinerant behavior in the
form of punctuated equilibria. One finds that the transition frequency matrix
between patterns shows non-trivial statistical properties in the chaotic
itinerant regime. We show that mixture input patterns can be temporally
segmented by the itinerant dynamics. The viability of a combinatorial
spatio-temporal neural code is also demonstrated.
"
q-bio.NC,"Stable Propagation of a Burst Through a One-Dimensional Homogeneous
  Excitatory Chain Model of Songbird Nucleus HVC","  We demonstrate numerically that a brief burst consisting of two to six spikes
can propagate in a stable manner through a one-dimensional homogeneous
feedforward chain of non-bursting neurons with excitatory synaptic connections.
Our results are obtained for two kinds of neuronal models, leaky
integrate-and-fire (LIF) neurons and Hodgkin-Huxley (HH) neurons with five
conductances. Over a range of parameters such as the maximum synaptic
conductance, both kinds of chains are found to have multiple attractors of
propagating bursts, with each attractor being distinguished by the number of
spikes and total duration of the propagating burst. These results make
plausible the hypothesis that sparse precisely-timed sequential bursts observed
in projection neurons of nucleus HVC of a singing zebra finch are intrinsic and
causally related.
"
q-bio.NC,Episodic synchronization in dynamically driven neurons,"  We examine the response of type II excitable neurons to trains of synaptic
pulses, as a function of the pulse frequency and amplitude. We show that the
resonant behavior characteristic of type II excitability, already described for
harmonic inputs, is also present for pulsed inputs. With this in mind, we study
the response of neurons to pulsed input trains whose frequency varies
continuously in time, and observe that the receiving neuron synchronizes
episodically to the input pulses, whenever the pulse frequency lies within the
neuron's locking range. We propose this behavior as a mechanism of rate-code
detection in neuronal populations. The results are obtained both in numerical
simulations of the Morris-Lecar model and in an electronic implementation of
the FitzHugh-Nagumo system, evidencing the robustness of the phenomenon.
"
q-bio.NC,Theory of Interaction of Memory Patterns in Layered Associative Networks,"  A synfire chain is a network that can generate repeated spike patterns with
millisecond precision. Although synfire chains with only one activity
propagation mode have been intensively analyzed with several neuron models,
those with several stable propagation modes have not been thoroughly
investigated. By using the leaky integrate-and-fire neuron model, we
constructed a layered associative network embedded with memory patterns. We
analyzed the network dynamics with the Fokker-Planck equation. First, we
addressed the stability of one memory pattern as a propagating spike volley. We
showed that memory patterns propagate as pulse packets. Second, we investigated
the activity when we activated two different memory patterns. Simultaneous
activation of two memory patterns with the same strength led the propagating
pattern to a mixed state. In contrast, when the activations had different
strengths, the pulse packet converged to a two-peak state. Finally, we studied
the effect of the preceding pulse packet on the following pulse packet. The
following pulse packet was modified from its original activated memory pattern,
and it converged to a two-peak state, mixed state or non-spike state depending
on the time interval.
"
q-bio.NC,A new stimulus approach in the search for biological nonlocality,"  Preliminary research in the area of biophysics appears to indicate the
existence of quantum entanglement and nonlocality at the biological level, both
for human subjects and for neurons derived from human neural stem cells. The
lack of clear and replicable findings, especially where human subjects are
concerned, results in conflicting and marginal correlations between stimulated
and nonstimulated subject's brainwaves. It is proposed to go beyond the use of
just patterned photostimulation, to encompass either transcranial magnetic
stimulation (TMS) or acupuncture, so that one can achieve a simultaneity of
several different brain events in both stimulated and nonstimulated subjects,
rather than just one EEG-type event.
"
q-bio.NC,Neural integrator - a sandpile model,"  We investigated a model for the neural integrator based on hysteretic units
connected by positive feedback. Hysteresis is assumed to emerge from the
intrinsic properties of the cells. We consider the recurrent networks
containing either bistable or multistable neurons. We apply our analysis to the
oculomotor velocity-to-position neural integrator that calculates the eye
positions from the inputs that carry information about eye angular velocity.
Using the analysis of the system in the parameter space we show the following.
The direction of hysteresis in the neuronal response may be reversed for the
system with recurrent connections compared to the case of unconnected neurons.
Thus, for the NMDA receptor based bistability the firing rates after ON
saccades may be higher than after OFF saccades for the same eye position. We
suggest that this is an emergent property due to the presence of global
recurrent feedback. The reversal of hysteresis occurs only when the size of
hysteresis differs from neuron to neuron. We also relate the macroscopic leak
time-constant of the integrator to the rate of microscopic spontaneous
noise-driven transitions in the hysteretic units. Finally, we argue that the
presence of neurons with small hysteresis may remove the threshold for
integration.
"
q-bio.NC,"Triangular lattice neurons may implement an advanced numeral system to
  precisely encode rat position over large ranges","  We argue by observation of the neural data that neurons in area dMEC of rats,
which fire whenever the rat is on any vertex of a regular triangular lattice
that tiles 2-d space, may be using an advanced numeral system to reversibly
encode rat position. We interpret measured dMEC properties within the framework
of a residue number system (RNS), and describe how RNS encoding -- which breaks
the non-periodic variable of rat position into a set of narrowly distributed
periodic variables -- allows a small set of cells to compactly represent and
efficiently update rat position with high resolution over a large range. We
show that the uniquely useful properties of RNS encoding still hold when the
encoded and encoding quantities are relaxed to be real numbers with built-in
uncertainties, and provide a numerical and functional estimate of the range and
resolution of rat positions that can be uniquely encoded in dMEC. The use of a
compact, `arithmetic-friendly' numeral system to encode a metric variable, as
we propose is happening in dMEC, is qualitatively different from all previously
identified examples of coding in the brain. We discuss the numerous
neurobiological implications and predictions of our hypothesis.
"
q-bio.NC,Scaling law for the transient behavior of type-II neuron models,"  We study the transient regime of type-II biophysical neuron models and
determine the scaling behavior of relaxation times $\tau$ near but below the
repetitive firing critical current, $\tau \simeq C (I_c-I)^{-\Delta}$. For both
the Hodgkin-Huxley and Morris-Lecar models we find that the critical exponent
is independent of the numerical integration time step and that both systems
belong to the same universality class, with $\Delta = 1/2$. For appropriately
chosen parameters, the FitzHugh-Nagumo model presents the same generic
transient behavior, but the critical region is significantly smaller. We
propose an experiment that may reveal nontrivial critical exponents in the
squid axon.
"
q-bio.NC,Perception proximale et distale  l'aide du dispositif de Lenay,"  We present in this article experimental results obtained with the ""dispositif
de Lenay"" : for a localization task (distal perception) and an orientation
estimation task (proximal perception of the orientation of a cylinder in a
plane). In this last experiment, a ""virtual"" version of the Lenay device was
used. Results are here used to illustrate methodological and theoretical
proposals for the study of cognitive and sensori-motor processes involved in
perception.
"
q-bio.NC,"Cerebellar Purkinje Cell Loss in Heterozygous Rora+/-Mice: A
  Longitudinal Study","  The staggerer (sg/sg) mutation is a spontaneous deletion in the Rora gene
that prevents the translation of the ligand-binding domain (LBD), leading to
the loss of ROR\alpha activity. The homozygous Rorasg/sg mutant mouse, whose
most obvious phenotype is ataxia associated with cerebellar degeneration, also
displays a variety of other phenotypes. The heterozygous Rora+/sg is able to
develop a cerebellum which is qualitatively normal but with advancing age
suffers a significant loss of cerebellar neuronal cells. A truncated protein
synthesized by the mutated allele may play a role, both in Rorasg/sg and
Rora+/sg. To determine the effects during life span of true haplo-insufficiency
of the ROR\alpha protein, derived from the invalidation of the gene, we
compared the evolution of Purkinje cell numbers in heterozygous Rora knock-out
males (Rora+/-) and in their wildtype counterparts from 1 to 24 months of age.
We also compared the evolution of Purkinje cell numbers in Rora+/- and Rora+/sg
males from 1 to 9 months. The main finding is that in Rora+/- mice, when only a
half dose of protein is synthesized, the deficit was already established at 1
month and did not change during life span....
"
q-bio.NC,"Pushing it to the Limit: Adaptation With Dynamically Switching Gain
  Control","  With this paper we propose a model to simulate the functional aspects of
light adaptation in retinal photoreceptors. Our model, however, does not link
specific stages to the detailed molecular processes which are thought to
mediate adaptation in real photoreceptors. We rather model the photoreceptor as
a self-adjusting integration device, which adds up properly amplified luminance
signals. The integration process and the amplification obey a switching
behavior that acts to locally shut down the integration process in dependence
on the internal state of the receptor. The mathematical structure of our model
is quite simple, and its computational complexity is quite low. We present
results of computer simulations which demonstrate that our model adapts
properly to at least four orders of input magnitude.
"
q-bio.NC,"Activity-dependent self-wiring is a basis of structural plastisity in
  neural networks","  Dynamical wiring and rewiring in neural networks are carried out by
activity-dependent growth and retraction of axons and dendrites, guided by
gudance molecules, released by target cells. Experience-dependent structural
changes in cortical microcurcuts lead to changes in activity, i.e. to changes
in information encoded. Specific pattens of external stimulation can lead to
creation of new synaptical connections between neurons. Calcium influxes
controlled by neuronal activity regulates processes of neurotrophic factors
release by neurons, growth cones movement and synapse differentiation in
developing neural system, therefore activity-dependent self-wiring can serve as
a basis of structural plasticity in cortical networks and can be considered as
a form of learning.
"
q-bio.NC,"Structural network heterogeneities and network dynamics: a possible
  dynamical mechanism for hippocampal memory reactivation","  The hippocampus has the capacity for reactivating recently acquired memories
[1-3] and it is hypothesized that one of the functions of sleep reactivation is
the facilitation of consolidation of novel memory traces [4-11]. The dynamic
and network processes underlying such a reactivation remain, however, unknown.
We show that such a reactivation characterized by local, self-sustained
activity of a network region may be an inherent property of the recurrent
excitatory-inhibitory network with a heterogeneous structure. The entry into
the reactivation phase is mediated through a physiologically feasible
regulation of global excitability and external input sources, while the
reactivated component of the network is formed through induced network
heterogeneities during learning. We show that structural changes needed for
robust reactivation of a given network region are well within known
physiological parameters [12,13].
"
q-bio.NC,"Unbalanced synaptic inhibition can create intensity-tuned auditory
  cortex neurons","  Intensity-tuned auditory cortex neurons may be formed by intensity-tuned
synaptic excitation. Synaptic inhibition has also been shown to enhance, and
possibly even create intensity-tuned neurons. Here we show, using in vivo whole
cell recordings in pentobarbital-anesthetized rats, that some intensity-tuned
neurons are indeed created solely through disproportionally large inhibition at
high intensities, without any intensity-tuned excitation. Since inhibition is
essentially cortical in origin, these neurons provide examples of auditory
feature-selectivity arising de novo at the cortex.
"
q-bio.NC,"Learning with incomplete information - and the mathematical structure
  behind it","  Learning and the ability to learn are important factors in development and
evolutionary processes [1]. Depending on the level, the complexity of learning
can strongly vary. While associative learning can explain simple learning
behaviour [1,2] much more sophisticated strategies seem to be involved in
complex learning tasks. This is particularly evident in machine learning theory
[3] (reinforcement learning [4], statistical learning [5]), but it equally
shows up in trying to model natural learning behaviour [2]. A general setting
for modelling learning processes in which statistical aspects are relevant is
provided by the neural network (NN) paradigm. This is in particular of interest
for natural, learning by experience situations. NN learning models can
incorporate elementary learning mechanisms based on neuro-physiological
analogies, such as the Hebb rule, and lead to quantitative results concerning
the dynamics of the learning process [6]. The Hebb rule, however, cannot be
directly applied in all cases, and in particular for realistic problems, such
as ""delayed reinforcement"" [4,6], the sophistication of the algorithms rapidly
increases. We want to present here a model which can cope with such non trivial
tasks, while still being elementary and based only on procedures which one may
think of as natural, without any appeal to higher strategies [7]. We can show
the capability of this model to provide good learning in many, very different
settings [7,8,9]. It may help therefore understanding some basic features of
learning.
"
q-bio.NC,"Searching for memories, Sudoku, implicit check-bits, and the iterative
  use of not-always-correct rapid neural computation","  The algorithms that simple feedback neural circuits representing a brain area
can rapidly carry out are often adequate to solve only easy problems, and for
more difficult problems can return incorrect answers. A new
excitatory-inhibitory circuit model of associative memory displays the common
human problem of failing to rapidly find a memory when only a small clue is
present. The memory model and a related computational network for solving
Sudoku puzzles produce answers that contain implicit check-bits in the
representation of information across neurons, allowing a rapid evaluation of
whether the putative answer is correct or incorrect through a computation
related to visual 'pop-out'. This fact may account for our strong psychological
feeling of right or wrong when we retrieve a nominal memory from a minimal
clue. This information allows more difficult computations or memory retrievals
to be done in a serial fashion by using the fast but limited capabilities of a
computational module multiple times. The mathematics of the
excitatory-inhibitory circuits for associative memory and for Sudoku, both of
which are understood in terms of 'energy' or Lyapunov functions, is described
in detail.
"
q-bio.NC,"Calculating event-triggered average synaptic conductances from the
  membrane potential","  The optimal patterns of synaptic conductances for spike generation in central
neurons is a subject of considerable interest. Ideally, such conductance time
courses should be extracted from membrane potential (Vm) activity, but this is
difficult because the nonlinear contribution of conductances to the Vm renders
their estimation from the membrane equation extremely sensitive. We outline
here a solution to this problem based on a discretization of the time axis.
This procedure can extract the time course of excitatory and inhibitory
conductances solely from the analysis of Vm activity. We test this method by
calculating spike-triggered averages of synaptic conductances using numerical
simulations of the integrate-and-fire model subject to colored conductance
noise. The procedure was also tested successfully in biological cortical
neurons using conductance noise injected with dynamic-clamp. This method should
allow the extraction of synaptic conductances from Vm recordings in vivo.
"
q-bio.NC,"The Hilbert transform of horizontal gaze position during natural image
  classification by saccades","  Eye movements are a behavioral response that can be involved in tasks as
complicated as natural image classification. This report confirms that pro- and
anti-saccades can be used by a volunteer to designate target (animal) or
non-target images that were centered 16 degrees off the fixation point. With
more than 86% correct responses, 11 participants responded to targets in 470
milliseconds on average, starting as quick as 245 milliseconds. Furthermore,
tracking the gaze position is considered a powerful method in the studies of
recognition as the saccade response times, ocular dynamics and the events
around the response time can be calculated from the data sampled 240 times per
second. The Hilbert transform is applied to obtain the analytic signal from the
horizontal gaze position. Its amplitude and phase are used to describe
differences between saccades that may testify to the recognition process.
"
q-bio.NC,"Toward an adequate mathematical model of mental space:
  conscious/unconscious dynamics on $m$-adic trees","  We try to perform geometrization of cognitive science and psychology by
representing information states of cognitive systems by points of {\it mental
space} given by a hierarchic $m$-adic tree. Associations are represented by
balls and ideas by collections of balls. We consider dynamics of ideas based on
lifting of dynamics of mental points. We apply our dynamical model for modeling
of flows of unconscious and conscious information in the human brain. In series
of models, Models 1-3, we consider cognitive systems with increasing complexity
of psychological behavior determined by structure of flows of associations and
ideas.
"
q-bio.NC,Correlation entropy of synaptic input-output dynamics,"  The responses of synapses in the neocortex show highly stochastic and
nonlinear behavior. The microscopic dynamics underlying this behavior, and its
computational consequences during natural patterns of synaptic input, are not
explained by conventional macroscopic models of deterministic ensemble mean
dynamics. Here, we introduce the correlation entropy of the synaptic
input-output map as a measure of synaptic reliability which explicitly includes
the microscopic dynamics. Applying this to experimental data, we find that
cortical synapses show a low-dimensional chaos driven by the natural input
pattern.
"
q-bio.NC,"Brain, Music and non-Poisson Renewal Processes","  In this paper we show that both music composition and brain function, as
revealed by the Electroencephalogram (EEG) analysis, are renewal non-Poisson
processes living in the non-ergodic dominion. To reach this important
conclusion we process the data with the minimum spanning tree method, so as to
detect significant events, thereby building a sequence of times, which is the
time series to analyze. Then we show that in both cases, EEG and music
composition, these significant events are the signature of a non-Poisson
renewal process. This conclusion is reached using a techniques of statistical
analysis recently developed by our group, the Aging Experiment (AE). First, we
find that in both cases the distances between two consecutive events are
described by non-exponential histograms, thereby proving the non-Poisson nature
of these processes. The corresponding survival probabilities $\Psi(t)$ are well
fitted by stretched exponentials ($\Psi(t) \propto exp(-(\gamma t)^\alpha$),
with $0.5 <\alpha <1$.) The second step rests on the adoption of the AE, which
shows that these are renewal processes. We show that the renewal stretched
exponential is the emerging tip of an iceberg, whose underwater part has slow
tails with an inverse power law structure with power index $\mu = 1 + \alpha$.
We find that both EEG and music composition yield $\mu < 2$. On the basis of
the recently discovered complexity matching effect, according to which a
complex system $S$ with $\mu_S < 2$ responds only to a complex driving signal
$P$ with $\mu_P = \mu_S$, we conclude that the results of our analysis may
explain the influence of music on human brain.
"
